{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a62fac68-0a57-47c6-b6cc-e62a02385a47",
   "metadata": {},
   "source": [
    "**analysis-network_data_output-GRP.ipynb - Programmatic network data output**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e7065e4-eddb-4319-bc23-f3a4e04e39ef",
   "metadata": {},
   "source": [
    "# Notes\n",
    "\n",
    "- for year comparisons, try to only do people for the years being compared, not all time (all time creates really large matrices).\n",
    "- to start, do matrices for year before and year after layoffs, with the person query including both years.\n",
    "\n",
    "    - store in the vm-share folder, mounted in VM.\n",
    "    - see how large they are.\n",
    "    - then, try loading them in R and doing a correlation. If we can make it work with relatively giant matrices on glassbox, great. if not, will need to try edge lists (I bet the real network packages don't store the whole graph, they just store the edge list and a list of the nodes).\n",
    "    \n",
    "- Need to go to edge lists - then, same edge list can be used with different node lists as long as all nodes are in node list.\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba4ccabf-42d0-4217-8baf-87bd790d8478",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "260fc288-1cd1-4af0-84f6-1e97011defdc",
   "metadata": {},
   "source": [
    "## Setup - Debug\n",
    "\n",
    "- Back to [Table of Contents](#Table-of-Contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2a6a3ed9-96c2-441d-a443-371200c275f0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-17T15:43:45.678275Z",
     "start_time": "2019-09-17T15:43:45.673730Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "debug_flag = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "089876e6-2d2f-4fd0-96f6-730520fb0bab",
   "metadata": {},
   "source": [
    "## Setup - Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "47c3f417-8fcb-49c1-85f3-44c070e980fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "packages imported at 2022-06-02 03:17:25.288841\n"
     ]
    }
   ],
   "source": [
    "# python base imports\n",
    "import copy\n",
    "import csv\n",
    "import datetime\n",
    "import hashlib\n",
    "import json\n",
    "import logging\n",
    "\n",
    "# import six\n",
    "import six\n",
    "\n",
    "print( \"packages imported at \" + str( datetime.datetime.now() ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "105b958b-0102-4978-9132-ab0af2cd81c2",
   "metadata": {},
   "source": [
    "## Setup - working folder paths\n",
    "\n",
    "- Back to [Table of Contents](#Table-of-Contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "47541bda-424d-4bf7-8897-3e2cc19acefc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-17T15:43:47.305614Z",
     "start_time": "2019-09-17T15:43:47.286949Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/jonathanmorgan/work/django/research/research/work/phd_work/analysis/network_data'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "324f5746-3fe3-49bc-ba43-f38602b06bba",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-17T15:43:48.159415Z",
     "start_time": "2019-09-17T15:43:48.153181Z"
    }
   },
   "outputs": [],
   "source": [
    "# current working folder\n",
    "project_name = \"research\"\n",
    "project_base_folder = \"/home/jonathanmorgan/work/django/{project_name}\".format( project_name = project_name )\n",
    "django_project_folder = \"{base_folder}/{project_name}\".format(\n",
    "    base_folder = project_base_folder,\n",
    "    project_name = project_name\n",
    ")\n",
    "current_working_folder = \"{django_project_folder}/work/phd_work/analysis/network_data\".format(\n",
    "    django_project_folder = django_project_folder\n",
    ")\n",
    "current_datetime = datetime.datetime.now()\n",
    "current_date_string = current_datetime.strftime( \"%Y-%m-%d-%H-%M-%S\" )\n",
    "\n",
    "# and, output path.\n",
    "network_data_output_folder_path = \"/tmp\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "206e3216-5791-4be6-aa90-ff0917a1b486",
   "metadata": {},
   "source": [
    "## Setup - logging\n",
    "\n",
    "- Back to [Table of Contents](#Table-of-Contents)\n",
    "\n",
    "configure logging for this notebook's kernel (If you do not run this cell, you'll get the django application's logging configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a9c7beff-b869-464c-9f82-66666ac46c69",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-17T15:43:49.038050Z",
     "start_time": "2019-09-17T15:43:49.031535Z"
    }
   },
   "outputs": [],
   "source": [
    "# build file name\n",
    "project_log_folder = \"{base_folder}/logs\".format( base_folder = project_base_folder )\n",
    "logging_file_name = \"{}/network_data_output-GRP-{}.log.txt\".format( project_log_folder, current_date_string )\n",
    "\n",
    "# set up logging.\n",
    "logging.basicConfig(\n",
    "    level = logging.DEBUG,\n",
    "    format = '%(asctime)s - %(levelname)s - %(name)s - %(message)s',\n",
    "    filename = logging_file_name,\n",
    "    filemode = 'w' # set to 'a' if you want to append, rather than overwrite each time.\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17939321-ff9b-4230-84ef-5165d0d8820a",
   "metadata": {},
   "source": [
    "## Setup - Initialize Django\n",
    "\n",
    "- Back to [Table of Contents](#Table-of-Contents)\n",
    "\n",
    "First, initialize my dev django project, so I can run code in this notebook that references my django models and can talk to the database using my project's settings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "daabb30f-9129-4d0a-85a2-f1b32a522275",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-17T15:43:52.077172Z",
     "start_time": "2019-09-17T15:43:52.071107Z"
    }
   },
   "outputs": [],
   "source": [
    "# init django\n",
    "django_init_folder = \"{django_project_folder}/work/phd_work\".format(\n",
    "    django_project_folder = django_project_folder\n",
    ")\n",
    "django_init_path = \"django_init.py\"\n",
    "if( ( django_init_folder is not None ) and ( django_init_folder != \"\" ) ):\n",
    "    \n",
    "    # add folder to front of path.\n",
    "    django_init_path = \"{}/{}\".format( django_init_folder, django_init_path )\n",
    "    \n",
    "#-- END check to see if django_init folder. --#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4d9e1ebf-ca96-45ed-8a3f-bfd418516e41",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-17T15:43:54.453200Z",
     "start_time": "2019-09-17T15:43:52.833671Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "django initialized at 2022-06-01 18:40:45.979616\n"
     ]
    }
   ],
   "source": [
    "%run $django_init_path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09170da9-c45c-4a93-8b0a-765ce4d9a790",
   "metadata": {},
   "source": [
    "### Setup - django-related imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4c4c9544-0d64-4cf1-baca-f1226617dec1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "django model packages imported at 2022-06-01 18:40:47.350739\n"
     ]
    }
   ],
   "source": [
    "# python utilities\n",
    "from python_utilities.strings.string_helper import StringHelper\n",
    "\n",
    "# import class that actually processes requests for outputting networks.\n",
    "from context_text.export.network_output import NetworkOutput\n",
    "\n",
    "print( \"django model packages imported at \" + str( datetime.datetime.now() ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "858984de-3b03-4475-9fbd-ee10f9c193e3",
   "metadata": {},
   "source": [
    "## Setup - functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1b94cfc-259a-4ffd-84e8-0848379f1e84",
   "metadata": {},
   "source": [
    "### Setup - function `make_string_hash()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "008b7d86-675e-497e-ac35-8b2349f67d48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "function make_string_hash() defined at 2022-06-01 18:40:49.642383\n"
     ]
    }
   ],
   "source": [
    "def make_string_hash( value_IN, hash_function_IN = hashlib.sha256 ):\n",
    "\n",
    "    # return reference\n",
    "    value_OUT = None\n",
    "\n",
    "    # declare variables\n",
    "    me = \"make_string_hash\"\n",
    "\n",
    "    # call StringHelper method.\n",
    "    value_OUT = StringHelper.make_string_hash( value_IN, hash_function_IN = hash_function_IN )\n",
    "\n",
    "    return value_OUT\n",
    "\n",
    "#-- END function make_string_hash() --#\n",
    "\n",
    "print( \"function make_string_hash() defined at \" + str( datetime.datetime.now() ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c03dbe0-2629-4761-907d-abc2af709dca",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Setup - base data spec\n",
    "\n",
    "Network data spec that includes:\n",
    "\n",
    "- `Article_Data` and `Person` queries the same...:\n",
    "\n",
    "    - _`coders` (`person_coders`)_: 2 (automated coder, id = 2)\n",
    "    - coder type \"OpenCalais_REST_API_v2\"\n",
    "    \n",
    "        - _`coder_type_filter_type` (`person_coder_type_filter_type`)_: \"automated\"\n",
    "        - _`coder_types_list` (`person_coder_types_list`)_: \"OpenCalais_REST_API_v2\"\n",
    "    \n",
    "    - _`publications` (`person_publications`)_: 1 (Grand Rapids Press)\n",
    "    - all dates in database (from 2005-01-01 to 2010-11-30)\n",
    "    \n",
    "        - _`start_date` (`person_start_date`)_: \"2005-01-01\"\n",
    "        - _`end_date` (`person_end_date`)_: \"2010-11-30\"\n",
    "    \n",
    "    - only articles tagged with `local_hard_news` and `coded-OpenCalaisV2ArticleCoder`.\n",
    "\n",
    "        - _`tags_list` (`person_tags_list`)_: \"local_hard_news,coded-OpenCalaisV2ArticleCoder\"\n",
    "\n",
    "- ...EXCEPT allowing duplicate articles for person so you get absolutely all persons, but not for `Article_Data` query.\n",
    "\n",
    "    - _`person_allow_duplicate_articles`_: \"yes\"\n",
    "\n",
    "- Network data creation options:\n",
    "\n",
    "    - excludes persons with single word (no spaces) `verbatim_name`.\n",
    "    \n",
    "        - _`include_persons_with_single_word_name`_: \"no\"\n",
    "    \n",
    "    - exclude render details\n",
    "        \n",
    "        - _`network_include_render_details`_: \"no\"\n",
    "        \n",
    "    - ouput as tab-delimited matrix, with node attributes as additional columns on the far right of the square network part of the matrix.\n",
    "\n",
    "        - _`output_type`_: \"tab_delimited_matrix\"\n",
    "        - _`network_data_output_type`_: \"net_and_attr_cols\"\n",
    "\n",
    "    - label - _`network_label`_: \"all_grp_hard_news\"\n",
    "    - include header row in the matrix output file.\n",
    "    \n",
    "        - _`network_include_headers`_: \"yes\"\n",
    "\n",
    "    - output spec plus the resulting network data to the database, with lable set to `network_label` plus a date-time string.\n",
    "    \n",
    "        - _`database_output`_: \"yes\",\n",
    "        - _`db_add_timestamp_to_label`_: \"yes\"\n",
    "\n",
    "_NOTE: only pass True to `network_outputter.process_network_output_request( debug_flag_IN )` if you really need to debug - it adds garbage data at the end of the output, even if you ask for no render details._\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "60ef24eb-3a73-4fc7-8b67-2979753889ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'start_date': '2005-01-01', 'end_date': '2005-12-31', 'date_range': '', 'publications': '1', 'coders': '2', 'coder_id_priority_list': '', 'coder_type_filter_type': 'automated', 'coder_types_list': 'OpenCalais_REST_API_v2', 'tags_list': 'local_hard_news', 'unique_identifiers': '', 'allow_duplicate_articles': 'no', 'person_query_type': 'custom', 'person_start_date': '2005-01-01', 'person_end_date': '2005-12-31', 'person_date_range': '', 'person_publications': '1', 'person_coders': '2', 'person_coder_id_priority_list': '', 'person_coder_type_filter_type': 'automated', 'person_coder_types_list': 'OpenCalais_REST_API_v2', 'person_tags_list': 'local_hard_news', 'person_unique_identifiers': '', 'person_allow_duplicate_articles': 'yes', 'include_source_contact_types': ['direct', 'event', 'past_quotes', 'document', 'other'], 'exclude_persons_with_tags_in_list': '', 'include_persons_with_single_word_name': 'no', 'network_download_as_file': 'no', 'network_include_render_details': 'no', 'output_type': 'tab_delimited_matrix', 'network_data_output_type': 'net_and_attr_cols', 'network_label': 'all_grp_hard_news_2005', 'network_include_headers': 'yes', 'database_output': 'yes', 'db_add_timestamp_to_label': 'yes', 'db_save_data_in_database': 'no', 'save_data_in_folder': '/tmp'}\n"
     ]
    }
   ],
   "source": [
    "base_data_spec_json_string = \"\"\"\n",
    "    \"start_date\": \"2005-01-01\",\n",
    "    \"end_date\": \"2005-12-31\",\n",
    "    \"date_range\": \"\",\n",
    "    \"publications\": \"1\",\n",
    "    \"coders\": \"2\",\n",
    "    \"coder_id_priority_list\": \"\",\n",
    "    \"coder_type_filter_type\": \"automated\",\n",
    "    \"coder_types_list\": \"OpenCalais_REST_API_v2\",\n",
    "    \"tags_list\": \"local_hard_news\",\n",
    "    \"unique_identifiers\": \"\",\n",
    "    \"allow_duplicate_articles\": \"no\",\n",
    "    \"person_query_type\": \"custom\",\n",
    "    \"person_start_date\": \"2005-01-01\",\n",
    "    \"person_end_date\": \"2005-12-31\",\n",
    "    \"person_date_range\": \"\",\n",
    "    \"person_publications\": \"1\",\n",
    "    \"person_coders\": \"2\",\n",
    "    \"person_coder_id_priority_list\": \"\",\n",
    "    \"person_coder_type_filter_type\": \"automated\",\n",
    "    \"person_coder_types_list\": \"OpenCalais_REST_API_v2\",\n",
    "    \"person_tags_list\": \"local_hard_news\",\n",
    "    \"person_unique_identifiers\": \"\",\n",
    "    \"person_allow_duplicate_articles\": \"yes\",\n",
    "    \"include_source_contact_types\": [\n",
    "        \"direct\",\n",
    "        \"event\",\n",
    "        \"past_quotes\",\n",
    "        \"document\",\n",
    "        \"other\"\n",
    "    ],\n",
    "    \"exclude_persons_with_tags_in_list\": \"\",\n",
    "    \"include_persons_with_single_word_name\": \"no\",\n",
    "    \"network_download_as_file\": \"no\",\n",
    "    \"network_include_render_details\": \"no\",\n",
    "    \"output_type\": \"tab_delimited_matrix\",\n",
    "    \"network_data_output_type\": \"net_and_attr_cols\",\n",
    "    \"network_label\": \"all_grp_hard_news_2005\",\n",
    "    \"network_include_headers\": \"yes\",\n",
    "    \"database_output\": \"yes\",\n",
    "    \"db_add_timestamp_to_label\": \"yes\",\n",
    "    \"db_save_data_in_database\": \"no\",\n",
    "    \"save_data_in_folder\": \"{output_folder_path}\"\n",
    "\"\"\"\n",
    "\n",
    "base_data_spec_json_string = base_data_spec_json_string.format(\n",
    "    output_folder_path = network_data_output_folder_path\n",
    ")\n",
    "base_data_spec_json_string = \"{left_curly}{json_properties}{right_curly}\".format(\n",
    "    left_curly = \"{\",\n",
    "    json_properties = base_data_spec_json_string,\n",
    "    right_curly = \"}\"\n",
    ")\n",
    "\n",
    "base_data_spec_json = json.loads( base_data_spec_json_string )\n",
    "print( base_data_spec_json ) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "045a0216-61d3-450e-b73a-9daf681b5407",
   "metadata": {},
   "source": [
    "### Setup - update base data spec for different time slices\n",
    "\n",
    "To update this for different time slices:\n",
    "\n",
    "- make a copy of `base_data_spec_json`:\n",
    "\n",
    "    - not threadsafe:\n",
    "    \n",
    "            my_timeslice_spec = copy.deepcopy( base_data_spec_json )\n",
    "    \n",
    "    - threadsafe (but doesn't handle complex data types - ours is just JSON, though, so fine here):\n",
    "    \n",
    "            my_timeslice_spec = json.loads( json.dumps( base_data_spec_json ) )\n",
    "\n",
    "- update the `start_date` and `end_date` to the period you want for your time slice.\n",
    "\n",
    "        my_timeslice_spec[ NetworkOutput.PARAM_START_DATE ] = \"2009-12-01\"\n",
    "        my_timeslice_spec[ NetworkOutput.PARAM_END_DATE ] = \"2009-12-31\"\n",
    "\n",
    "- update the `network_label` value so that it captures what time slice you are making.\n",
    "\n",
    "        my_timeslice_spec[ NetworkOutput.PARAM_NETWORK_LABEL ] = \"month-grp-automated-20091201-20091231\"\n",
    "\n",
    "    - example pattern: <type>-<paper>-<coder>-<start_date>-<end_date>\n",
    "    - examples:\n",
    "        \n",
    "            week-grp-automated-20050501-20050507\n",
    "            7day-grp-automated-20050502-20050508\n",
    "\n",
    "    - type would be either:\n",
    "\n",
    "        - actual time period:\n",
    "\n",
    "            - week\n",
    "            - month\n",
    "            - quarter\n",
    "            - half-year\n",
    "            - year\n",
    "\n",
    "        - conceptual time period:\n",
    "\n",
    "            - sliding week = \"7day\"\n",
    "            - sliding month = \"31day\"\n",
    "            - sliding quarter = \"92day\"\n",
    "            - sliding half-year = \"183day\"\n",
    "            - sliding year = \"365day\"\n",
    "\n",
    "_NOTE: leave person query parameters the same for all networks if you want all your network matrices to have same set of people (same count and position of rows and columns) so each network can be compared to all others, regardless of time period of a given network slice._"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f20460e6-825e-4fb4-a403-eb320537bedb",
   "metadata": {},
   "source": [
    "# network data output example - base data spec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0d270743-ddb6-40b9-b77b-83f4e21fbb1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> starting network creation at 2022-06-02 01:22:23.153878\n",
      "==> network creation complete at 2022-06-02 01:22:23.153878\n",
      "----> duration: 0:13:31.538857\n"
     ]
    }
   ],
   "source": [
    "# try creating network data.\n",
    "start_dt = datetime.datetime.now()\n",
    "print( \"==> starting network creation at {}\".format( start_dt ) )\n",
    "\n",
    "network_outputter = NetworkOutput()\n",
    "network_data = network_outputter.process_network_output_request(\n",
    "    params_IN = base_data_spec_json,\n",
    "    debug_flag_IN = False\n",
    ")\n",
    "\n",
    "end_dt = datetime.datetime.now()\n",
    "print( \"==> network creation complete at {}\".format( start_dt ) )\n",
    "\n",
    "# duration:\n",
    "my_duration = end_dt - start_dt\n",
    "print( \"----> duration: {}\".format( my_duration ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa7e6a5d-221b-459e-bf26-2dc96afdc494",
   "metadata": {},
   "source": [
    "- if include_persons_with_single_word_name = \"yes\": 2427606\n",
    "- if include_persons_with_single_word_name = \"no\": 2344545"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a765cbff-c2e9-4075-b0ee-7f9d351e0d46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a hash of the data, for comparison\n",
    "network_data_hash = make_string_hash( network_data )\n",
    "print( \"Network data hash: {}\".format( network_data_hash ) )\n",
    "\n",
    "# match?\n",
    "should_be = \"0f8a530f18a724b3d724d7fe9caa3082954c049abdc02b77bc480fc432d0a770\"\n",
    "if ( network_data_hash != should_be ):\n",
    "    \n",
    "    # not right hash. Error.\n",
    "    print( \"ERROR! network data hash is {}, should be {}\".format( network_data_hash, should_be ) )\n",
    "    \n",
    "else:\n",
    "    \n",
    "    # a match\n",
    "    print( \"MATCH - network data hash {} matches expected. hooray!\".format( network_data_hash ) )\n",
    "    \n",
    "#-- END debug/test --#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a35b214c-d25d-4278-a645-bacec4eed51c",
   "metadata": {},
   "outputs": [],
   "source": [
    "network_data_length = len( network_data )\n",
    "should_be = 11534\n",
    "print( \"Network data length: {}\".format( network_data_length ) )\n",
    "if ( network_data_length != should_be ):\n",
    "    \n",
    "    # not right length. Error.\n",
    "    print( \"ERROR! network data length is {}, should be {}\".format( network_data_length, should_be ) )\n",
    "    \n",
    "else:\n",
    "    \n",
    "    # a match\n",
    "    print( \"MATCH - string len()gth of {} matches expected. hooray!\".format( network_data_length ) )\n",
    "    \n",
    "#-- END debug/test --#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "793e0488-d275-4469-867f-f24a1c61b1b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# look at master person dict\n",
    "master_person_dict = network_outputter.create_person_dict( load_person_IN = True )\n",
    "\n",
    "# how many entries?\n",
    "person_count = len( master_person_dict )\n",
    "print( \"- person count: {person_count}\".format( person_count = person_count ) )\n",
    "\n",
    "# right number?\n",
    "should_be = 66\n",
    "if ( person_count != should_be ):\n",
    "    \n",
    "    # not right length. Error.\n",
    "    print( \"ERROR! person count is {}, should be {}\".format( person_count, should_be ) )\n",
    "    \n",
    "else:\n",
    "    \n",
    "    # a match\n",
    "    print( \"MATCH - person count of {} matches expected. hooray!\".format( person_count ) )\n",
    "    \n",
    "#-- END debug/test --#\n",
    "\n",
    "# the following persons should not be present\n",
    "find_person_list = list()\n",
    "\n",
    "# 1049, 752 (single names)\n",
    "find_person_list.append( 1049 )\n",
    "find_person_list.append( 752 )\n",
    "\n",
    "# 102, 224, 261 (tag `from_press_release`)\n",
    "find_person_list.append( 102 )\n",
    "find_person_list.append( 224 )\n",
    "find_person_list.append( 261 )\n",
    "\n",
    "# 187, 188, 189 (tag `godwin_heights`)\n",
    "find_person_list.append( 187 )\n",
    "find_person_list.append( 188 )\n",
    "find_person_list.append( 189 )\n",
    "\n",
    "# check for people who should have been removed.\n",
    "for find_person_id in find_person_list:\n",
    "\n",
    "    if ( find_person_id in master_person_dict ):\n",
    "    \n",
    "        print( \"ERROR - single-name person {} is in dictionary\".format( find_person_id ) )\n",
    "    \n",
    "    else:\n",
    "    \n",
    "        print( \"SUCCESS - single-name person {} not in dictionary\".format( find_person_id ) )\n",
    "    \n",
    "    #-- END check for person --#\n",
    "\n",
    "#-- END loop over persons to find. --#\n",
    "\n",
    "# output all persons.\n",
    "for person_id, person_instance in master_person_dict.items():\n",
    "    \n",
    "    print( \"\\n==> Person {person_id}: {person_instance}\".format( person_id = person_id, person_instance = person_instance ) )\n",
    "    \n",
    "#-- END loop over persons --#"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42cc27cf-4318-4042-8932-3e24915392bf",
   "metadata": {},
   "source": [
    "# write network data to file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82aada51-899c-476b-b564-4fd5ef4a7889",
   "metadata": {},
   "outputs": [],
   "source": [
    "# write the output to a file\n",
    "current_date_time = None\n",
    "my_file_extension = None\n",
    "network_data_file_path = None\n",
    "network_data_file = None\n",
    "\n",
    "# time stamp and file extension to append to file name\n",
    "current_date_time = datetime.datetime.now().strftime( '%Y%m%d-%H%M%S' )\n",
    "my_file_extension = \"txt\"\n",
    "\n",
    "# make file path.\n",
    "network_data_file_path = \"context_text_data-{timestamp}.{file_extension}\".format(\n",
    "    timestamp = current_date_time,\n",
    "    file_extension = my_file_extension\n",
    ")\n",
    "\n",
    "# write to file.\n",
    "with open( network_data_file_path, 'w' ) as network_data_file:\n",
    "\n",
    "    # output all the data to file.\n",
    "    network_data_file.write( network_data )\n",
    "    \n",
    "#-- END with open( network_data_file_path, 'w' ) as network_data_file --#\n",
    "\n",
    "print( \"network data written to file {} at {}\".format( network_data_file_path, datetime.datetime.now() ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff4e4c78-8c4e-4089-a0ee-9a4d1a0eabc5",
   "metadata": {},
   "source": [
    "# Explore data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "ae05edff-fbf5-461b-9e8b-2e12dc9c3047",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/tmp/all_grp_hard_news_2005-20220602-012223\n"
     ]
    }
   ],
   "source": [
    "# set data file path.\n",
    "data_file_name = \"all_grp_hard_news_2005-20220602-012223\"\n",
    "data_file_path = \"{output_folder_path}/{data_file_name}\".format(\n",
    "    output_folder_path = network_data_output_folder_path,\n",
    "    data_file_name = data_file_name\n",
    ")\n",
    "update_every_x = 1000\n",
    "\n",
    "print( data_file_path )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "f02901e8-0065-4ad0-801c-c707e04b3599",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----> finished processing 1000 of 50826 @ 2022-06-02 03:58:19.605687\n",
      "----> finished processing 2000 of 50826 @ 2022-06-02 03:58:19.606245\n",
      "----> finished processing 3000 of 50826 @ 2022-06-02 03:58:19.606605\n",
      "----> finished processing 4000 of 50826 @ 2022-06-02 03:58:19.606950\n",
      "----> finished processing 5000 of 50826 @ 2022-06-02 03:58:19.607343\n",
      "----> finished processing 6000 of 50826 @ 2022-06-02 03:58:19.607777\n",
      "----> finished processing 7000 of 50826 @ 2022-06-02 03:58:19.608136\n",
      "----> finished processing 8000 of 50826 @ 2022-06-02 03:58:19.608495\n",
      "----> finished processing 9000 of 50826 @ 2022-06-02 03:58:19.608850\n",
      "----> finished processing 10000 of 50826 @ 2022-06-02 03:58:19.609200\n",
      "----> finished processing 11000 of 50826 @ 2022-06-02 03:58:19.609554\n",
      "----> finished processing 12000 of 50826 @ 2022-06-02 03:58:19.609913\n",
      "----> finished processing 13000 of 50826 @ 2022-06-02 03:58:19.610270\n",
      "----> finished processing 14000 of 50826 @ 2022-06-02 03:58:19.610617\n",
      "----> finished processing 15000 of 50826 @ 2022-06-02 03:58:19.610957\n",
      "----> finished processing 16000 of 50826 @ 2022-06-02 03:58:19.611309\n",
      "----> finished processing 17000 of 50826 @ 2022-06-02 03:58:19.611657\n",
      "----> finished processing 18000 of 50826 @ 2022-06-02 03:58:19.611998\n",
      "----> finished processing 19000 of 50826 @ 2022-06-02 03:58:19.612351\n",
      "----> finished processing 20000 of 50826 @ 2022-06-02 03:58:19.612703\n",
      "----> finished processing 21000 of 50826 @ 2022-06-02 03:58:19.613270\n",
      "----> finished processing 22000 of 50826 @ 2022-06-02 03:58:19.613724\n",
      "----> finished processing 23000 of 50826 @ 2022-06-02 03:58:19.614124\n",
      "----> finished processing 24000 of 50826 @ 2022-06-02 03:58:19.614487\n",
      "----> finished processing 25000 of 50826 @ 2022-06-02 03:58:19.614835\n",
      "----> finished processing 26000 of 50826 @ 2022-06-02 03:58:19.615178\n",
      "----> finished processing 27000 of 50826 @ 2022-06-02 03:58:19.615534\n",
      "----> finished processing 28000 of 50826 @ 2022-06-02 03:58:19.615881\n",
      "----> finished processing 29000 of 50826 @ 2022-06-02 03:58:19.616535\n",
      "----> finished processing 30000 of 50826 @ 2022-06-02 03:58:19.616895\n",
      "----> finished processing 31000 of 50826 @ 2022-06-02 03:58:19.617239\n",
      "----> finished processing 32000 of 50826 @ 2022-06-02 03:58:19.617595\n",
      "----> finished processing 33000 of 50826 @ 2022-06-02 03:58:19.617963\n",
      "----> finished processing 34000 of 50826 @ 2022-06-02 03:58:19.618319\n",
      "----> finished processing 35000 of 50826 @ 2022-06-02 03:58:19.618666\n",
      "----> finished processing 36000 of 50826 @ 2022-06-02 03:58:19.619005\n",
      "----> finished processing 37000 of 50826 @ 2022-06-02 03:58:19.619356\n",
      "----> finished processing 38000 of 50826 @ 2022-06-02 03:58:19.619702\n",
      "----> finished processing 39000 of 50826 @ 2022-06-02 03:58:19.620042\n",
      "----> finished processing 40000 of 50826 @ 2022-06-02 03:58:19.620402\n",
      "----> finished processing 41000 of 50826 @ 2022-06-02 03:58:19.620747\n",
      "----> finished processing 42000 of 50826 @ 2022-06-02 03:58:19.621088\n",
      "----> finished processing 43000 of 50826 @ 2022-06-02 03:58:19.621439\n",
      "----> finished processing 44000 of 50826 @ 2022-06-02 03:58:19.621792\n",
      "----> finished processing 45000 of 50826 @ 2022-06-02 03:58:19.622129\n",
      "----> finished processing 46000 of 50826 @ 2022-06-02 03:58:19.622481\n",
      "----> finished processing 47000 of 50826 @ 2022-06-02 03:58:19.622826\n",
      "----> finished processing 48000 of 50826 @ 2022-06-02 03:58:19.623157\n",
      "----> finished processing 49000 of 50826 @ 2022-06-02 03:58:19.623632\n",
      "----> finished processing 50000 of 50826 @ 2022-06-02 03:58:19.624054\n",
      "\n",
      "\n",
      "Finished processing 50826 header column names:\n",
      "- counter_unknown = 37069\n",
      "- counter_author = 197\n",
      "- counter_source = 13540\n",
      "- counter_both = 17\n"
     ]
    }
   ],
   "source": [
    "# declare variables\n",
    "data_file = None\n",
    "data_file_reader = None\n",
    "data_file_line = None\n",
    "data_file_line_item_list = None\n",
    "person_info = None\n",
    "person_info_count = None\n",
    "person_info_counter = None\n",
    "person_info_lower = None\n",
    "counter_unknown = None\n",
    "counter_author = None\n",
    "counter_source = None\n",
    "counter_both = None\n",
    "update_every_x = 1000\n",
    "\n",
    "# Open network data output file for reading.\n",
    "with open( data_file_path, \"r\" ) as data_file:\n",
    "    \n",
    "    # csv.reader\n",
    "    #data_file_reader = csv.reader( data_file, delimiter=':', quoting=csv.QUOTE_NONE )\n",
    "    \n",
    "    # read first line.\n",
    "    data_file_line = data_file.readline()\n",
    "\n",
    "    # split on tabs.\n",
    "    data_file_line_item_list = data_file_line.split( \"\\t\" )\n",
    "    \n",
    "#-- END with open( data_file_path, \"r\" ) as data_file_name: --#\n",
    "\n",
    "person_info_count = len( data_file_line_item_list )\n",
    "person_info_counter = 0\n",
    "\n",
    "# loop and add up different person types.\n",
    "counter_unknown = 0\n",
    "counter_author = 0\n",
    "counter_source = 0\n",
    "counter_both = 0\n",
    "for person_info in data_file_line_item_list:\n",
    "    \n",
    "    # increment counter\n",
    "    person_info_counter += 1\n",
    "    \n",
    "    # does string contain...\n",
    "    person_info_lower = person_info.lower()\n",
    "    \n",
    "    # ==> \"unknown\"\n",
    "    if ( \"unknown\" in person_info_lower ):\n",
    "        \n",
    "        counter_unknown += 1\n",
    "        \n",
    "    #== END check if unknown --#\n",
    "\n",
    "    # ==> \"author\"\n",
    "    if ( \"author\" in person_info_lower ):\n",
    "\n",
    "        counter_author += 1\n",
    "\n",
    "    #== END check if author --#\n",
    "\n",
    "    # ==> \"source\"\n",
    "    if ( \"source\" in person_info_lower ):\n",
    "\n",
    "        counter_source += 1\n",
    "\n",
    "    #== END check if source --#\n",
    "        \n",
    "    # ==> \"both\"\n",
    "    if ( \"both\" in person_info_lower ):\n",
    "    \n",
    "        counter_both += 1\n",
    "    \n",
    "    #== END check if both --#\n",
    "        \n",
    "    # time to give brief update?\n",
    "    if ( ( person_info_counter % update_every_x ) == 0 ):\n",
    "        \n",
    "        # yes.\n",
    "        status_message = \"----> finished processing {counter} of {total} @ {my_timestamp}\".format(\n",
    "            counter = person_info_counter,\n",
    "            total = person_info_count,\n",
    "            my_timestamp = datetime.datetime.now()\n",
    "        )\n",
    "        print( status_message )\n",
    "        \n",
    "    #-- END check if update time. --#\n",
    "\n",
    "# END loop over header line of data file. --#\n",
    "\n",
    "print( \"\\n\" )\n",
    "print( \"Finished processing {record_count} header column names:\".format( record_count = person_info_counter ) )\n",
    "print( \"- counter_unknown = {}\".format( counter_unknown ) )\n",
    "print( \"- counter_author = {}\".format( counter_author ) )\n",
    "print( \"- counter_source = {}\".format( counter_source ) )\n",
    "print( \"- counter_both = {}\".format( counter_both ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "d16042d3-3cd8-4ffb-a787-42052f66311f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----> finished processing 1000 @ 2022-06-02 04:47:31.804961\n",
      "----> finished processing 2000 @ 2022-06-02 04:47:48.413077\n",
      "----> finished processing 3000 @ 2022-06-02 04:48:05.196557\n",
      "----> finished processing 4000 @ 2022-06-02 04:48:21.790662\n",
      "----> finished processing 5000 @ 2022-06-02 04:48:38.028474\n",
      "----> finished processing 6000 @ 2022-06-02 04:48:54.599644\n",
      "----> finished processing 7000 @ 2022-06-02 04:49:11.442007\n",
      "----> finished processing 8000 @ 2022-06-02 04:49:28.300056\n",
      "----> finished processing 9000 @ 2022-06-02 04:49:44.722230\n",
      "----> finished processing 10000 @ 2022-06-02 04:50:01.366893\n",
      "----> finished processing 11000 @ 2022-06-02 04:50:18.256683\n",
      "----> finished processing 12000 @ 2022-06-02 04:50:35.311835\n",
      "----> finished processing 13000 @ 2022-06-02 04:50:52.288272\n",
      "----> finished processing 14000 @ 2022-06-02 04:51:08.947850\n",
      "----> finished processing 15000 @ 2022-06-02 04:51:25.640168\n",
      "----> finished processing 16000 @ 2022-06-02 04:51:42.697295\n",
      "----> finished processing 17000 @ 2022-06-02 04:51:59.942590\n",
      "----> finished processing 18000 @ 2022-06-02 04:52:17.071250\n",
      "----> finished processing 19000 @ 2022-06-02 04:52:34.010772\n",
      "----> finished processing 20000 @ 2022-06-02 04:52:50.797600\n",
      "----> finished processing 21000 @ 2022-06-02 04:53:07.510704\n",
      "----> finished processing 22000 @ 2022-06-02 04:53:24.637548\n",
      "----> finished processing 23000 @ 2022-06-02 04:53:41.810679\n",
      "----> finished processing 24000 @ 2022-06-02 04:53:59.158985\n",
      "----> finished processing 25000 @ 2022-06-02 04:54:16.378707\n",
      "----> finished processing 26000 @ 2022-06-02 04:54:33.923516\n",
      "----> finished processing 27000 @ 2022-06-02 04:54:51.701021\n",
      "----> finished processing 28000 @ 2022-06-02 04:55:09.117647\n",
      "----> finished processing 29000 @ 2022-06-02 04:55:26.447680\n",
      "----> finished processing 30000 @ 2022-06-02 04:55:43.664721\n",
      "----> finished processing 31000 @ 2022-06-02 04:56:00.722186\n",
      "----> finished processing 32000 @ 2022-06-02 04:56:17.895337\n",
      "----> finished processing 33000 @ 2022-06-02 04:56:34.953761\n",
      "----> finished processing 34000 @ 2022-06-02 04:56:52.232492\n",
      "----> finished processing 35000 @ 2022-06-02 04:57:09.371553\n",
      "----> finished processing 36000 @ 2022-06-02 04:57:26.523473\n",
      "----> finished processing 37000 @ 2022-06-02 04:57:43.783273\n",
      "----> finished processing 38000 @ 2022-06-02 04:58:01.078425\n",
      "----> finished processing 39000 @ 2022-06-02 04:58:18.772898\n",
      "----> finished processing 40000 @ 2022-06-02 04:58:36.256899\n",
      "----> finished processing 41000 @ 2022-06-02 04:58:53.171758\n",
      "----> finished processing 42000 @ 2022-06-02 04:59:10.208942\n",
      "----> finished processing 43000 @ 2022-06-02 04:59:27.338985\n",
      "----> finished processing 44000 @ 2022-06-02 04:59:44.334815\n",
      "----> finished processing 45000 @ 2022-06-02 05:00:01.468789\n",
      "----> finished processing 46000 @ 2022-06-02 05:00:19.306062\n",
      "----> finished processing 47000 @ 2022-06-02 05:00:36.869313\n",
      "----> finished processing 48000 @ 2022-06-02 05:00:54.570358\n",
      "----> finished processing 49000 @ 2022-06-02 05:01:12.873065\n",
      "----> finished processing 50000 @ 2022-06-02 05:01:31.192095\n",
      "\n",
      "\n",
      "Finished processing 50823 rows:\n",
      "- counter_tie = 137547\n",
      "-----> sum_weight = 2077935942\n",
      "- counter_zero = 2582941428\n",
      "- counter_negative = 0\n",
      "- counter_other = 50823\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'counter_empty' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [49]\u001b[0m, in \u001b[0;36m<cell line: 118>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    116\u001b[0m \u001b[38;5;28mprint\u001b[39m( \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m- counter_negative = \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat( counter_negative ) )\n\u001b[1;32m    117\u001b[0m \u001b[38;5;28mprint\u001b[39m( \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m- counter_other = \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat( counter_other ) )\n\u001b[0;32m--> 118\u001b[0m \u001b[38;5;28mprint\u001b[39m( \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m- counter_empty = \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat( \u001b[43mcounter_empty\u001b[49m ) )\n",
      "\u001b[0;31mNameError\u001b[0m: name 'counter_empty' is not defined"
     ]
    }
   ],
   "source": [
    "# declare variables\n",
    "data_file = None\n",
    "data_file_reader = None\n",
    "data_file_line = None\n",
    "data_file_line_item_list = None\n",
    "data_file_item = None\n",
    "data_file_item_value = None\n",
    "counter_tie = None\n",
    "sum_weight = None\n",
    "counter_zero = None\n",
    "counter_negative = None\n",
    "counter_other = None\n",
    "counter_empty = None\n",
    "update_every_x = None\n",
    "row_counter = None\n",
    "column_counter = None\n",
    "\n",
    "# Open network data output file for reading.\n",
    "counter_tie = 0\n",
    "sum_weight = 0\n",
    "counter_zero = 0\n",
    "counter_negative = 0\n",
    "counter_other = 0\n",
    "counter_empty = 0\n",
    "update_every_x = 1000\n",
    "with open( data_file_path, \"r\" ) as data_file:\n",
    "    \n",
    "    # csv.reader\n",
    "    #data_file_reader = csv.reader( data_file, delimiter=':', quoting=csv.QUOTE_NONE )\n",
    "    \n",
    "    # try to move past header first line.\n",
    "    data_file_line = data_file.readline()\n",
    "\n",
    "    # loop over lines in file\n",
    "    row_counter = 0\n",
    "    for data_file_line in data_file:\n",
    "    \n",
    "        row_counter += 1\n",
    "    \n",
    "        # split on tabs.\n",
    "        data_file_line_item_list = data_file_line.split( \"\\t\" )\n",
    "        \n",
    "        # then, loop over items in list. For each, if not empty and not 0,\n",
    "        #     add 1 to counter of empty cells and add number to weight-aggregator.\n",
    "        column_counter = 0\n",
    "        for data_file_item in data_file_line_item_list:\n",
    "            \n",
    "            column_counter += 1\n",
    "            \n",
    "            # is it a number > 0?\n",
    "            if (\n",
    "                ( data_file_item is not None )\n",
    "                and ( data_file_item != \"\" )\n",
    "            ):\n",
    "                \n",
    "                # try to cast to int.\n",
    "                try:\n",
    "                    \n",
    "                    # cast to int\n",
    "                    data_file_item_value = int( data_file_item )\n",
    "                    \n",
    "                    # an int! > 0?\n",
    "                    if ( data_file_item_value > 0 ):\n",
    "                   \n",
    "                        # yes! an actual tie\n",
    "                        counter_tie += 1\n",
    "                        sum_weight += data_file_item_value\n",
    "                        \n",
    "                    elif ( data_file_item_value == 0 ): \n",
    "                        \n",
    "                        # no tie - increment counter_zero\n",
    "                        counter_zero += 1\n",
    "                        \n",
    "                    else:\n",
    "                        \n",
    "                        # neither 0 or greater than 0...\n",
    "                        counter_negative += 1\n",
    "                        \n",
    "                    #-- END check what is in int... --#\n",
    "                    \n",
    "                except:\n",
    "            \n",
    "                    # either string (or something else).\n",
    "                    counter_other += 1\n",
    "\n",
    "                #-- END try...except --#\n",
    "                \n",
    "            else:\n",
    "                \n",
    "                # empty...?\n",
    "                counter_empty += 1\n",
    "                \n",
    "            #-- END check if None or \"\" --#\n",
    "            \n",
    "        #-- END loop over items in line --#\n",
    "\n",
    "        # time to give brief update?\n",
    "        if ( ( row_counter % update_every_x ) == 0 ):\n",
    "\n",
    "            # yes.\n",
    "            status_message = \"----> finished processing {row_counter} rows @ {my_timestamp}\".format(\n",
    "                row_counter = row_counter,\n",
    "                my_timestamp = datetime.datetime.now()\n",
    "            )\n",
    "            print( status_message )\n",
    "\n",
    "        #-- END check if update time. --#\n",
    "          \n",
    "    #--- END loop over lines in file. --#\n",
    "    \n",
    "#-- END with open( data_file_path, \"r\" ) as data_file_name: --#\n",
    "\n",
    "print( \"\\n\" )\n",
    "print( \"Finished processing {row_counter} rows:\".format( row_counter = row_counter ) )\n",
    "print( \"- counter_tie = {}\".format( counter_tie ) )\n",
    "print( \"-----> sum_weight = {}\".format( sum_weight ) )\n",
    "print( \"- counter_zero = {}\".format( counter_zero ) )\n",
    "print( \"- counter_negative = {}\".format( counter_negative ) )\n",
    "print( \"- counter_other = {}\".format( counter_other ) )\n",
    "print( \"- counter_empty = {}\".format( counter_empty ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9223786-5f6b-45e9-973a-327457c81e58",
   "metadata": {},
   "source": [
    "Finished processing 50823 rows:\n",
    "- counter_tie = 137547\n",
    "- -----> sum_weight = 2077935942\n",
    "- counter_zero = 2582941428\n",
    "- counter_negative = 0\n",
    "- counter_other = 50823"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "research_virtualenv",
   "language": "python",
   "name": "research"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "toc-autonumbering": true
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
