{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a62fac68-0a57-47c6-b6cc-e62a02385a47",
   "metadata": {},
   "source": [
    "**analysis-network_data_output-GRP.ipynb - Programmatic network data output**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e7065e4-eddb-4319-bc23-f3a4e04e39ef",
   "metadata": {},
   "source": [
    "# Notes\n",
    "\n",
    "- for year comparisons, try to only do people for the years being compared, not all time (all time creates really large matrices).\n",
    "- to start, do matrices for year before and year after layoffs, with the person query including both years.\n",
    "\n",
    "    - store in the vm-share folder, mounted in VM.\n",
    "    - see how large they are.\n",
    "    - then, try loading them in R and doing a correlation. If we can make it work with relatively giant matrices on glassbox, great. if not, will need to try edge lists (I bet the real network packages don't store the whole graph, they just store the edge list and a list of the nodes).\n",
    "    \n",
    "- Need to go to edge lists - then, same edge list can be used with different node lists as long as all nodes are in node list.\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba4ccabf-42d0-4217-8baf-87bd790d8478",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "260fc288-1cd1-4af0-84f6-1e97011defdc",
   "metadata": {},
   "source": [
    "## Setup - Debug\n",
    "\n",
    "- Back to [Table of Contents](#Table-of-Contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a6a3ed9-96c2-441d-a443-371200c275f0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-17T15:43:45.678275Z",
     "start_time": "2019-09-17T15:43:45.673730Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "debug_flag = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "089876e6-2d2f-4fd0-96f6-730520fb0bab",
   "metadata": {},
   "source": [
    "## Setup - Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47c3f417-8fcb-49c1-85f3-44c070e980fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# python base imports\n",
    "import copy\n",
    "import csv\n",
    "import datetime\n",
    "import hashlib\n",
    "import json\n",
    "import logging\n",
    "\n",
    "# import six\n",
    "import six\n",
    "\n",
    "print( \"packages imported at \" + str( datetime.datetime.now() ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "105b958b-0102-4978-9132-ab0af2cd81c2",
   "metadata": {},
   "source": [
    "## Setup - working folder paths\n",
    "\n",
    "- Back to [Table of Contents](#Table-of-Contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47541bda-424d-4bf7-8897-3e2cc19acefc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-17T15:43:47.305614Z",
     "start_time": "2019-09-17T15:43:47.286949Z"
    }
   },
   "outputs": [],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "324f5746-3fe3-49bc-ba43-f38602b06bba",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-17T15:43:48.159415Z",
     "start_time": "2019-09-17T15:43:48.153181Z"
    }
   },
   "outputs": [],
   "source": [
    "# current working folder\n",
    "project_name = \"research\"\n",
    "project_base_folder = \"/home/jonathanmorgan/work/django/{project_name}\".format( project_name = project_name )\n",
    "django_project_folder = \"{base_folder}/{project_name}\".format(\n",
    "    base_folder = project_base_folder,\n",
    "    project_name = project_name\n",
    ")\n",
    "current_working_folder = \"{django_project_folder}/work/phd_work/analysis/network_data\".format(\n",
    "    django_project_folder = django_project_folder\n",
    ")\n",
    "current_datetime = datetime.datetime.now()\n",
    "current_date_string = current_datetime.strftime( \"%Y-%m-%d-%H-%M-%S\" )\n",
    "\n",
    "# and, output path.\n",
    "#network_data_output_folder_path = \"/media/psf/phd_work/network_data\"\n",
    "network_data_output_folder_path = \"/home/jonathanmorgan/shares/phd_work/network_data\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "206e3216-5791-4be6-aa90-ff0917a1b486",
   "metadata": {},
   "source": [
    "## Setup - logging\n",
    "\n",
    "- Back to [Table of Contents](#Table-of-Contents)\n",
    "\n",
    "configure logging for this notebook's kernel (If you do not run this cell, you'll get the django application's logging configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9c7beff-b869-464c-9f82-66666ac46c69",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-17T15:43:49.038050Z",
     "start_time": "2019-09-17T15:43:49.031535Z"
    }
   },
   "outputs": [],
   "source": [
    "# build file name\n",
    "project_log_folder = \"{base_folder}/logs\".format( base_folder = project_base_folder )\n",
    "logging_file_name = \"{}/network_data_output-GRP-{}.log.txt\".format( project_log_folder, current_date_string )\n",
    "\n",
    "# set up logging.\n",
    "logging.basicConfig(\n",
    "    level = logging.DEBUG,\n",
    "    format = '%(asctime)s - %(levelname)s - %(name)s - %(message)s',\n",
    "    filename = logging_file_name,\n",
    "    filemode = 'w' # set to 'a' if you want to append, rather than overwrite each time.\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17939321-ff9b-4230-84ef-5165d0d8820a",
   "metadata": {},
   "source": [
    "## Setup - Initialize Django\n",
    "\n",
    "- Back to [Table of Contents](#Table-of-Contents)\n",
    "\n",
    "First, initialize my dev django project, so I can run code in this notebook that references my django models and can talk to the database using my project's settings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daabb30f-9129-4d0a-85a2-f1b32a522275",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-17T15:43:52.077172Z",
     "start_time": "2019-09-17T15:43:52.071107Z"
    }
   },
   "outputs": [],
   "source": [
    "# init django\n",
    "django_init_folder = \"{django_project_folder}/work/phd_work\".format(\n",
    "    django_project_folder = django_project_folder\n",
    ")\n",
    "django_init_path = \"django_init.py\"\n",
    "if( ( django_init_folder is not None ) and ( django_init_folder != \"\" ) ):\n",
    "    \n",
    "    # add folder to front of path.\n",
    "    django_init_path = \"{}/{}\".format( django_init_folder, django_init_path )\n",
    "    \n",
    "#-- END check to see if django_init folder. --#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d9e1ebf-ca96-45ed-8a3f-bfd418516e41",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-17T15:43:54.453200Z",
     "start_time": "2019-09-17T15:43:52.833671Z"
    }
   },
   "outputs": [],
   "source": [
    "%run $django_init_path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09170da9-c45c-4a93-8b0a-765ce4d9a790",
   "metadata": {},
   "source": [
    "### Setup - django-related imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c4c9544-0d64-4cf1-baca-f1226617dec1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# python utilities\n",
    "from python_utilities.strings.string_helper import StringHelper\n",
    "\n",
    "# import class that actually processes requests for outputting networks.\n",
    "from context_text.export.network_output import NetworkOutput\n",
    "\n",
    "print( \"django model packages imported at \" + str( datetime.datetime.now() ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "858984de-3b03-4475-9fbd-ee10f9c193e3",
   "metadata": {},
   "source": [
    "## Setup - functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1b94cfc-259a-4ffd-84e8-0848379f1e84",
   "metadata": {},
   "source": [
    "### Setup - function `make_string_hash()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "008b7d86-675e-497e-ac35-8b2349f67d48",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_string_hash( value_IN, hash_function_IN = hashlib.sha256 ):\n",
    "\n",
    "    # return reference\n",
    "    value_OUT = None\n",
    "\n",
    "    # declare variables\n",
    "    me = \"make_string_hash\"\n",
    "\n",
    "    # call StringHelper method.\n",
    "    value_OUT = StringHelper.make_string_hash( value_IN, hash_function_IN = hash_function_IN )\n",
    "\n",
    "    return value_OUT\n",
    "\n",
    "#-- END function make_string_hash() --#\n",
    "\n",
    "print( \"function make_string_hash() defined at \" + str( datetime.datetime.now() ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "197feb49-4b6e-4a2a-b6df-5574f61c2e85",
   "metadata": {},
   "source": [
    "### Setup - function `create_pre_post_network_pairs()`\n",
    "\n",
    "Accepts...\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a69821f4-f374-497a-99a8-d6ad6f7e5428",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_pre_post_network_pairs(\n",
    "    data_spec_IN = None,\n",
    "    network_timedelta_IN = None,\n",
    "    increment_timedelta_IN = None,\n",
    "    label_prefix_IN = None,\n",
    "    output_folder_path_IN = None,\n",
    "    do_create_IN = False,\n",
    "    debug_flag_IN = False,\n",
    "    network_outputter_IN = None\n",
    "):\n",
    "    \n",
    "    # return reference\n",
    "    outputter_OUT = None\n",
    "    \n",
    "    # declare variables\n",
    "    me = \"create_pre_post_network_pairs\"\n",
    "    status_message = None\n",
    "    my_debug_flag = None\n",
    "    start_base_date = None\n",
    "    end_base_date = None\n",
    "    my_data_spec_json = None\n",
    "    do_create = None\n",
    "    network_outputter = None\n",
    "\n",
    "    # program control\n",
    "    network_timedelta = None\n",
    "    increment_timedelta = None  # how much time between base dates where we measure?\n",
    "    output_folder_path = None\n",
    "\n",
    "    # declare variables - loop processing.\n",
    "    base_date = None\n",
    "    time_period_index = None\n",
    "    pre_start_date = None\n",
    "    pre_end_date = None\n",
    "    pre_label = None\n",
    "    post_start_date = None\n",
    "    post_end_date = None\n",
    "    post_label = None\n",
    "    label_prefix = None\n",
    "\n",
    "    #--------------------------------------------------------------------------#\n",
    "    # initialize\n",
    "    \n",
    "    # init - from params\n",
    "    my_data_spec_json = copy.deepcopy( data_spec_IN )\n",
    "    network_timedelta = network_timedelta_IN\n",
    "    increment_timedelta = increment_timedelta_IN\n",
    "    label_prefix = label_prefix_IN\n",
    "    output_folder_path = output_folder_path_IN\n",
    "    do_create = do_create_IN\n",
    "    my_debug_flag = debug_flag_IN\n",
    "    network_outputter = network_outputter_IN\n",
    "    \n",
    "    # got a network outputter?\n",
    "    if ( network_outputter is None ):\n",
    "        \n",
    "        # create one.\n",
    "        network_outputter = NetworkOutput()\n",
    "        \n",
    "    #-- END check if network outputter --#\n",
    "\n",
    "    # init - length of network slices, and start and end base dates.\n",
    "    start_base_date = first_article_date + network_timedelta\n",
    "    #start_base_date = start_base_date + one_day_delta\n",
    "    end_base_date = last_article_date - network_timedelta\n",
    "    end_base_date = end_base_date + one_day_delta\n",
    "    \n",
    "    # set output folder\n",
    "    my_data_spec_json[ NetworkOutput.PARAM_NAME_SAVE_DATA_IN_FOLDER ] = output_folder_path\n",
    "\n",
    "    print( \"processing base dates from {} to {}\".format( start_base_date, end_base_date ) )\n",
    "\n",
    "    #--------------------------------------------------------------------------#\n",
    "    # loop over base dates, creating previous year and current year matrices for each.\n",
    "    base_date = start_base_date\n",
    "    #end_base_date = start_base_date\n",
    "    time_period_index = 0\n",
    "    while base_date <= end_base_date:\n",
    "\n",
    "        # increment index\n",
    "        time_period_index += 1\n",
    "\n",
    "        # set up start and end dates for pre- and post-networks.\n",
    "        pre_start_date = base_date - network_timedelta\n",
    "        pre_end_date = base_date - one_day_delta\n",
    "        post_start_date = base_date\n",
    "        post_end_date = base_date + network_timedelta - one_day_delta\n",
    "        status_message = \"==> current time range ( {current_index} ): {pre_start} - {pre_end}; {post_start} - {post_end}\".format(\n",
    "            current_index = time_period_index,\n",
    "            pre_start = pre_start_date,\n",
    "            pre_end = pre_end_date,\n",
    "            post_start = post_start_date,\n",
    "            post_end = post_end_date\n",
    "        )\n",
    "        print( status_message )\n",
    "\n",
    "        if ( do_create == True ):\n",
    "\n",
    "            # create label prefix\n",
    "            label_prefix = \"{prefix}_{base_date}_\".format(\n",
    "                prefix = label_prefix,\n",
    "                base_date = base_date\n",
    "            )\n",
    "            pre_label = \"{}pre\".format( label_prefix )\n",
    "            post_label = \"{}post\".format( label_prefix )\n",
    "\n",
    "            # set person query start and end dates.\n",
    "            my_data_spec_json[ NetworkOutput.PARAM_PERSON_START_DATE ] = pre_start_date.isoformat()\n",
    "            my_data_spec_json[ NetworkOutput.PARAM_PERSON_END_DATE ] = post_end_date.isoformat()\n",
    "\n",
    "            #------------------------------------------------------------------#\n",
    "            # ==> pre\n",
    "\n",
    "            # update data creation spec.\n",
    "            my_data_spec_json[ NetworkOutput.PARAM_START_DATE ] = pre_start_date.isoformat()\n",
    "            my_data_spec_json[ NetworkOutput.PARAM_END_DATE ] = pre_end_date.isoformat()\n",
    "            my_data_spec_json[ NetworkOutput.PARAM_NETWORK_LABEL ] = pre_label\n",
    "\n",
    "            # make and output pre-network\n",
    "            start_dt = datetime.datetime.now()\n",
    "            \n",
    "            if ( my_debug_flag == True ):\n",
    "                print( \"----> pre - starting network creation at {}\".format( start_dt ) )\n",
    "            #-- END DEBUG --#\n",
    "\n",
    "            network_outputter = NetworkOutput()\n",
    "            network_data = network_outputter.process_network_output_request(\n",
    "                params_IN = my_data_spec_json,\n",
    "                debug_flag_IN = False\n",
    "            )\n",
    "\n",
    "            # end time and duration\n",
    "            end_dt = datetime.datetime.now()\n",
    "            my_duration = end_dt - start_dt\n",
    "            \n",
    "            if ( my_debug_flag == True ):\n",
    "                print( \"----> pre - network creation complete at {}\".format( end_dt ) )\n",
    "                print( \"--------> pre - duration: {}\".format( my_duration ) )\n",
    "            #-- END DEBUG --#\n",
    "\n",
    "            #------------------------------------------------------------------#\n",
    "            # ==> post\n",
    "\n",
    "            # update data creation spec.\n",
    "            my_data_spec_json[ NetworkOutput.PARAM_START_DATE ] = post_start_date.isoformat()\n",
    "            my_data_spec_json[ NetworkOutput.PARAM_END_DATE ] = post_end_date.isoformat()\n",
    "            my_data_spec_json[ NetworkOutput.PARAM_NETWORK_LABEL ] = post_label\n",
    "\n",
    "            # make and output post-network\n",
    "            start_dt = datetime.datetime.now()\n",
    "\n",
    "            if ( my_debug_flag == True ):\n",
    "                print( \"----> post - starting network creation at {}\".format( start_dt ) )\n",
    "            #-- END DEBUG --#\n",
    "\n",
    "            network_outputter = NetworkOutput()\n",
    "            network_data = network_outputter.process_network_output_request(\n",
    "                params_IN = my_data_spec_json,\n",
    "                debug_flag_IN = False\n",
    "            )\n",
    "\n",
    "            # end time and duration\n",
    "            end_dt = datetime.datetime.now()\n",
    "            my_duration = end_dt - start_dt\n",
    "            \n",
    "            if ( my_debug_flag == True ):\n",
    "                print( \"----> post - network creation complete at {}\".format( end_dt ) )\n",
    "                print( \"--------> post - duration: {}\".format( my_duration ) )\n",
    "            #-- END DEBUG --#\n",
    "\n",
    "        #-- END check if we actually do the work --#\n",
    "\n",
    "        # increment base date before starting loop again.\n",
    "        #print( \"increment_time_delta: {}\".format( increment_timedelta ) )\n",
    "        base_date = base_date + increment_timedelta\n",
    "\n",
    "    #-- END loop over base dates --#\n",
    "    \n",
    "    outputter_OUT = network_outputter\n",
    "    \n",
    "    return outputter_OUT\n",
    "    \n",
    "#-- END function create_pre_post_network_pairs --#\n",
    "\n",
    "print( \"function create_pre_post_network_pairs() defined at \" + str( datetime.datetime.now() ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c03dbe0-2629-4761-907d-abc2af709dca",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Setup - base data spec\n",
    "\n",
    "Network data spec that includes:\n",
    "\n",
    "- `Article_Data` and `Person` queries the same...:\n",
    "\n",
    "    - _`coders` (`person_coders`)_: 2 (automated coder, id = 2)\n",
    "    - coder type \"OpenCalais_REST_API_v2\"\n",
    "    \n",
    "        - _`coder_type_filter_type` (`person_coder_type_filter_type`)_: \"automated\"\n",
    "        - _`coder_types_list` (`person_coder_types_list`)_: \"OpenCalais_REST_API_v2\"\n",
    "    \n",
    "    - _`publications` (`person_publications`)_: 1 (Grand Rapids Press)\n",
    "    - all dates in database (from 2005-01-01 to 2010-11-30)\n",
    "    \n",
    "        - _`start_date` (`person_start_date`)_: \"2005-01-01\"\n",
    "        - _`end_date` (`person_end_date`)_: \"2010-11-30\"\n",
    "    \n",
    "    - only articles tagged with `local_hard_news` and `coded-OpenCalaisV2ArticleCoder`.\n",
    "\n",
    "        - _`tags_list` (`person_tags_list`)_: \"local_hard_news,coded-OpenCalaisV2ArticleCoder\"\n",
    "\n",
    "- ...EXCEPT allowing duplicate articles for person so you get absolutely all persons, but not for `Article_Data` query.\n",
    "\n",
    "    - _`person_allow_duplicate_articles`_: \"yes\"\n",
    "\n",
    "- Network data creation options:\n",
    "\n",
    "    - excludes persons with single word (no spaces) `verbatim_name`.\n",
    "    \n",
    "        - _`include_persons_with_single_word_name`_: \"no\"\n",
    "    \n",
    "    - exclude render details\n",
    "        \n",
    "        - _`network_include_render_details`_: \"no\"\n",
    "        \n",
    "    - ouput as tab-delimited matrix, with node attributes as additional columns on the far right of the square network part of the matrix.\n",
    "\n",
    "        - _`output_type`_: \"tab_delimited_matrix\"\n",
    "        - _`network_data_output_type`_: \"net_and_attr_cols\"\n",
    "\n",
    "    - label - _`network_label`_: \"all_grp_hard_news\"\n",
    "    - include header row in the matrix output file.\n",
    "    \n",
    "        - _`network_include_headers`_: \"yes\"\n",
    "\n",
    "    - output spec plus the resulting network data to the database, with lable set to `network_label` plus a date-time string.\n",
    "    \n",
    "        - _`database_output`_: \"yes\",\n",
    "        - _`db_add_timestamp_to_label`_: \"yes\"\n",
    "\n",
    "_NOTE: only pass True to `network_outputter.process_network_output_request( debug_flag_IN )` if you really need to debug - it adds garbage data at the end of the output, even if you ask for no render details._\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60ef24eb-3a73-4fc7-8b67-2979753889ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_data_spec_json_string = \"\"\"\n",
    "    \"start_date\": \"2005-01-01\",\n",
    "    \"end_date\": \"2005-12-31\",\n",
    "    \"date_range\": \"\",\n",
    "    \"publications\": \"1\",\n",
    "    \"coders\": \"2\",\n",
    "    \"coder_id_priority_list\": \"\",\n",
    "    \"coder_type_filter_type\": \"automated\",\n",
    "    \"coder_types_list\": \"OpenCalais_REST_API_v2\",\n",
    "    \"tags_list\": \"local_hard_news\",\n",
    "    \"unique_identifiers\": \"\",\n",
    "    \"allow_duplicate_articles\": \"no\",\n",
    "    \"person_query_type\": \"custom\",\n",
    "    \"person_start_date\": \"2005-01-01\",\n",
    "    \"person_end_date\": \"2005-12-31\",\n",
    "    \"person_date_range\": \"\",\n",
    "    \"person_publications\": \"1\",\n",
    "    \"person_coders\": \"2\",\n",
    "    \"person_coder_id_priority_list\": \"\",\n",
    "    \"person_coder_type_filter_type\": \"automated\",\n",
    "    \"person_coder_types_list\": \"OpenCalais_REST_API_v2\",\n",
    "    \"person_tags_list\": \"local_hard_news\",\n",
    "    \"person_unique_identifiers\": \"\",\n",
    "    \"person_allow_duplicate_articles\": \"yes\",\n",
    "    \"include_source_contact_types\": [\n",
    "        \"direct\",\n",
    "        \"event\",\n",
    "        \"past_quotes\",\n",
    "        \"document\",\n",
    "        \"other\"\n",
    "    ],\n",
    "    \"exclude_persons_with_tags_in_list\": \"\",\n",
    "    \"include_persons_with_single_word_name\": \"no\",\n",
    "    \"network_download_as_file\": \"no\",\n",
    "    \"network_include_render_details\": \"no\",\n",
    "    \"output_type\": \"tab_delimited_matrix\",\n",
    "    \"network_data_output_type\": \"net_and_attr_cols\",\n",
    "    \"network_label\": \"all_grp_hard_news_2005\",\n",
    "    \"network_include_headers\": \"yes\",\n",
    "    \"database_output\": \"yes\",\n",
    "    \"db_add_timestamp_to_label\": \"yes\",\n",
    "    \"db_save_data_in_database\": \"no\",\n",
    "    \"save_data_in_folder\": \"{output_folder_path}\"\n",
    "\"\"\"\n",
    "\n",
    "base_data_spec_json_string = base_data_spec_json_string.format(\n",
    "    output_folder_path = network_data_output_folder_path\n",
    ")\n",
    "base_data_spec_json_string = \"{left_curly}{json_properties}{right_curly}\".format(\n",
    "    left_curly = \"{\",\n",
    "    json_properties = base_data_spec_json_string,\n",
    "    right_curly = \"}\"\n",
    ")\n",
    "\n",
    "base_data_spec_json = json.loads( base_data_spec_json_string )\n",
    "print( base_data_spec_json ) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "045a0216-61d3-450e-b73a-9daf681b5407",
   "metadata": {},
   "source": [
    "### Setup - update base data spec for different time slices\n",
    "\n",
    "To update this for different time slices:\n",
    "\n",
    "- make a copy of `base_data_spec_json`:\n",
    "\n",
    "    - not threadsafe:\n",
    "    \n",
    "            my_timeslice_spec = copy.deepcopy( base_data_spec_json )\n",
    "    \n",
    "    - threadsafe (but doesn't handle complex data types - ours is just JSON, though, so fine here):\n",
    "    \n",
    "            my_timeslice_spec = json.loads( json.dumps( base_data_spec_json ) )\n",
    "\n",
    "- update the `start_date` and `end_date` to the period you want for your time slice.\n",
    "\n",
    "        my_timeslice_spec[ NetworkOutput.PARAM_START_DATE ] = \"2009-12-01\"\n",
    "        my_timeslice_spec[ NetworkOutput.PARAM_END_DATE ] = \"2009-12-31\"\n",
    "\n",
    "- update the `network_label` value so that it captures what time slice you are making.\n",
    "\n",
    "        my_timeslice_spec[ NetworkOutput.PARAM_NETWORK_LABEL ] = \"month-grp-automated-20091201-20091231\"\n",
    "\n",
    "    - example pattern: <type>-<paper>-<coder>-<start_date>-<end_date>\n",
    "    - examples:\n",
    "        \n",
    "            week-grp-automated-20050501-20050507\n",
    "            7day-grp-automated-20050502-20050508\n",
    "\n",
    "    - type would be either:\n",
    "\n",
    "        - actual time period:\n",
    "\n",
    "            - week\n",
    "            - month\n",
    "            - quarter\n",
    "            - half-year\n",
    "            - year\n",
    "\n",
    "        - conceptual time period:\n",
    "\n",
    "            - sliding week = \"7day\"\n",
    "            - sliding month = \"31day\"\n",
    "            - sliding quarter = \"92day\"\n",
    "            - sliding half-year = \"183day\"\n",
    "            - sliding year = \"365day\"\n",
    "\n",
    "_NOTE: leave person query parameters the same for all networks if you want all your network matrices to have same set of people (same count and position of rows and columns) so each network can be compared to all others, regardless of time period of a given network slice._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50b987c7-3587-489f-b36a-3fa1c4ffc530",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a copy of base data spec\n",
    "before_data_spec_json = copy.deepcopy( base_data_spec_json )\n",
    "\n",
    "# update properties\n",
    "before_data_spec_json[ NetworkOutput.PARAM_START_DATE ] = \"2009-01-08\"\n",
    "before_data_spec_json[ NetworkOutput.PARAM_END_DATE ] = \"2010-01-08\"\n",
    "before_data_spec_json[ NetworkOutput.PARAM_PERSON_START_DATE ] = \"2009-01-08\"\n",
    "before_data_spec_json[ NetworkOutput.PARAM_PERSON_END_DATE ] = \"2011-01-08\"\n",
    "before_data_spec_json[ NetworkOutput.PARAM_NETWORK_LABEL ] = \"grp_year_before_layoff\"\n",
    "\n",
    "print( \"updated data spec:\\n{}\".format( json.dumps( before_data_spec_json, sort_keys = True, indent = 4 ) ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc737e7c-659b-4eeb-bc65-bcc1cf949b17",
   "metadata": {},
   "source": [
    "## Setup - shared datetime instances"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe689035-2d55-4b37-bb47-d5a9818cbdaf",
   "metadata": {},
   "source": [
    "To start, make:\n",
    "\n",
    "- `datetime.date`s for:\n",
    "\n",
    "    - start date (2005-01-01)\n",
    "    - end date (2010-11-30)\n",
    "    - layoff date (2010-01-08)\n",
    "\n",
    "- `datetime.timedelta`s for 1 year (365 days) and 1 day."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32ef53d3-d094-4163-99be-b338b9e15bce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# declare variables\n",
    "first_article_date = None\n",
    "last_article_date = None\n",
    "layoff_date = None\n",
    "one_year_delta = None\n",
    "six_month_delta = None\n",
    "three_month_delta = None\n",
    "month_31_delta = None\n",
    "one_day_delta = None\n",
    "\n",
    "# make dates and timedeltas\n",
    "first_article_date = datetime.date( 2005, 1, 1 )\n",
    "print( \"First article date: {}\".format( first_article_date ) )\n",
    "\n",
    "last_article_date = datetime.date( 2010, 11, 30 )\n",
    "print( \"Last article date: {}\".format( last_article_date ) )\n",
    "\n",
    "layoff_date = datetime.date( 2010, 1, 8 )\n",
    "print( \"Layoff date: {}\".format( layoff_date ) )\n",
    "\n",
    "one_year_delta = datetime.timedelta( days = 365 )\n",
    "print( \"One year delta: {}\".format( one_year_delta ) )\n",
    "\n",
    "six_month_delta = datetime.timedelta( days = 183 )\n",
    "print( \"six month-ish delta: {}\".format( six_month_delta ) )\n",
    "\n",
    "three_month_delta = datetime.timedelta( days = 92 )\n",
    "print( \"three month-ish delta: {}\".format( three_month_delta ) )\n",
    "\n",
    "month_31_delta = datetime.timedelta( days = 31 )\n",
    "print( \"month-ish delta: {}\".format( month_31_delta ) )\n",
    "\n",
    "one_day_delta = datetime.timedelta( days = 1 )\n",
    "print( \"One day delta: {}\".format( one_day_delta ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f20460e6-825e-4fb4-a403-eb320537bedb",
   "metadata": {
    "tags": []
   },
   "source": [
    "# network data output example - base data spec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d270743-ddb6-40b9-b77b-83f4e21fbb1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# try creating network data.\n",
    "start_dt = datetime.datetime.now()\n",
    "print( \"==> starting network creation at {}\".format( start_dt ) )\n",
    "\n",
    "network_outputter = NetworkOutput()\n",
    "network_data = network_outputter.process_network_output_request(\n",
    "    params_IN = base_data_spec_json,\n",
    "    debug_flag_IN = False\n",
    ")\n",
    "\n",
    "end_dt = datetime.datetime.now()\n",
    "print( \"==> network creation complete at {}\".format( end_dt ) )\n",
    "\n",
    "# duration:\n",
    "my_duration = end_dt - start_dt\n",
    "print( \"----> duration: {}\".format( my_duration ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa7e6a5d-221b-459e-bf26-2dc96afdc494",
   "metadata": {},
   "source": [
    "- if include_persons_with_single_word_name = \"yes\": 2427606\n",
    "- if include_persons_with_single_word_name = \"no\": 2344545"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a765cbff-c2e9-4075-b0ee-7f9d351e0d46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a hash of the data, for comparison\n",
    "network_data_hash = make_string_hash( network_data )\n",
    "print( \"Network data hash: {}\".format( network_data_hash ) )\n",
    "\n",
    "# match?\n",
    "should_be = \"d3fd8b3a0daa0c9e4b05a7017b51b16bbae95be1e11b0cb1293c6554867bf201\"\n",
    "if ( network_data_hash != should_be ):\n",
    "    \n",
    "    # not right hash. Error.\n",
    "    print( \"ERROR! network data hash is {}, should be {}\".format( network_data_hash, should_be ) )\n",
    "    \n",
    "else:\n",
    "    \n",
    "    # a match\n",
    "    print( \"MATCH - network data hash {} matches expected. hooray!\".format( network_data_hash ) )\n",
    "    \n",
    "#-- END debug/test --#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a35b214c-d25d-4278-a645-bacec4eed51c",
   "metadata": {},
   "outputs": [],
   "source": [
    "network_data_length = len( network_data )\n",
    "should_be = 379118986\n",
    "print( \"Network data length: {}\".format( network_data_length ) )\n",
    "if ( network_data_length != should_be ):\n",
    "    \n",
    "    # not right length. Error.\n",
    "    print( \"ERROR! network data length is {}, should be {}\".format( network_data_length, should_be ) )\n",
    "    \n",
    "else:\n",
    "    \n",
    "    # a match\n",
    "    print( \"MATCH - string len()gth of {} matches expected. hooray!\".format( network_data_length ) )\n",
    "    \n",
    "#-- END debug/test --#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "793e0488-d275-4469-867f-f24a1c61b1b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# look at master person dict\n",
    "master_person_dict = network_outputter.create_person_dict( load_person_IN = True )\n",
    "\n",
    "# how many entries?\n",
    "person_count = len( master_person_dict )\n",
    "print( \"- person count: {person_count}\".format( person_count = person_count ) )\n",
    "\n",
    "# right number?\n",
    "should_be = 13755\n",
    "if ( person_count != should_be ):\n",
    "    \n",
    "    # not right length. Error.\n",
    "    print( \"ERROR! person count is {}, should be {}\".format( person_count, should_be ) )\n",
    "    \n",
    "else:\n",
    "    \n",
    "    # a match\n",
    "    print( \"MATCH - person count of {} matches expected. hooray!\".format( person_count ) )\n",
    "    \n",
    "#-- END debug/test --#"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c74008c9-804f-49a3-8c59-83e9c7a0b9e1",
   "metadata": {
    "tags": []
   },
   "source": [
    "# network data output - years around 1/8/2010"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18170716-edcd-40eb-ad29-a77908964af3",
   "metadata": {},
   "outputs": [],
   "source": [
    "layoff_date = datetime.date( 2010, 1, 8 )\n",
    "print( \"Layoff date: {}\".format( layoff_date ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bff9f57-1ecd-4fc3-86c8-0a37ff8a3610",
   "metadata": {},
   "outputs": [],
   "source": [
    "one_year_delta = datetime.timedelta( days = 365 )\n",
    "print( \"One year delta: {}\".format( one_year_delta ) ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a591808a-604f-49a9-8a7c-79ac0de68250",
   "metadata": {},
   "outputs": [],
   "source": [
    "one_year_before_date = layoff_date - one_year_delta\n",
    "print( \"one_year_before_date = {}\".format( one_year_before_date ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "598169e2-8053-4b25-8691-7883a5918474",
   "metadata": {},
   "outputs": [],
   "source": [
    "one_year_after_date = layoff_date + one_year_delta\n",
    "print( \"one_year_after_date = {}\".format( one_year_after_date ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4806d76-0b05-4852-9b80-1acc9064114c",
   "metadata": {},
   "source": [
    "## year before 1/8/2010"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71e35693-c9d5-46ae-a05d-43d7274f5e2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a copy of base data spec\n",
    "before_data_spec_json = copy.deepcopy( base_data_spec_json )\n",
    "\n",
    "# update properties\n",
    "before_data_spec_json[ NetworkOutput.PARAM_START_DATE ] = \"2009-01-08\"\n",
    "before_data_spec_json[ NetworkOutput.PARAM_END_DATE ] = \"2010-01-08\"\n",
    "before_data_spec_json[ NetworkOutput.PARAM_PERSON_START_DATE ] = \"2009-01-08\"\n",
    "before_data_spec_json[ NetworkOutput.PARAM_PERSON_END_DATE ] = \"2011-01-08\"\n",
    "before_data_spec_json[ NetworkOutput.PARAM_NETWORK_LABEL ] = \"grp_year_before_layoff\"\n",
    "\n",
    "print( \"updated data spec:\\n{}\".format( json.dumps( before_data_spec_json, sort_keys = True, indent = 4 ) ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05e2ae77-0131-4651-bdd3-26f9b4023f4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# try creating network data.\n",
    "start_dt = datetime.datetime.now()\n",
    "print( \"----> starting network creation at {}\".format( start_dt ) )\n",
    "\n",
    "network_outputter = NetworkOutput()\n",
    "network_data = network_outputter.process_network_output_request(\n",
    "    params_IN = before_data_spec_json,\n",
    "    debug_flag_IN = False\n",
    ")\n",
    "\n",
    "end_dt = datetime.datetime.now()\n",
    "print( \"----> network creation complete at {}\".format( end_dt ) )\n",
    "\n",
    "# duration:\n",
    "my_duration = end_dt - start_dt\n",
    "print( \"--------> duration: {}\".format( my_duration ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea658082-f27a-4ea7-b21f-adf002c4594e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a hash of the data, for comparison\n",
    "network_data_hash = make_string_hash( network_data )\n",
    "print( \"Network data hash: {}\".format( network_data_hash ) )\n",
    "\n",
    "# match?\n",
    "should_be = \"bdb2945558d568ad8758d78c4b7be3e3f65ff3f569acc14c4c837c2b14170266\"\n",
    "if ( network_data_hash != should_be ):\n",
    "    \n",
    "    # not right hash. Error.\n",
    "    print( \"ERROR! network data hash is {}, should be {}\".format( network_data_hash, should_be ) )\n",
    "    \n",
    "else:\n",
    "    \n",
    "    # a match\n",
    "    print( \"MATCH - network data hash {} matches expected. hooray!\".format( network_data_hash ) )\n",
    "    \n",
    "#-- END debug/test --#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83b5ba7f-fec7-44f1-ba1d-6d501853240c",
   "metadata": {},
   "outputs": [],
   "source": [
    "network_data_length = len( network_data )\n",
    "should_be = 579121117\n",
    "print( \"Network data length: {}\".format( network_data_length ) )\n",
    "if ( network_data_length != should_be ):\n",
    "    \n",
    "    # not right length. Error.\n",
    "    print( \"ERROR! network data length is {}, should be {}\".format( network_data_length, should_be ) )\n",
    "    \n",
    "else:\n",
    "    \n",
    "    # a match\n",
    "    print( \"MATCH - string len()gth of {} matches expected. hooray!\".format( network_data_length ) )\n",
    "    \n",
    "#-- END debug/test --#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "472d7e4f-5b2e-47a4-ab33-35b96e910f7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# look at master person dict\n",
    "master_person_dict = network_outputter.create_person_dict( load_person_IN = True )\n",
    "\n",
    "# how many entries?\n",
    "person_count = len( master_person_dict )\n",
    "print( \"- person count: {person_count}\".format( person_count = person_count ) )\n",
    "\n",
    "# right number?\n",
    "should_be = 17003\n",
    "if ( person_count != should_be ):\n",
    "    \n",
    "    # not right length. Error.\n",
    "    print( \"ERROR! person count is {}, should be {}\".format( person_count, should_be ) )\n",
    "    \n",
    "else:\n",
    "    \n",
    "    # a match\n",
    "    print( \"MATCH - person count of {} matches expected. hooray!\".format( person_count ) )\n",
    "    \n",
    "#-- END debug/test --#"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "404ebfce-f11f-47ce-8a27-9ff9260e0df3",
   "metadata": {
    "tags": []
   },
   "source": [
    "## year after 1/8/2020"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2269c859-e07a-49c2-8495-08dfecddff1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a copy of base data spec\n",
    "after_data_spec_json = copy.deepcopy( base_data_spec_json )\n",
    "\n",
    "# update properties\n",
    "after_data_spec_json[ NetworkOutput.PARAM_START_DATE ] = \"2010-01-08\"\n",
    "after_data_spec_json[ NetworkOutput.PARAM_END_DATE ] = \"2011-01-08\"\n",
    "after_data_spec_json[ NetworkOutput.PARAM_PERSON_START_DATE ] = \"2009-01-08\"\n",
    "after_data_spec_json[ NetworkOutput.PARAM_PERSON_END_DATE ] = \"2011-01-08\"\n",
    "after_data_spec_json[ NetworkOutput.PARAM_NETWORK_LABEL ] = \"grp_year_after_layoff\"\n",
    "\n",
    "print( \"updated data spec:\\n{}\".format( json.dumps( after_data_spec_json, sort_keys = True, indent = 4 ) ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3bd24f9-3057-41b3-b4f8-5ceb96ee2c4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# try creating network data.\n",
    "start_dt = datetime.datetime.now()\n",
    "print( \"==> starting network creation at {}\".format( start_dt ) )\n",
    "\n",
    "network_outputter = NetworkOutput()\n",
    "network_data = network_outputter.process_network_output_request(\n",
    "    params_IN = after_data_spec_json,\n",
    "    debug_flag_IN = False\n",
    ")\n",
    "\n",
    "end_dt = datetime.datetime.now()\n",
    "print( \"==> network creation complete at {}\".format( end_dt ) )\n",
    "\n",
    "# duration:\n",
    "my_duration = end_dt - start_dt\n",
    "print( \"----> duration: {}\".format( my_duration ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0f497f3-fb52-49da-b60e-e15cb596baa6",
   "metadata": {},
   "source": [
    "- if include_persons_with_single_word_name = \"yes\": 2427606\n",
    "- if include_persons_with_single_word_name = \"no\": 2344545"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78e6d972-a490-485d-b104-f8d7eb900cce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a hash of the data, for comparison\n",
    "network_data_hash = make_string_hash( network_data )\n",
    "print( \"Network data hash: {}\".format( network_data_hash ) )\n",
    "\n",
    "# match?\n",
    "should_be = \"03562c3e38bb0f0a2feda08da44912291f1fc443c6c7f75bdd43f182cc30ecfa\"\n",
    "if ( network_data_hash != should_be ):\n",
    "    \n",
    "    # not right hash. Error.\n",
    "    print( \"ERROR! network data hash is {}, should be {}\".format( network_data_hash, should_be ) )\n",
    "    \n",
    "else:\n",
    "    \n",
    "    # a match\n",
    "    print( \"MATCH - network data hash {} matches expected. hooray!\".format( network_data_hash ) )\n",
    "    \n",
    "#-- END debug/test --#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "562ed85c-75ce-454e-ae50-204060b83364",
   "metadata": {},
   "outputs": [],
   "source": [
    "network_data_length = len( network_data )\n",
    "should_be = 579123903\n",
    "print( \"Network data length: {}\".format( network_data_length ) )\n",
    "if ( network_data_length != should_be ):\n",
    "    \n",
    "    # not right length. Error.\n",
    "    print( \"ERROR! network data length is {}, should be {}\".format( network_data_length, should_be ) )\n",
    "    \n",
    "else:\n",
    "    \n",
    "    # a match\n",
    "    print( \"MATCH - string len()gth of {} matches expected. hooray!\".format( network_data_length ) )\n",
    "    \n",
    "#-- END debug/test --#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0fb970f-8d73-48b9-973f-197158a62207",
   "metadata": {},
   "outputs": [],
   "source": [
    "# look at master person dict\n",
    "master_person_dict = network_outputter.create_person_dict( load_person_IN = True )\n",
    "\n",
    "# how many entries?\n",
    "person_count = len( master_person_dict )\n",
    "print( \"- person count: {person_count}\".format( person_count = person_count ) )\n",
    "\n",
    "# right number?\n",
    "should_be = 13755\n",
    "if ( person_count != should_be ):\n",
    "    \n",
    "    # not right length. Error.\n",
    "    print( \"ERROR! person count is {}, should be {}\".format( person_count, should_be ) )\n",
    "    \n",
    "else:\n",
    "    \n",
    "    # a match\n",
    "    print( \"MATCH - person count of {} matches expected. hooray!\".format( person_count ) )\n",
    "    \n",
    "#-- END debug/test --#"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9720606c-d11d-409a-b455-04ced049b7b5",
   "metadata": {},
   "source": [
    "# pairs of years within GRP data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "499dd7ac-5cae-41f7-b66e-470a3b2c3ec7",
   "metadata": {},
   "source": [
    "Start with first article date plus 365 days, go forward one day at a time making 1 year snapshots for the year before and after each date, with person query covering both years. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c7a4b23-1ad7-4205-a463-2e1092167f00",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# make a NetworkOutput instance to re-use\n",
    "network_outputter = NetworkOutput()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10e0c508-78fc-4874-ba5f-338087f457ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up parameters\n",
    "my_label = \"grp_years\"\n",
    "my_output_folder_path = \"{base_output_path}/{label}\".format(\n",
    "    base_output_path = network_data_output_folder_path,\n",
    "    label = my_label\n",
    ")\n",
    "\n",
    "# call function to make data.\n",
    "create_pre_post_network_pairs(\n",
    "    data_spec_IN = base_data_spec_json,\n",
    "    network_timedelta_IN = one_year_delta,\n",
    "    increment_timedelta_IN = month_31_delta,\n",
    "    label_prefix_IN = my_label,\n",
    "    output_folder_path_IN = my_output_folder_path,\n",
    "    do_create_IN = False,\n",
    "    debug_flag_IN = False,\n",
    "    network_outputter_IN = network_outputter\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0772fc03-2482-4457-8b9b-fe9bac64f2f8",
   "metadata": {},
   "source": [
    "# pairs of 6-months within GRP data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0f2cc00-5ef8-4e16-928e-57e8d500ec5c",
   "metadata": {},
   "source": [
    "Start with first article date plus 183 (365/2, rounded up) days, go forward one day at a time making half-year snapshots for the half-year before and after each date, with person query covering both half-years. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9343bfe-ad62-46ca-b82b-9ce6fa69c6e1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# make a NetworkOutput instance to re-use\n",
    "network_outputter = NetworkOutput()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1dfcc18-8dc1-4f9a-aa61-3849f4168b10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up parameters\n",
    "my_label = \"grp_6mos\"\n",
    "my_output_folder_path = \"{base_output_path}/{label}\".format(\n",
    "    base_output_path = network_data_output_folder_path,\n",
    "    label = my_label\n",
    ")\n",
    "\n",
    "# call function to make data.\n",
    "create_pre_post_network_pairs(\n",
    "    data_spec_IN = base_data_spec_json,\n",
    "    network_timedelta_IN = six_month_delta,\n",
    "    increment_timedelta_IN = one_day_delta,\n",
    "    label_prefix_IN = my_label,\n",
    "    output_folder_path_IN = my_output_folder_path,\n",
    "    do_create_IN = False,\n",
    "    debug_flag_IN = False,\n",
    "    network_outputter_IN = network_outputter\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42cc27cf-4318-4042-8932-3e24915392bf",
   "metadata": {},
   "source": [
    "# write network data to file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82aada51-899c-476b-b564-4fd5ef4a7889",
   "metadata": {},
   "outputs": [],
   "source": [
    "# write the output to a file\n",
    "current_date_time = None\n",
    "my_file_extension = None\n",
    "network_data_file_path = None\n",
    "network_data_file = None\n",
    "\n",
    "# time stamp and file extension to append to file name\n",
    "current_date_time = datetime.datetime.now().strftime( '%Y%m%d-%H%M%S' )\n",
    "my_file_extension = \"txt\"\n",
    "\n",
    "# make file path.\n",
    "network_data_file_path = \"context_text_data-{timestamp}.{file_extension}\".format(\n",
    "    timestamp = current_date_time,\n",
    "    file_extension = my_file_extension\n",
    ")\n",
    "\n",
    "# write to file.\n",
    "with open( network_data_file_path, 'w' ) as network_data_file:\n",
    "\n",
    "    # output all the data to file.\n",
    "    network_data_file.write( network_data )\n",
    "    \n",
    "#-- END with open( network_data_file_path, 'w' ) as network_data_file --#\n",
    "\n",
    "print( \"network data written to file {} at {}\".format( network_data_file_path, datetime.datetime.now() ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff4e4c78-8c4e-4089-a0ee-9a4d1a0eabc5",
   "metadata": {},
   "source": [
    "# Explore data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae05edff-fbf5-461b-9e8b-2e12dc9c3047",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set data file path.\n",
    "data_file_name = \"all_grp_hard_news_2005-20220602-012223\"\n",
    "data_file_path = \"{output_folder_path}/{data_file_name}\".format(\n",
    "    output_folder_path = network_data_output_folder_path,\n",
    "    data_file_name = data_file_name\n",
    ")\n",
    "update_every_x = 1000\n",
    "\n",
    "print( data_file_path )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f02901e8-0065-4ad0-801c-c707e04b3599",
   "metadata": {},
   "outputs": [],
   "source": [
    "# declare variables\n",
    "data_file = None\n",
    "data_file_reader = None\n",
    "data_file_line = None\n",
    "data_file_line_item_list = None\n",
    "person_info = None\n",
    "person_info_count = None\n",
    "person_info_counter = None\n",
    "person_info_lower = None\n",
    "counter_unknown = None\n",
    "counter_author = None\n",
    "counter_source = None\n",
    "counter_both = None\n",
    "update_every_x = 1000\n",
    "\n",
    "# Open network data output file for reading.\n",
    "with open( data_file_path, \"r\" ) as data_file:\n",
    "    \n",
    "    # csv.reader\n",
    "    #data_file_reader = csv.reader( data_file, delimiter=':', quoting=csv.QUOTE_NONE )\n",
    "    \n",
    "    # read first line.\n",
    "    data_file_line = data_file.readline()\n",
    "\n",
    "    # split on tabs.\n",
    "    data_file_line_item_list = data_file_line.split( \"\\t\" )\n",
    "    \n",
    "#-- END with open( data_file_path, \"r\" ) as data_file_name: --#\n",
    "\n",
    "person_info_count = len( data_file_line_item_list )\n",
    "person_info_counter = 0\n",
    "\n",
    "# loop and add up different person types.\n",
    "counter_unknown = 0\n",
    "counter_author = 0\n",
    "counter_source = 0\n",
    "counter_both = 0\n",
    "for person_info in data_file_line_item_list:\n",
    "    \n",
    "    # increment counter\n",
    "    person_info_counter += 1\n",
    "    \n",
    "    # does string contain...\n",
    "    person_info_lower = person_info.lower()\n",
    "    \n",
    "    # ==> \"unknown\"\n",
    "    if ( \"unknown\" in person_info_lower ):\n",
    "        \n",
    "        counter_unknown += 1\n",
    "        \n",
    "    #== END check if unknown --#\n",
    "\n",
    "    # ==> \"author\"\n",
    "    if ( \"author\" in person_info_lower ):\n",
    "\n",
    "        counter_author += 1\n",
    "\n",
    "    #== END check if author --#\n",
    "\n",
    "    # ==> \"source\"\n",
    "    if ( \"source\" in person_info_lower ):\n",
    "\n",
    "        counter_source += 1\n",
    "\n",
    "    #== END check if source --#\n",
    "        \n",
    "    # ==> \"both\"\n",
    "    if ( \"both\" in person_info_lower ):\n",
    "    \n",
    "        counter_both += 1\n",
    "    \n",
    "    #== END check if both --#\n",
    "        \n",
    "    # time to give brief update?\n",
    "    if ( ( person_info_counter % update_every_x ) == 0 ):\n",
    "        \n",
    "        # yes.\n",
    "        status_message = \"----> finished processing {counter} of {total} @ {my_timestamp}\".format(\n",
    "            counter = person_info_counter,\n",
    "            total = person_info_count,\n",
    "            my_timestamp = datetime.datetime.now()\n",
    "        )\n",
    "        print( status_message )\n",
    "        \n",
    "    #-- END check if update time. --#\n",
    "\n",
    "# END loop over header line of data file. --#\n",
    "\n",
    "print( \"\\n\" )\n",
    "print( \"Finished processing {record_count} header column names:\".format( record_count = person_info_counter ) )\n",
    "print( \"- counter_unknown = {}\".format( counter_unknown ) )\n",
    "print( \"- counter_author = {}\".format( counter_author ) )\n",
    "print( \"- counter_source = {}\".format( counter_source ) )\n",
    "print( \"- counter_both = {}\".format( counter_both ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d16042d3-3cd8-4ffb-a787-42052f66311f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# declare variables\n",
    "data_file = None\n",
    "data_file_reader = None\n",
    "data_file_line = None\n",
    "data_file_line_item_list = None\n",
    "data_file_item = None\n",
    "data_file_item_value = None\n",
    "counter_tie = None\n",
    "sum_weight = None\n",
    "counter_zero = None\n",
    "counter_negative = None\n",
    "counter_other = None\n",
    "counter_empty = None\n",
    "update_every_x = None\n",
    "row_counter = None\n",
    "column_counter = None\n",
    "\n",
    "# Open network data output file for reading.\n",
    "counter_tie = 0\n",
    "sum_weight = 0\n",
    "counter_zero = 0\n",
    "counter_negative = 0\n",
    "counter_other = 0\n",
    "counter_empty = 0\n",
    "update_every_x = 1000\n",
    "with open( data_file_path, \"r\" ) as data_file:\n",
    "    \n",
    "    # csv.reader\n",
    "    #data_file_reader = csv.reader( data_file, delimiter=':', quoting=csv.QUOTE_NONE )\n",
    "    \n",
    "    # try to move past header first line.\n",
    "    data_file_line = data_file.readline()\n",
    "\n",
    "    # loop over lines in file\n",
    "    row_counter = 0\n",
    "    for data_file_line in data_file:\n",
    "    \n",
    "        row_counter += 1\n",
    "    \n",
    "        # split on tabs.\n",
    "        data_file_line_item_list = data_file_line.split( \"\\t\" )\n",
    "        \n",
    "        # then, loop over items in list. For each, if not empty and not 0,\n",
    "        #     add 1 to counter of empty cells and add number to weight-aggregator.\n",
    "        column_counter = 0\n",
    "        for data_file_item in data_file_line_item_list:\n",
    "            \n",
    "            column_counter += 1\n",
    "            \n",
    "            # is it a number > 0?\n",
    "            if (\n",
    "                ( data_file_item is not None )\n",
    "                and ( data_file_item != \"\" )\n",
    "            ):\n",
    "                \n",
    "                # try to cast to int.\n",
    "                try:\n",
    "                    \n",
    "                    # cast to int\n",
    "                    data_file_item_value = int( data_file_item )\n",
    "                    \n",
    "                    # an int! > 0?\n",
    "                    if ( data_file_item_value > 0 ):\n",
    "                   \n",
    "                        # yes! an actual tie\n",
    "                        counter_tie += 1\n",
    "                        sum_weight += data_file_item_value\n",
    "                        \n",
    "                    elif ( data_file_item_value == 0 ): \n",
    "                        \n",
    "                        # no tie - increment counter_zero\n",
    "                        counter_zero += 1\n",
    "                        \n",
    "                    else:\n",
    "                        \n",
    "                        # neither 0 or greater than 0...\n",
    "                        counter_negative += 1\n",
    "                        \n",
    "                    #-- END check what is in int... --#\n",
    "                    \n",
    "                except:\n",
    "            \n",
    "                    # either string (or something else).\n",
    "                    counter_other += 1\n",
    "\n",
    "                #-- END try...except --#\n",
    "                \n",
    "            else:\n",
    "                \n",
    "                # empty...?\n",
    "                counter_empty += 1\n",
    "                \n",
    "            #-- END check if None or \"\" --#\n",
    "            \n",
    "        #-- END loop over items in line --#\n",
    "\n",
    "        # time to give brief update?\n",
    "        if ( ( row_counter % update_every_x ) == 0 ):\n",
    "\n",
    "            # yes.\n",
    "            status_message = \"----> finished processing {row_counter} rows @ {my_timestamp}\".format(\n",
    "                row_counter = row_counter,\n",
    "                my_timestamp = datetime.datetime.now()\n",
    "            )\n",
    "            print( status_message )\n",
    "\n",
    "        #-- END check if update time. --#\n",
    "          \n",
    "    #--- END loop over lines in file. --#\n",
    "    \n",
    "#-- END with open( data_file_path, \"r\" ) as data_file_name: --#\n",
    "\n",
    "print( \"\\n\" )\n",
    "print( \"Finished processing {row_counter} rows:\".format( row_counter = row_counter ) )\n",
    "print( \"- counter_tie = {}\".format( counter_tie ) )\n",
    "print( \"-----> sum_weight = {}\".format( sum_weight ) )\n",
    "print( \"- counter_zero = {}\".format( counter_zero ) )\n",
    "print( \"- counter_negative = {}\".format( counter_negative ) )\n",
    "print( \"- counter_other = {}\".format( counter_other ) )\n",
    "print( \"- counter_empty = {}\".format( counter_empty ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9223786-5f6b-45e9-973a-327457c81e58",
   "metadata": {},
   "source": [
    "Finished processing 50823 rows:\n",
    "- counter_tie = 137547\n",
    "- -----> sum_weight = 2077935942\n",
    "- counter_zero = 2582941428\n",
    "- counter_negative = 0\n",
    "- counter_other = 50823"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "research_virtualenv",
   "language": "python",
   "name": "research"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "toc-autonumbering": true
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
