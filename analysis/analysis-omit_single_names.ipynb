{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ba4ccabf-42d0-4217-8baf-87bd790d8478",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "089876e6-2d2f-4fd0-96f6-730520fb0bab",
   "metadata": {},
   "source": [
    "## Setup - Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47c3f417-8fcb-49c1-85f3-44c070e980fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# python base imports\n",
    "import datetime\n",
    "import json\n",
    "\n",
    "# import six\n",
    "import six\n",
    "\n",
    "print( \"packages imported at \" + str( datetime.datetime.now() ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17939321-ff9b-4230-84ef-5165d0d8820a",
   "metadata": {},
   "source": [
    "## Setup - Initialize Django\n",
    "\n",
    "- Back to [Table of Contents](#Table-of-Contents)\n",
    "\n",
    "First, initialize my dev django project, so I can run code in this notebook that references my django models and can talk to the database using my project's settings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2ea6891-3f5d-48f3-b819-a8ba8fa9d869",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run ../django_init.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c4c9544-0d64-4cf1-baca-f1226617dec1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# django imports\n",
    "from django.contrib.auth.models import User\n",
    "\n",
    "# sourcenet imports\n",
    "from context_text.shared.context_text_base import ContextTextBase\n",
    "\n",
    "# context_analysis imports\n",
    "from context_analysis.network.network_person_info import NetworkPersonInfo\n",
    "\n",
    "# sourcenet imports\n",
    "from context_text.models import Article\n",
    "from context_text.models import Article_Author\n",
    "from context_text.models import Article_Data\n",
    "from context_text.models import Article_Subject\n",
    "from context_text.models import Newspaper\n",
    "from context_text.models import Person\n",
    "\n",
    "# article coding\n",
    "from context_text.article_coding.article_coder import ArticleCoder\n",
    "#from context_text.article_coding.article_coding import ArticleCoding\n",
    "from context_text.article_coding.open_calais_v2.open_calais_v2_article_coder import OpenCalaisV2ArticleCoder\n",
    "\n",
    "# import class that actually processes requests for outputting networks.\n",
    "from context_text.export.network_output import NetworkOutput\n",
    "\n",
    "# context_text shared\n",
    "from context_text.shared.context_text_base import ContextTextBase\n",
    "\n",
    "print( \"django model packages imported at \" + str( datetime.datetime.now() ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "732b5099-9487-4c60-a4bd-35df7a6bf144",
   "metadata": {},
   "source": [
    "## Setup - shared variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3033a36d-b69a-4caf-89bd-9d8d7780e83c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get ArticleCoding instance.\n",
    "#article_coding = ArticleCoding()\n",
    "\n",
    "# automated coding user\n",
    "automated_coder = ArticleCoder.get_automated_coding_user()\n",
    "\n",
    "# newspapers for Grand Rapids Press and Detroit News.\n",
    "grand_rapids_press = Newspaper.objects.get( newsbank_code = \"GRPB\" )\n",
    "detroit_news = Newspaper.objects.get( newsbank_code = \"DTNB\" )\n",
    "\n",
    "# OpenCalais v2 coder type\n",
    "ocv2_coder_type = OpenCalaisV2ArticleCoder.CONFIG_APPLICATION"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b25d18a6-95bf-4363-bda0-91f87e96fd35",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Filter Article_Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9986c8bb-60b7-420d-be6a-eb1f81e38eee",
   "metadata": {},
   "outputs": [],
   "source": [
    "article_data_qs = None\n",
    "article_data_count = None\n",
    "\n",
    "# get all Article_Data.\n",
    "article_data_qs = Article_Data.objects.all()\n",
    "\n",
    "# how many we starting with?\n",
    "article_data_count = article_data_qs.count()\n",
    "\n",
    "print( \"Starting with {} total Article_Data instances.\".format( article_data_count ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fc98b10-3624-4df2-9f07-a091fe43a465",
   "metadata": {},
   "source": [
    "## Detect single-name people within Article_Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd856293-bc61-41d1-9d3e-ca420bce5479",
   "metadata": {},
   "source": [
    "- code to filter out those with single names... where?\n",
    "\n",
    "    - notebook where work was done originally (just notes - it was manual): [prelim_month-create_Reliability_Names_data.ipynb](./methods/data_creation/prelim_month-create_Reliability_Names_data.ipynb)\n",
    "    - code to filter to just single first names is in `context_analysis/views.py --> reliability_names_disagreement_view()`:\n",
    "    \n",
    "            if ( reliability_names_only_first_name == True ):\n",
    "\n",
    "                # to start, first name needs to not be null and\n",
    "                #     not be empty.\n",
    "                reliability_names_qs = reliability_names_qs.filter( \n",
    "                    Q( person__first_name__isnull = False ) & ~Q( person__first_name = \"\" ),\n",
    "                    Q( person__middle_name__isnull = True ) | Q( person__middle_name = \"\" ),\n",
    "                    Q( person__last_name__isnull = True ) | Q( person__last_name = \"\" ),\n",
    "                    Q( person__name_prefix__isnull = True ) | Q( person__name_prefix = \"\" ),\n",
    "                    Q( person__name_suffix__isnull = True ) | Q( person__name_suffix = \"\" ),\n",
    "                    Q( person__nickname__isnull = True ) | Q( person__nickname = \"\" ),\n",
    "                )\n",
    "\n",
    "            #-- END only first name --#\n",
    "\n",
    "Person in Article_Data\n",
    "\n",
    "- Article_Data\n",
    "\n",
    "    - Article_Author\n",
    "    - Article_Subject\n",
    "    - both Article_Author and Article_Subject have \"person\" relation that ties to person instance for name and other details.\n",
    "    - They also have name fields:\n",
    "    \n",
    "        - name\n",
    "        - verbatim_name\n",
    "        - lookup_name\n",
    "        \n",
    "    - Should mine the above to see how widely and reliably the name fields were set - could just look for names with no internal spaces there..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fc80f91-90a1-4cd5-b637-b85ce0130cac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# start with Article_Data QuerySet\n",
    "article_data_qs = Article_Data.objects.all()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "470f9c64-065a-4c0a-a185-0d33d419c798",
   "metadata": {},
   "source": [
    "### Article_Subject\n",
    "\n",
    "- Some have no name, verbatim_name, or lookup_name.\n",
    "\n",
    "    - small sample included valid people where this data was just not captured early on.\n",
    "    - see if I can write a script to populate from saved data.\n",
    "\n",
    "- Some, name/verbatim_name/lookup_name are single name. These we'd want to omit, as long as this is actually the verbatim name from the article.\n",
    "\n",
    "    - spot-check in [View article + coding](https://research.local/research/context/text/article/article_data/view_with_text/)\n",
    "    - looks like it is, and so omit if the subject's name in Article_Subject has no spaces.\n",
    "    - check programatically for single-name people (not only is verbatim name in Article_Subject single word, but also look at the different name elements of the \"Person\" associated with the single-name mention to see if the Person has just first names, also). Two reasons that single  of match:\n",
    "    \n",
    "        - from early days, program created person with single name part from article.\n",
    "        - ...?  I already forgot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2677e932-5b1f-4ce2-a6a7-c2ab3f3a1828",
   "metadata": {},
   "outputs": [],
   "source": [
    "# declare variables\n",
    "work_article_subject_qs = None\n",
    "test_article_subject_qs = None\n",
    "match_count = None\n",
    "article_subject = None\n",
    "my_name = None\n",
    "my_verbatim_name = None\n",
    "my_lookup_name = None\n",
    "do_output_details = None\n",
    "\n",
    "# configure\n",
    "do_output_details = False\n",
    "do_limit_to_sources = False\n",
    "\n",
    "# set up base queryset\n",
    "work_article_subject_qs = Article_Subject.objects.all()\n",
    "match_count = work_article_subject_qs.count()\n",
    "print( \"total Article_Subject count = {}\".format( match_count ) )\n",
    "\n",
    "# just automated coder.\n",
    "work_article_subject_qs = work_article_subject_qs.filter( article_data__coder = automated_coder )\n",
    "match_count = work_article_subject_qs.count()\n",
    "print( \"automated Article_Subject count = {}\".format( match_count ) )\n",
    "\n",
    "# only OpenCalais V2.\n",
    "work_article_subject_qs = work_article_subject_qs.filter( article_data__coder_type = ocv2_coder_type )\n",
    "match_count = work_article_subject_qs.count()\n",
    "print( \"automated OpenCalais v.2 Article_Subject count = {}\".format( match_count ) )\n",
    "\n",
    "# only Grand Rapids Press.\n",
    "work_article_subject_qs = work_article_subject_qs.filter( article_data__article__newspaper = grand_rapids_press )\n",
    "match_count = work_article_subject_qs.count()\n",
    "print( \"automated GRP Article_Subject count = {}\".format( match_count ) )\n",
    "\n",
    "# only subjects of type source?\n",
    "if ( do_limit_to_sources == True ):\n",
    "\n",
    "    work_article_subject_qs = work_article_subject_qs.filter( subject_type = Article_Subject.SUBJECT_TYPE_QUOTED )\n",
    "    match_count = work_article_subject_qs.count()\n",
    "    print( \"automated GRP Article_Subject quoted acount = {}\".format( match_count ) )\n",
    "\n",
    "#-- END check if we limit to quoted/sources --#\n",
    "    \n",
    "# look for any where name is not NULL.\n",
    "work_article_subject_qs = work_article_subject_qs.filter( name__isnull = False )\n",
    "match_count = work_article_subject_qs.count()\n",
    "print( \"only those with name set - name__isnull = False --> match count = {}\".format( match_count ) )\n",
    "\n",
    "# look for any that have no space in name.\n",
    "test_article_subject_qs = work_article_subject_qs.exclude( name__contains = \" \" )\n",
    "match_count = test_article_subject_qs.count()\n",
    "print( \"single_name records - exclude name__contains = \\\"<space>\\\" --> match count = {}\".format( match_count ) )\n",
    "\n",
    "if ( do_output_details == True ):\n",
    "\n",
    "    for article_subject in work_article_subject_qs[ 0 : 10 ]:\n",
    "\n",
    "        # get all names\n",
    "        my_name = article_subject.name\n",
    "        my_verbatim_name = article_subject.verbatim_name\n",
    "        my_lookup_name = article_subject.lookup_name\n",
    "\n",
    "        print( \"\\n{article_subject}:\".format( article_subject = article_subject ) )\n",
    "        print( \"-          name: {}\".format( my_name ) )\n",
    "        print( \"- verbatim_name: {}\".format( my_verbatim_name ) )\n",
    "        print( \"-   lookup_name: {}\".format( my_lookup_name ) )\n",
    "\n",
    "        my_article_data = article_subject.article_data\n",
    "        print( \"- Article_Data: {}\".format( my_article_data ) ) \n",
    "\n",
    "        my_article = my_article_data.article\n",
    "        print( \"- Article: {}\".format( my_article ) ) \n",
    "\n",
    "    #-- END loop over sample of Article_Subject instances --#\n",
    "\n",
    "    print( \"\\n\" )\n",
    "    \n",
    "#-- END check if output details --#\n",
    "\n",
    "# look for any that have no space in verbatim_name.\n",
    "test_article_subject_qs = work_article_subject_qs.exclude( verbatim_name__contains = \" \" )\n",
    "match_count = test_article_subject_qs.count()\n",
    "print( \"exclude verbatim_name__contains = \\\"<space>\\\" --> match count = {}\".format( match_count ) )\n",
    "\n",
    "# look for any that have no space in lookup_name.\n",
    "test_article_subject_qs = work_article_subject_qs.exclude( lookup_name__contains = \" \" )\n",
    "match_count = test_article_subject_qs.count()\n",
    "print( \"exclude lookup_name__contains = \\\"<space>\\\" --> match count = {}\".format( match_count ) )\n",
    "\n",
    "if ( do_output_details == True ):\n",
    "\n",
    "    for article_subject in work_article_subject_qs[ 0 : 10 ]:\n",
    "\n",
    "        # get all names\n",
    "        my_name = article_subject.name\n",
    "        my_verbatim_name = article_subject.verbatim_name\n",
    "        my_lookup_name = article_subject.lookup_name\n",
    "\n",
    "        print( \"\\n{article_subject}:\".format( article_subject = article_subject ) )\n",
    "        print( \"-          name: {}\".format( my_name ) )\n",
    "        print( \"- verbatim_name: {}\".format( my_verbatim_name ) )\n",
    "        print( \"-   lookup_name: {}\".format( my_lookup_name ) )\n",
    "\n",
    "        my_article_data = article_subject.article_data\n",
    "        print( \"- Article_Data: {}\".format( my_article_data ) ) \n",
    "\n",
    "        my_article = my_article_data.article\n",
    "        print( \"- Article: {}\".format( my_article ) ) \n",
    "\n",
    "    #-- END loop over sample of Article_Subject instances --#\n",
    "    \n",
    "#-- END check if output details. --#"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fa1ea99-9ea5-4ec8-b75c-184934b6f9ea",
   "metadata": {},
   "source": [
    "#### Article_Subject - collect information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97585ffb-b461-4521-9131-34794cdfe62b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#=============================================================================#\n",
    "# declare variables\n",
    "#=============================================================================#\n",
    "article_person = None\n",
    "my_name = None\n",
    "my_verbatim_name = None\n",
    "my_verbatim_name_part_list = None\n",
    "my_verbatim_name_part_count = None\n",
    "my_verbatim_name_has_spaces = None\n",
    "my_official_name_part_count = None\n",
    "my_person = None\n",
    "person_name_string = None\n",
    "person_name_part_count = None\n",
    "is_single_token = None\n",
    "is_single_name = None\n",
    "\n",
    "# declare variables - auditing\n",
    "do_output_progress = None\n",
    "print_every_x_records = None\n",
    "record_counter = None\n",
    "my_start_dt = None\n",
    "current_dt = None\n",
    "current_elapsed = None\n",
    "total_elapsed = None\n",
    "total_average = None\n",
    "previous_dt = None\n",
    "\n",
    "# declare variables - collect data\n",
    "article_person_count = None\n",
    "my_names_different_count = None\n",
    "my_names_same_count = None\n",
    "has_spaces_count = None\n",
    "single_name_count = None\n",
    "single_name_mismatch_count = None\n",
    "single_name_mismatch_list = None\n",
    "single_token_to_multi_name_person_count = None\n",
    "single_token_to_multi_name_list = None\n",
    "multi_token_to_single_name_person_count = None\n",
    "person_single_name_count = None\n",
    "my_name_counts_different_count = None\n",
    "name_counts_different_count = None\n",
    "same_name_as_person_count = None\n",
    "different_name_from_person_count = None\n",
    "\n",
    "#=============================================================================#\n",
    "# config/init\n",
    "#=============================================================================#\n",
    "do_output_progress = True\n",
    "print_every_x_records = 10000\n",
    "my_start_dt = datetime.datetime.now()\n",
    "previous_dt = my_start_dt\n",
    "single_name_mismatch_list = list()\n",
    "single_token_to_multi_name_list = list()\n",
    "\n",
    "# sort QuerySet by ID.\n",
    "work_article_subject_qs = work_article_subject_qs.order_by( 'id' )\n",
    "\n",
    "# initialize counts\n",
    "record_counter = 0\n",
    "my_names_different_count = 0\n",
    "my_names_same_count = 0\n",
    "has_spaces_count = 0\n",
    "single_name_count = 0\n",
    "single_name_mismatch_count = 0\n",
    "single_token_to_multi_name_person_count = 0\n",
    "multi_token_to_single_name_person_count = 0\n",
    "person_single_name_count = 0\n",
    "my_name_counts_different_count = 0\n",
    "name_counts_different_count = 0\n",
    "same_name_as_person_count = 0\n",
    "different_name_from_person_count = 0\n",
    "\n",
    "#=============================================================================#\n",
    "# check out selected records.\n",
    "#=============================================================================#\n",
    "article_person_count = work_article_subject_qs.count()\n",
    "for article_person in work_article_subject_qs:\n",
    "    \n",
    "    # increment overall counter\n",
    "    record_counter += 1\n",
    "    \n",
    "    # init\n",
    "    my_verbatim_name_has_spaces = None\n",
    "    \n",
    "    # get my name strings\n",
    "    my_name = article_person.name\n",
    "    my_verbatim_name = article_person.verbatim_name\n",
    "    \n",
    "    # are name and verbatim name the same?\n",
    "    if ( my_name == my_verbatim_name ):\n",
    "        \n",
    "        # same!\n",
    "        my_names_same_count += 1\n",
    "        \n",
    "    else:\n",
    "        \n",
    "        # not the same!\n",
    "        my_names_different_count += 1\n",
    "        \n",
    "    #-- END check if names same or different within Article_Person record --#\n",
    "    \n",
    "    # does verbatim_name have spaces?\n",
    "    my_verbatim_name_part_list = article_person.get_verbatim_name_token_list()\n",
    "    my_verbatim_name_part_count = len( my_verbatim_name_part_list )\n",
    "    if ( my_verbatim_name_part_count > 1 ):\n",
    "        \n",
    "        # has at least one space.\n",
    "        has_spaces_count += 1\n",
    "        is_single_token = False\n",
    "        \n",
    "    else:\n",
    "        \n",
    "        # no spaces, single name?\n",
    "        single_name_count += 1\n",
    "        is_single_token = True\n",
    "        \n",
    "    #-- END check if spaces present --#\n",
    "    \n",
    "    # retrieve person\n",
    "    my_person = article_person.person\n",
    "    \n",
    "    # use person to get official name part counts\n",
    "    my_official_name_part_count = article_person.get_name_part_count_from_name( my_verbatim_name )\n",
    "    person_name_part_count = my_person.get_name_part_count()\n",
    "    \n",
    "    # how many parts from person instance?\n",
    "    if ( person_name_part_count == 1 ):\n",
    "        \n",
    "        person_single_name_count += 1\n",
    "        is_single_name = True\n",
    "        \n",
    "    else:\n",
    "        \n",
    "        is_single_name = False\n",
    "        \n",
    "    #-- END check if person is single-name --#\n",
    "    \n",
    "    # string count comparison: does official match space-based?\n",
    "    if ( my_verbatim_name_part_count != my_official_name_part_count ):\n",
    "        \n",
    "        # they do not. Interesting.\n",
    "        my_name_counts_different_count += 1\n",
    "        \n",
    "    #-- END check if string-based name counts match --#\n",
    "    \n",
    "    # single name mismatch?\n",
    "    if ( is_single_name != is_single_token ):\n",
    "        \n",
    "        single_name_mismatch_count += 1\n",
    "        single_name_mismatch_list.append( article_person.id )\n",
    "        \n",
    "        # is it single token to multi-name person?\n",
    "        if ( ( is_single_token == True ) and ( is_single_name == False ) ):\n",
    "            \n",
    "            # single token in Article_Person name maps to Person with multiple name parts.\n",
    "            single_token_to_multi_name_person_count += 1\n",
    "            single_token_to_multi_name_list.append( article_person.id )\n",
    "            \n",
    "            # TODO - do something here? - if single token to multi-name person, probably wrong. --#\n",
    "            \n",
    "        elif ( ( is_single_token == False ) and ( is_single_name == True ) ):\n",
    "            \n",
    "            # multi-token name in Article_Person maps to Person with single name part.\n",
    "            multi_token_to_single_name_person_count += 1\n",
    "            \n",
    "        #-- END check of verbatim has one token, name has multiple name parts --#\n",
    "        \n",
    "    #-- END check if single-name booleans match --#\n",
    "    \n",
    "    # compare string to person\n",
    "    if ( my_official_name_part_count != person_name_part_count ):      \n",
    "        \n",
    "        # they do not. Interesting.\n",
    "        name_counts_different_count += 1\n",
    "        \n",
    "    #-- END check if string part count matches person --#\n",
    "    \n",
    "    # retrieve person name string\n",
    "    person_name_string = my_person.get_name_string()\n",
    "    #print ( \"----> person name string: \\\"{}\\\"\".format( person_name_string ) )\n",
    "    \n",
    "    # are the two strings the same?\n",
    "    if ( my_verbatim_name == person_name_string ):\n",
    "        \n",
    "        # same.\n",
    "        same_name_as_person_count += 1\n",
    "    \n",
    "    else:\n",
    "        \n",
    "        # not same.\n",
    "        different_name_from_person_count += 1\n",
    "        \n",
    "    #-- END check if string from article and rendered person name are the same. --#\n",
    "\n",
    "    # output a progress message?\n",
    "    if ( ( ( record_counter % print_every_x_records ) == 0 )\n",
    "        and ( do_output_progress == True ) ):\n",
    "\n",
    "        # basic timing analysis.\n",
    "        current_dt = datetime.datetime.now()\n",
    "        current_elapsed = current_dt - previous_dt\n",
    "        total_elapsed = current_dt - my_start_dt\n",
    "        total_average = total_elapsed / record_counter\n",
    "        previous_dt = current_dt\n",
    "\n",
    "        status_message = \"processed {counter} of {count} records @ {right_now} ( timing: last {current_count} elapsed = {current_elapsed}; total elapsed = {total_elapsed}; average = {total_average} ).\".format(\n",
    "            counter = record_counter,\n",
    "            count = article_person_count,\n",
    "            right_now = current_dt,\n",
    "            current_count = print_every_x_records,\n",
    "            current_elapsed = current_elapsed,\n",
    "            total_elapsed = total_elapsed,\n",
    "            total_average = total_average\n",
    "        )\n",
    "        print( status_message )\n",
    "        #self.output_log_message(\n",
    "        #    status_message,\n",
    "        #    method_IN = me,\n",
    "        #    indent_with_IN = \"\\n\\n----> \",\n",
    "        #    log_level_code_IN = logging.INFO,\n",
    "        #    do_print_IN = True\n",
    "        #)\n",
    "\n",
    "    #-- END periodic status update. --#\n",
    "    \n",
    "#-- END loop over busted Article_Author instances --#\n",
    "\n",
    "print( \"Processed {} people:\".format( article_person_count ) )\n",
    "print( \"- my_names_different_count = {}\".format( my_names_different_count ) )\n",
    "print( \"- my_names_same_count = {}\".format( my_names_same_count ) )\n",
    "print( \"- has_spaces_count = {}\".format( has_spaces_count ) )\n",
    "print( \"- single_name_count = {}\".format( single_name_count ) )\n",
    "print( \"- person_single_name_count = {}\".format( person_single_name_count ) )\n",
    "print( \"- single_name_mismatch_count = {}\".format( single_name_mismatch_count ) )\n",
    "print( \"- single_token_to_multi_name_person_count = {}\".format( single_token_to_multi_name_person_count ) )\n",
    "print( \"- multi_token_to_single_name_person_count = {}\".format( multi_token_to_single_name_person_count ) )\n",
    "print( \"- my_name_counts_different_count = {}\".format( my_name_counts_different_count ) )\n",
    "print( \"- name_counts_different_count = {}\".format( name_counts_different_count ) )\n",
    "print( \"- same_name_as_person_count = {}\".format( same_name_as_person_count ) )\n",
    "print( \"- different_name_from_person_count = {}\".format( different_name_from_person_count ) )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a34207df-1fb1-4e22-8f62-ad2c49fdffdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "print( \"Processed {} people:\".format( article_person_count ) )\n",
    "print( \"- my_names_different_count = {}\".format( my_names_different_count ) )\n",
    "print( \"- my_names_same_count = {}\".format( my_names_same_count ) )\n",
    "print( \"- has_spaces_count = {}\".format( has_spaces_count ) )\n",
    "print( \"- single_name_count = {}\".format( single_name_count ) )\n",
    "print( \"- person_single_name_count = {}\".format( person_single_name_count ) )\n",
    "print( \"- single_name_mismatch_count = {}\".format( single_name_mismatch_count ) )\n",
    "print( \"- single_token_to_multi_name_person_count = {}\".format( single_token_to_multi_name_person_count ) )\n",
    "print( \"- my_name_counts_different_count = {}\".format( my_name_counts_different_count ) )\n",
    "print( \"- name_counts_different_count = {}\".format( name_counts_different_count ) )\n",
    "print( \"- same_name_as_person_count = {}\".format( same_name_as_person_count ) )\n",
    "print( \"- different_name_from_person_count = {}\".format( different_name_from_person_count ) )\n",
    "if ( ( single_name_mismatch_list is not None ) and ( len( single_name_mismatch_list ) > 0 ) ):\n",
    "    \n",
    "    print( \"- single_name_mismatch_list: \\n{}\".format( single_name_mismatch_list ) )\n",
    "    \n",
    "#-- END see if there are mismatch list items. --#\n",
    "if ( ( single_token_to_multi_name_list is not None ) and ( len( single_token_to_multi_name_list ) > 0 ) ):\n",
    "    \n",
    "    print( \"- single_token_to_multi_name_list: \\n{}\".format( single_token_to_multi_name_list ) )\n",
    "    \n",
    "#-- END see if there are mismatch list items. --#"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67782e3a-c036-4d4b-8688-a0fdc2ce8be1",
   "metadata": {},
   "source": [
    "### Article_Author"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59644429-112b-4aba-990e-2f2674d84da0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# declare variables\n",
    "match_count = None\n",
    "article_subject = None\n",
    "my_name = None\n",
    "my_verbatim_name = None\n",
    "my_lookup_name = None\n",
    "\n",
    "# set up base queryset\n",
    "all_article_subjects_qs = Article_Subject.objects.all()\n",
    "match_count = all_article_subjects_qs.count()\n",
    "print( \"total Article_Subject count = {}\".format( match_count ) )\n",
    "\n",
    "# just automated coder.\n",
    "all_article_subjects_qs = all_article_subjects_qs.filter( article_data__coder = automated_coder )\n",
    "match_count = all_article_subjects_qs.count()\n",
    "print( \"automated Article_Subject count = {}\".format( match_count ) )\n",
    "\n",
    "# look for any that have no space in name.\n",
    "article_subject_qs = all_article_subjects_qs.exclude( name__contains = \" \" )\n",
    "match_count = article_subject_qs.count()\n",
    "print( \"exclude name__contains = \\\"<space>\\\" --> match count = {}\".format( match_count ) )\n",
    "\n",
    "# look for any that have no space in name.\n",
    "article_subject_qs = article_subject_qs.exclude( name__isnull = True )\n",
    "match_count = article_subject_qs.count()\n",
    "print( \"exclude name__contains = \\\"<space>\\\"; exclude NULL --> match count = {}\".format( match_count ) )\n",
    "\n",
    "for article_subject in article_subject_qs[ 0 : 10 ]:\n",
    "    \n",
    "    # get all names\n",
    "    my_name = article_subject.name\n",
    "    my_verbatim_name = article_subject.verbatim_name\n",
    "    my_lookup_name = article_subject.lookup_name\n",
    "\n",
    "    print( \"\\n{article_subject}:\".format( article_subject = article_subject ) )\n",
    "    print( \"-          name: {}\".format( my_name ) )\n",
    "    print( \"- verbatim_name: {}\".format( my_verbatim_name ) )\n",
    "    print( \"-   lookup_name: {}\".format( my_lookup_name ) )\n",
    "    \n",
    "#-- END loop over sample of Article_Subject instances --#\n",
    "\n",
    "print( \"\\n\" )\n",
    "\n",
    "# look for any that have no space in verbatim_name.\n",
    "article_subject_qs = all_article_subjects_qs.exclude( verbatim_name__contains = \" \" )\n",
    "match_count = article_subject_qs.count()\n",
    "print( \"exclude verbatim_name__contains = \\\"<space>\\\" --> match count = {}\".format( match_count ) )\n",
    "\n",
    "# look for any that have no space in lookup_name.\n",
    "article_subject_qs = all_article_subjects_qs.exclude( lookup_name__contains = \" \" )\n",
    "match_count = article_subject_qs.count()\n",
    "print( \"exclude lookup_name__contains = \\\"<space>\\\" --> match count = {}\".format( match_count ) )\n",
    "\n",
    "for article_subject in article_subject_qs[ 0 : 10 ]:\n",
    "    \n",
    "    # get all names\n",
    "    my_name = article_subject.name\n",
    "    my_verbatim_name = article_subject.verbatim_name\n",
    "    my_lookup_name = article_subject.lookup_name\n",
    "\n",
    "    print( \"\\n{article_subject}:\".format( article_subject = article_subject ) )\n",
    "    print( \"-          name: {}\".format( my_name ) )\n",
    "    print( \"- verbatim_name: {}\".format( my_verbatim_name ) )\n",
    "    print( \"-   lookup_name: {}\".format( my_lookup_name ) )\n",
    "    \n",
    "#-- END loop over sample of Article_Subject instances --#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1b470d7-a079-42cc-90d7-9c47827cbbf2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# declare variables\n",
    "article_author = None\n",
    "article_data = None\n",
    "article = None\n",
    "author_string_parts_list = None\n",
    "author_name_string = None\n",
    "author_person = None\n",
    "person_name_string = None\n",
    "do_updates = None\n",
    "\n",
    "# configure\n",
    "do_updates = True\n",
    "\n",
    "# output the Article_Author records to fix\n",
    "name_qs = name_qs.order_by( 'id' )\n",
    "for article_author in name_qs:\n",
    "    \n",
    "    # just print it out.\n",
    "    print( \"\\nArticle_Author to fix: {}\".format( article_author ) )\n",
    "    \n",
    "    # retrieve the Article_Data\n",
    "    article_data = article_author.article_data\n",
    "    \n",
    "    # retrieve the article\n",
    "    article = article_data.article\n",
    "    \n",
    "    # grab the author string\n",
    "    author_string = article.author_string\n",
    "    \n",
    "    print ( \"----> author_string: {}\".format( author_string ) )\n",
    "    \n",
    "    # split on \"/\", get first token, and strip it.\n",
    "    author_string_parts_list = author_string.split( \"/\" )\n",
    "    author_name_string = author_string_parts_list[ 0 ]\n",
    "    author_name_string = author_name_string.strip()\n",
    "    print ( \"----> article author name string: \\\"{}\\\"\".format( author_name_string ) )\n",
    "    \n",
    "    # retrieve person\n",
    "    author_person = article_author.person\n",
    "    \n",
    "    # retrieve person name string\n",
    "    person_name_string = author_person.get_name_string()\n",
    "    print ( \"----> person name string: \\\"{}\\\"\".format( person_name_string ) )\n",
    "    \n",
    "    # are the two strings the same? If so, fix Article_Author. If not, output.\n",
    "    if ( author_name_string == person_name_string ):\n",
    "        \n",
    "        # same.\n",
    "        print( \"----> SAME\" )\n",
    "        if ( do_updates == True ):\n",
    "            \n",
    "            # update Article_Author.\n",
    "            article_author.name = author_name_string\n",
    "            article_author.verbatim_name = author_name_string\n",
    "            article_author.lookup_name = author_name_string\n",
    "            article_author.save()\n",
    "\n",
    "            print( \"----> ...fixed\" )\n",
    "    \n",
    "        #-- END check if we want to actually update/fix --#\n",
    "    \n",
    "    else:\n",
    "        \n",
    "        # not same.\n",
    "        print( \"----> NOT same\" )\n",
    "        \n",
    "    #-- END check if string from article and rendered person name are the same. --#\n",
    "    \n",
    "#-- END loop over busted Article_Author instances --#"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b90018a1-b761-40b0-bdd4-09bbdb880bd5",
   "metadata": {},
   "source": [
    "# Article_Data for GRP analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdac1569-8541-449c-8f0b-358bacfe4fdd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# init\n",
    "grp_article_data_qs = None\n",
    "article_data_count = None\n",
    "work_qs = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cb76b34-df3c-4663-9578-65386210b8e3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# get all Article_Data.\n",
    "grp_article_data_qs = Article_Data.objects.all()\n",
    "\n",
    "# how many we starting with?\n",
    "article_data_count = grp_article_data_qs.count()\n",
    "\n",
    "print( \"Starting with {} total Article_Data instances.\".format( article_data_count ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1911602a-42b2-4956-aa63-829f0f92bc22",
   "metadata": {},
   "source": [
    "## GRP - Only OpenCalais v.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bc88a6c-d5d9-4323-b547-91f3c585552c",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_coder_type = ocv2_coder_type\n",
    "\n",
    "# filter to just Article_Data with coder_type of \"OpenCalais_REST_API_v2\"\n",
    "grp_article_data_qs = grp_article_data_qs.filter( coder = automated_coder )\n",
    "grp_article_data_qs = grp_article_data_qs.filter( coder_type = my_coder_type )\n",
    "\n",
    "# how many now?\n",
    "article_data_count = grp_article_data_qs.count()\n",
    "\n",
    "print( \"{} Article_Data instances for coder_type {}.\".format( article_data_count, my_coder_type ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a332960-3fa9-4a78-8f4d-fdd4aa23a08a",
   "metadata": {
    "tags": []
   },
   "source": [
    "## GRP - Only Grand Rapids Press"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee245b06-07aa-4a46-8c49-532c8e0fde8a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# get automated coder user.\n",
    "#automated_coder = ArticleCoder.get_automated_coding_user()\n",
    "my_newspaper = grand_rapids_press\n",
    "\n",
    "# filter to just Article_Data coded by this user.\n",
    "grp_article_data_qs = grp_article_data_qs.filter( article__newspaper = my_newspaper )\n",
    "\n",
    "# how many now?\n",
    "article_data_count = grp_article_data_qs.count()\n",
    "\n",
    "print( \"{} Article_Data instances for newspaper {}.\".format( article_data_count, my_newspaper ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50476142-ac19-46f0-a798-bb55e86322c4",
   "metadata": {},
   "source": [
    "# Person exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16148bb1-228e-46b8-a31e-ef5b317adc03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get all Persons\n",
    "person_qs = Person.objects.all()\n",
    "person_count = person_qs.count()\n",
    "print( \"{} Persons\".format( person_count ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65f1ab65-0b6e-4782-8601-8b5b2cd64083",
   "metadata": {},
   "outputs": [],
   "source": [
    "#=============================================================================#\n",
    "# declare variables\n",
    "#=============================================================================#\n",
    "person = None\n",
    "name_part_count = None\n",
    "is_single_name = None\n",
    "print_every_x_records = None\n",
    "record_counter = None\n",
    "person_article_subject_qs = None\n",
    "my_article_subject = None\n",
    "subject_verbatim_name = None\n",
    "article_subject_name_token_count = None\n",
    "article_subject_name_part_count = None\n",
    "\n",
    "# declare variables - auditing\n",
    "do_output_progress = None\n",
    "print_every_x_records = None\n",
    "record_counter = None\n",
    "my_start_dt = None\n",
    "current_dt = None\n",
    "current_elapsed = None\n",
    "total_elapsed = None\n",
    "total_average = None\n",
    "previous_dt = None\n",
    "\n",
    "# declare variables - counts\n",
    "single_name_count = None\n",
    "multi_name_part_count = None\n",
    "crazy_name_part_count = None\n",
    "subject_name_count_mismatch_count = None\n",
    "subject_token_mismatch_count = None\n",
    "missing_verbatim_name_count = None\n",
    "\n",
    "#=============================================================================#\n",
    "# config/init\n",
    "#=============================================================================#\n",
    "do_output_progress = True\n",
    "print_every_x_records = 10000\n",
    "my_start_dt = datetime.datetime.now()\n",
    "previous_dt = my_start_dt\n",
    "\n",
    "# init counters\n",
    "record_counter = 0\n",
    "single_name_count = 0\n",
    "multi_name_part_count = 0\n",
    "crazy_name_part_count = 0\n",
    "subject_name_count_mismatch_count = 0\n",
    "subject_token_mismatch_count = 0\n",
    "missing_verbatim_name_count = 0\n",
    "\n",
    "#=============================================================================#\n",
    "# loop.\n",
    "#=============================================================================#\n",
    "record_counter = 0\n",
    "for person in person_qs:\n",
    "    \n",
    "    record_counter += 1\n",
    "    \n",
    "    # get name part count\n",
    "    name_part_count = person.get_name_part_count()\n",
    "    \n",
    "    # single name?\n",
    "    if ( name_part_count == 1 ):\n",
    "    \n",
    "        # is single name.\n",
    "        is_single_name = True\n",
    "        \n",
    "        # increment count\n",
    "        single_name_count += 1\n",
    "    \n",
    "    elif ( name_part_count > 1 ):\n",
    "        \n",
    "        # multiple name parts.\n",
    "        multi_name_part_count += 1\n",
    "        \n",
    "    else:\n",
    "        \n",
    "        # hmmm. What?\n",
    "        crazy_name_part_count += 1\n",
    "        \n",
    "    #-- END check if single_name --#\n",
    "    \n",
    "    # get set of related Article_Subject instances\n",
    "    person_article_subject_qs = person.article_subject_set.all()\n",
    "    for my_article_subject in person_article_subject_qs:\n",
    "        \n",
    "        # got subject_verbatim_name\n",
    "        subject_verbatim_name = my_article_subject.verbatim_name\n",
    "        if ( ( subject_verbatim_name is not None ) and ( subject_verbatim_name != \"\" ) ):\n",
    "            \n",
    "            # get name part and name token counts.\n",
    "            article_subject_name_token_count = my_article_subject.get_verbatim_name_token_count()\n",
    "            article_subject_name_part_count = my_article_subject.get_verbatim_name_part_count()\n",
    "\n",
    "            # same token count as person name part count?\n",
    "            if ( article_subject_name_token_count != name_part_count ):\n",
    "\n",
    "                # not same - mismatch\n",
    "                subject_token_mismatch_count += 1\n",
    "\n",
    "            #-- END check if token count = name part count --#\n",
    "\n",
    "            # same name part count as person name part count?\n",
    "            if ( article_subject_name_part_count != name_part_count ):\n",
    "\n",
    "                # not same - mismatch\n",
    "                subject_name_count_mismatch_count += 1\n",
    "\n",
    "            #-- END check if token count = name part count --#\n",
    "            \n",
    "        else:\n",
    "            \n",
    "            missing_verbatim_name_count += 1\n",
    "            \n",
    "        #-- END check if verbatim name present. --#\n",
    "        \n",
    "    #-- END loop over related Article_Subject instances --#\n",
    "    \n",
    "    # output a progress message?\n",
    "    if ( ( ( record_counter % print_every_x_records ) == 0 )\n",
    "        and ( do_output_progress == True ) ):\n",
    "\n",
    "        # basic timing analysis.\n",
    "        current_dt = datetime.datetime.now()\n",
    "        current_elapsed = current_dt - previous_dt\n",
    "        total_elapsed = current_dt - my_start_dt\n",
    "        total_average = total_elapsed / record_counter\n",
    "        previous_dt = current_dt\n",
    "\n",
    "        status_message = \"processed {counter} of {count} records @ {right_now} ( timing: last {current_count} elapsed = {current_elapsed}; total elapsed = {total_elapsed}; average = {total_average} ).\".format(\n",
    "            counter = record_counter,\n",
    "            count = person_count,\n",
    "            right_now = current_dt,\n",
    "            current_count = print_every_x_records,\n",
    "            current_elapsed = current_elapsed,\n",
    "            total_elapsed = total_elapsed,\n",
    "            total_average = total_average\n",
    "        )\n",
    "        print( status_message )\n",
    "        #self.output_log_message(\n",
    "        #    status_message,\n",
    "        #    method_IN = me,\n",
    "        #    indent_with_IN = \"\\n\\n----> \",\n",
    "        #    log_level_code_IN = logging.INFO,\n",
    "        #    do_print_IN = True\n",
    "        #)\n",
    "\n",
    "    #-- END periodic status update. --#\n",
    "\n",
    "#-- END loop over persons. --#\n",
    "\n",
    "print( \"Processed {} people:\".format( record_counter ) )\n",
    "print( \"- single_name_count = {}\".format( single_name_count ) )\n",
    "print( \"- multi_name_part_count = {}\".format( multi_name_part_count ) )\n",
    "print( \"- crazy_name_part_count = {}\".format( crazy_name_part_count ) )\n",
    "print( \"- subject_token_mismatch_count = {}\".format( subject_token_mismatch_count ) )\n",
    "print( \"- subject_name_count_mismatch_count = {}\".format( subject_name_count_mismatch_count ) )\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "research_virtualenv",
   "language": "python",
   "name": "research"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "toc-autonumbering": true
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
