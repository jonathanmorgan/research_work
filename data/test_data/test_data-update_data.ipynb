{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4c07e508-d83b-4d08-92b6-41f43cf4c9e0",
   "metadata": {},
   "source": [
    "**_test_data-update_data.ipynb_ - Update unit test data to include OpenCalais V.2 coding**\n",
    "\n",
    "- derived from: [newsbank-article_coding-unittest.ipynb](../article_coding/newsbank-article_coding-unittest.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6a15e82-789c-43a4-a4c3-9edae9efba62",
   "metadata": {
    "tags": []
   },
   "source": [
    "# TODO\n",
    "\n",
    "- update test data:\n",
    "\n",
    "    - load existing fixtures.\n",
    "    - includes 43 coded with OpenCalais.\n",
    "    - make sure we can generate network data from them.\n",
    "    - single-name data\n",
    "    \n",
    "        - there are two single-name sources.\n",
    "        - things to include in actual data:\n",
    "            - build out network data for a few different specs (one with single name, one without).\n",
    "            \n",
    "                - specs are in [analysis-network_data_output_example.ipynb](./analysis/analysis-network_data_output_example.ipynb)\n",
    "            \n",
    "            - for each data spec, in NetworkDataOutputLog, capture output from no-single-names for original data and data where set records have single-names introduced, both with details on and details off.\n",
    "            - also get the hashes and length of output strings you'd expect and store the values in the test case.\n",
    "\n",
    "    - remove superuser user from auth.\n",
    "    - re-export the \"export\" fixture that includes the network data output log (should just need that one).\n",
    "\n",
    "- make sure existing unit tests work with new data.\n",
    "- new unit tests:\n",
    "\n",
    "    - unit test code is in [analysis-network_data_output_example.ipynb](./analysis/analysis-network_data_output_example.ipynb)\n",
    "    - simple network data creation test - run with a few specs against test data, make sure I get the right size of output back for each.\n",
    "    - even lower level, make tests for the method to build person dictionaries, and the base lookup method.\n",
    "    - ? - make sure the Article_Data method `filter_article_persons()` works as I intend. To start, create tests in notebook against actual database, using full Article_Subject and Article_Author QuerySets, compare numbers to raw queries. Then, do the same against test database, use numbers to create unit tests. This should be covered by simple network creation tests (they call this method)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff8fc975-af92-414e-879a-966ebc7c6131",
   "metadata": {},
   "source": [
    "# Setup\n",
    "\n",
    "- Back to [Table of Contents](#Table-of-Contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7472a19-8a94-4874-97c5-e439b4d95e93",
   "metadata": {},
   "source": [
    "## Setup - Debug\n",
    "\n",
    "- Back to [Table of Contents](#Table-of-Contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b227033c-54b0-4a7c-8fcb-321f79b49e3c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-17T15:43:45.678275Z",
     "start_time": "2019-09-17T15:43:45.673730Z"
    }
   },
   "outputs": [],
   "source": [
    "debug_flag = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e935a804-bd7c-4f9b-bee9-fe0c19a8f4c3",
   "metadata": {},
   "source": [
    "## Setup - Imports\n",
    "\n",
    "- Back to [Table of Contents](#Table-of-Contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bbea8370-9e1e-4814-a3bf-bffbdfc28061",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-17T15:43:46.692379Z",
     "start_time": "2019-09-17T15:43:46.578604Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "packages imported at 2022-05-27 04:04:15.893683\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "from django.db.models import Avg, Max, Min\n",
    "import json\n",
    "import logging\n",
    "import six\n",
    "\n",
    "print( \"packages imported at \" + str( datetime.datetime.now() ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfda3c6c-66f1-4b63-935c-0be496897b7d",
   "metadata": {},
   "source": [
    "## Setup - working folder paths\n",
    "\n",
    "- Back to [Table of Contents](#Table-of-Contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f8f7f040-8330-4516-a99e-033ddc4e35fe",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-17T15:43:47.305614Z",
     "start_time": "2019-09-17T15:43:47.286949Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/jonathanmorgan/work/django/research/research/work/phd_work/data/test_data'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7da0eb89-3e23-46e0-ba4b-e50e52e56932",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-17T15:43:48.159415Z",
     "start_time": "2019-09-17T15:43:48.153181Z"
    }
   },
   "outputs": [],
   "source": [
    "# current working folder\n",
    "current_working_folder = \"/home/jonathanmorgan/work/django/research/research/work/phd_work/analysis\"\n",
    "current_datetime = datetime.datetime.now()\n",
    "current_date_string = current_datetime.strftime( \"%Y-%m-%d-%H-%M-%S\" )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47cc9023-52b4-4501-a158-154a390bd0e4",
   "metadata": {},
   "source": [
    "## Setup - logging\n",
    "\n",
    "- Back to [Table of Contents](#Table-of-Contents)\n",
    "\n",
    "configure logging for this notebook's kernel (If you do not run this cell, you'll get the django application's logging configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e69e52fb-2f35-4dda-8726-ae7ff20a05d6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-17T15:43:49.038050Z",
     "start_time": "2019-09-17T15:43:49.031535Z"
    }
   },
   "outputs": [],
   "source": [
    "# build file name\n",
    "logging_file_name = \"{}/article_coding-{}.log.txt\".format( current_working_folder, current_date_string )\n",
    "\n",
    "# set up logging.\n",
    "logging.basicConfig(\n",
    "    level = logging.DEBUG,\n",
    "    format = '%(asctime)s - %(levelname)s - %(name)s - %(message)s',\n",
    "    filename = logging_file_name,\n",
    "    filemode = 'w' # set to 'a' if you want to append, rather than overwrite each time.\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd3718b3-341f-4ed2-8e03-327c5d577884",
   "metadata": {},
   "source": [
    "## Setup - virtualenv jupyter kernel\n",
    "\n",
    "- Back to [Table of Contents](#Table-of-Contents)\n",
    "\n",
    "If you are using a virtualenv, make sure that you:\n",
    "\n",
    "- have installed your virtualenv as a kernel.\n",
    "- choose the kernel for your virtualenv as the kernel for your notebook (Kernel --> Change kernel).\n",
    "\n",
    "Since I use a virtualenv, need to get that activated somehow inside this notebook.  One option is to run `../dev/wsgi.py` in this notebook, to configure the python environment manually as if you had activated the `sourcenet` virtualenv.  To do this, you'd make a code cell that contains:\n",
    "\n",
    "    %run ../dev/wsgi.py\n",
    "    \n",
    "This is sketchy, however, because of the changes it makes to your Python environment within the context of whatever your current kernel is.  I'd worry about collisions with the actual Python 3 kernel.  Better, one can install their virtualenv as a separate kernel.  Steps:\n",
    "\n",
    "- activate your virtualenv:\n",
    "\n",
    "        workon research\n",
    "\n",
    "- in your virtualenv, install the package `ipykernel`.\n",
    "\n",
    "        pip install ipykernel\n",
    "\n",
    "- use the ipykernel python program to install the current environment as a kernel:\n",
    "\n",
    "        python -m ipykernel install --user --name <env_name> --display-name \"<display_name>\"\n",
    "        \n",
    "    `sourcenet` example:\n",
    "    \n",
    "        python -m ipykernel install --user --name sourcenet --display-name \"research (Python 3)\"\n",
    "        \n",
    "More details: [http://ipython.readthedocs.io/en/stable/install/kernel_install.html](http://ipython.readthedocs.io/en/stable/install/kernel_install.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78500ee3-1953-4c10-9e7b-5b26ea94952e",
   "metadata": {},
   "source": [
    "## Setup - Initialize Django\n",
    "\n",
    "- Back to [Table of Contents](#Table-of-Contents)\n",
    "\n",
    "First, initialize my dev django project, so I can run code in this notebook that references my django models and can talk to the database using my project's settings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0955ef1d-8300-4c01-9e1d-ef232e0646d8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-17T15:43:52.077172Z",
     "start_time": "2019-09-17T15:43:52.071107Z"
    }
   },
   "outputs": [],
   "source": [
    "# init django\n",
    "django_init_folder = \"/home/jonathanmorgan/work/django/research/research/work/phd_work\"\n",
    "django_init_path = \"django_init.py\"\n",
    "if( ( django_init_folder is not None ) and ( django_init_folder != \"\" ) ):\n",
    "    \n",
    "    # add folder to front of path.\n",
    "    django_init_path = \"{}/{}\".format( django_init_folder, django_init_path )\n",
    "    \n",
    "#-- END check to see if django_init folder. --#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "06d7b339-deb5-48fe-855a-f6f5dabc9ff7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-17T15:43:54.453200Z",
     "start_time": "2019-09-17T15:43:52.833671Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "django initialized at 2022-05-27 04:04:25.107908\n"
     ]
    }
   ],
   "source": [
    "%run $django_init_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c6a4e76a-de34-4d86-9e43-b74e4e662332",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "django model packages imported at 2022-05-27 04:06:00.596985\n"
     ]
    }
   ],
   "source": [
    "# django imports\n",
    "from django.contrib.auth.models import User\n",
    "from django.db.models import Max\n",
    "from django.db.models import Min\n",
    "\n",
    "# sourcenet imports\n",
    "from context_text.shared.context_text_base import ContextTextBase\n",
    "\n",
    "# context_analysis imports\n",
    "from context_analysis.network.network_person_info import NetworkPersonInfo\n",
    "\n",
    "# sourcenet imports\n",
    "from context_text.models import Article\n",
    "from context_text.models import Article_Author\n",
    "from context_text.models import Article_Data\n",
    "from context_text.models import Article_Subject\n",
    "from context_text.models import Newspaper\n",
    "from context_text.models import Person\n",
    "\n",
    "# article coding\n",
    "from context_text.article_coding.article_coder import ArticleCoder\n",
    "from context_text.article_coding.article_coding import ArticleCoding\n",
    "from context_text.article_coding.open_calais_v2.open_calais_v2_article_coder import OpenCalaisV2ArticleCoder\n",
    "from context_text.article_coding.open_calais_v2.open_calais_v2_api_response import OpenCalaisV2ApiResponse\n",
    "\n",
    "# article data collection\n",
    "from context_text.collectors.newsbank.newspapers.GRPB import GRPB\n",
    "from context_text.collectors.newsbank.newspapers.DTNB import DTNB\n",
    "\n",
    "# import class that actually processes requests for outputting networks.\n",
    "from context_text.export.network_output import NetworkOutput\n",
    "\n",
    "# context_text shared\n",
    "from context_text.shared.context_text_base import ContextTextBase\n",
    "\n",
    "print( \"django model packages imported at \" + str( datetime.datetime.now() ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "834b9dd9-c280-4622-a5b7-f5ebb34f3da4",
   "metadata": {},
   "source": [
    "## Setup - Initialize LoggingHelper\n",
    "\n",
    "- Back to [Table of Contents](#Table-of-Contents)\n",
    "\n",
    "Create a LoggingHelper instance to use to log debug and also print at the same time.\n",
    "\n",
    "Preconditions: Must be run after Django is initialized, since `python_utilities` is in the django path."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "54ddcce8-4812-4daa-9a22-1419f9861ca8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-17T15:43:55.604797Z",
     "start_time": "2019-09-17T15:43:55.589736Z"
    }
   },
   "outputs": [],
   "source": [
    "# python_utilities\n",
    "from python_utilities.logging.logging_helper import LoggingHelper\n",
    "\n",
    "# init\n",
    "my_logging_helper = LoggingHelper()\n",
    "my_logging_helper.set_logger_name( \"newsbank-article_coding-unittest\" )\n",
    "log_message = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fcc2eb0-805c-44ba-a465-69c5c0a3daae",
   "metadata": {},
   "source": [
    "## Setup - load fixtures and prepare database\n",
    "\n",
    "- Back to [Table of Contents](#Table-of-Contents)\n",
    "\n",
    "Load base unit test fixtures (uncoded):\n",
    "\n",
    "- `python manage.py loaddata context_text_unittest_auth_data.json`\n",
    "- `python manage.py loaddata context_text_unittest_django_config_data.json`\n",
    "- `python manage.py loaddata context_text_unittest_data.json`\n",
    "- `python manage.py loaddata context_text_unittest_taggit_data.json`\n",
    "\n",
    "If you load context_text unit test fixtures into a database (research_test), it will not have an OpenCalais token set in the django_configuration, nor will there be a staff user you can use to log in and poke around.\n",
    "\n",
    "To create the user whose credentials are stored here as a superuser:\n",
    "\n",
    "    python manage.py createsuperuser\n",
    "\n",
    "Create the superuser with the username and password above.\n",
    "\n",
    "Then, you can set the OpenCalais v.2 Access Token `django_config` property (application = “OpenCalais_REST_API_v2”; property name = “open_calais_access_token”) to your OpenCalais Token value.  This should let OpenCalais work correctly on this database."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6807bbbf-ecfb-4a1f-b6d2-2e326020192e",
   "metadata": {},
   "source": [
    "## Setup - shared variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1950db4c-a0f2-4e6e-b319-1a261e649732",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get ArticleCoding instance.\n",
    "#article_coding = ArticleCoding()\n",
    "\n",
    "# automated coding user\n",
    "automated_coder = ArticleCoder.get_automated_coding_user()\n",
    "\n",
    "# newspapers for Grand Rapids Press and Detroit News.\n",
    "grand_rapids_press = Newspaper.objects.get( newsbank_code = \"GRPB\" )\n",
    "detroit_news = Newspaper.objects.get( newsbank_code = \"DTNB\" )\n",
    "\n",
    "# OpenCalais v2 coder type\n",
    "ocv2_coder_type = OpenCalaisV2ArticleCoder.CONFIG_APPLICATION"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca3d85f8-5a4e-42fe-b868-ec82afa16c0d",
   "metadata": {},
   "source": [
    "# Find articles to be coded\n",
    "\n",
    "- Back to [Table of Contents](#Table-of-Contents)\n",
    "\n",
    "Tag all locally implemented hard news articles in database and all that have already been coded using Open Calais V2, then work through using OpenCalais to code all local hard news that hasn't alredy been coded, starting with those proximal to the coding sample for methods paper."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98193820-335b-4263-bd8d-d5f3dfa1b803",
   "metadata": {},
   "source": [
    "## which articles have already been coded?\n",
    "\n",
    "- Back to [Table of Contents](#Table-of-Contents)\n",
    "\n",
    "More precisely, find all articles that have Article_Data coded by the automated coder with type \"OpenCalais_REST_API_v2\" and tag the articles as \"coded-open_calais_v2\" or something like that.\n",
    "\n",
    "Then, for articles without that tag, use our criteria for local hard news to filter out and tag publications in the year before and after the month used to evaluate the automated coder, in both the Grand Rapids Press and the Detroit News, so I can look at longer time frames, then code all articles currently in database.\n",
    "\n",
    "Eventually, then, we'll code and examine before and after layoffs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "eb9ed921-13ca-4d38-b683-34b3b8f07434",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-17T16:46:33.532929Z",
     "start_time": "2019-09-17T16:46:33.526815Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-05-27 04:06:14.879123 - Loaded automated user: automated, id = 7\n"
     ]
    }
   ],
   "source": [
    "# look for publications that have article data:\n",
    "# - coded by automated coder\n",
    "# - with coder type of \"OpenCalais_REST_API_v2\"\n",
    "\n",
    "# get automated coder\n",
    "automated_coder_user = ArticleCoder.get_automated_coding_user()\n",
    "\n",
    "print( \"{} - Loaded automated user: {}, id = {}\".format( datetime.datetime.now(), automated_coder_user, automated_coder_user.id ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fd048431-10c5-4a4d-86f3-2a62aa3311a3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-17T16:46:36.155654Z",
     "start_time": "2019-09-17T16:46:36.145324Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'pub_date__max': datetime.date(2010, 2, 13), 'pub_date__min': datetime.date(2009, 12, 7)}\n"
     ]
    }
   ],
   "source": [
    "# try aggregates\n",
    "article_qs = Article.objects.all()\n",
    "pub_date_info = article_qs.aggregate( Max( 'pub_date' ), Min( 'pub_date' ) )\n",
    "print( pub_date_info )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "75aa15c5-e0d5-4ff6-a30b-a997e85d0a06",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-17T16:46:38.265503Z",
     "start_time": "2019-09-17T16:46:38.247164Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 46 articles\n"
     ]
    }
   ],
   "source": [
    "# find articles with Article_Data created by the automated user...\n",
    "article_qs = Article.objects.filter( article_data__coder = automated_coder_user )\n",
    "\n",
    "# ...and specifically coded using OpenCalais V2...\n",
    "article_qs = article_qs.filter( article_data__coder_type = OpenCalaisV2ArticleCoder.CONFIG_APPLICATION )\n",
    "\n",
    "# ...and finally, we just want the distinct articles by ID.\n",
    "article_qs = article_qs.order_by( \"id\" ).distinct( \"id\" )\n",
    "\n",
    "# count?\n",
    "article_count = article_qs.count()\n",
    "print( \"Found {} articles\".format( article_count ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ba0be6f-e582-485c-adea-6638d8959e6f",
   "metadata": {},
   "source": [
    "### Tag the coded articles\n",
    "\n",
    "- Back to [Table of Contents](#Table-of-Contents)\n",
    "\n",
    "Removing duplicates present from joining with Article_Data yields 579 articles that were coded by the automated coder.\n",
    "\n",
    "Tag all the coded articles with `OpenCalaisV2ArticleCoder.TAG_CODED_BY_ME`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1e833f54-16e3-4c99-bdd4-8a40f001ffe7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-17T16:46:46.423691Z",
     "start_time": "2019-09-17T16:46:46.293284Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Article counts:\n",
      "- total articles: 46\n",
      "- untagged articles: 0\n",
      "- already tagged: 46\n",
      "- newly tagged: 0\n",
      "- count sum: 46\n"
     ]
    }
   ],
   "source": [
    "# declare variables\n",
    "current_article = None\n",
    "tag_name_list = None\n",
    "article_count = None\n",
    "untagged_count = None\n",
    "already_tagged_count = None\n",
    "newly_tagged_count = None\n",
    "count_sum = None\n",
    "do_add_tag = False\n",
    "\n",
    "# init\n",
    "do_add_tag = False\n",
    "\n",
    "# get article_count\n",
    "article_count = article_qs.count()\n",
    "\n",
    "# loop over articles.\n",
    "untagged_count = 0\n",
    "already_tagged_count = 0\n",
    "newly_tagged_count = 0\n",
    "for current_article in article_qs:\n",
    "    \n",
    "    # get list of tags for this publication\n",
    "    tag_name_list = current_article.tags.names()\n",
    "    \n",
    "    # is the coded tag in the list?\n",
    "    if ( OpenCalaisV2ArticleCoder.TAG_CODED_BY_ME not in tag_name_list ):\n",
    "        \n",
    "        # are we adding tag?\n",
    "        if ( do_add_tag == True ):\n",
    "\n",
    "            # add tag.\n",
    "            current_article.tags.add( OpenCalaisV2ArticleCoder.TAG_CODED_BY_ME )\n",
    "            newly_tagged_count += 1\n",
    "            \n",
    "        else:\n",
    "\n",
    "            # for now, increment untagged count\n",
    "            untagged_count += 1\n",
    "            \n",
    "        #-- END check to see if we are adding tag. --#\n",
    "        \n",
    "    else:\n",
    "        \n",
    "        # already tagged\n",
    "        already_tagged_count += 1\n",
    "        \n",
    "    #-- END check to see if coded tag is set --#\n",
    "    \n",
    "#-- END loop over articles. --#\n",
    "\n",
    "print( \"Article counts:\" )\n",
    "print( \"- total articles: {}\".format( article_count ) )\n",
    "print( \"- untagged articles: {}\".format( untagged_count ) )\n",
    "print( \"- already tagged: {}\".format( already_tagged_count ) )\n",
    "print( \"- newly tagged: {}\".format( newly_tagged_count ) )\n",
    "count_sum = untagged_count + already_tagged_count + newly_tagged_count\n",
    "print( \"- count sum: {}\".format( count_sum ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8103acb-8361-4954-bfdc-f4af3b2a13fa",
   "metadata": {},
   "source": [
    "### Profile the coded articles\n",
    "\n",
    "- Back to [Table of Contents](#Table-of-Contents)\n",
    "\n",
    "Look at range of pub dates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7a009481-e5a3-42e2-8ba9-b234043255b0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-03T03:52:35.494718Z",
     "start_time": "2019-08-03T03:52:35.473029Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matching article count: 46\n"
     ]
    }
   ],
   "source": [
    "tags_in_list = []\n",
    "tags_in_list.append( OpenCalaisV2ArticleCoder.TAG_CODED_BY_ME )\n",
    "article_qs = Article.objects.filter( tags__name__in = tags_in_list )\n",
    "print( \"Matching article count: {}\".format( article_qs.count() ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12d7a6bb-baab-47ef-ad6b-d44288af48c9",
   "metadata": {},
   "source": [
    "- Original: 579\n",
    "- after coding 10: 589 (tag is being set correctly by Open Calais V2 coder)\n",
    "- 2019.08.02 - after 5000 (minus a few errors because 2 seconds isn't quite enough for rate limit): 5518"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fb756371-8206-4ca9-b5a8-dbe04b910af5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-31T17:49:07.328924Z",
     "start_time": "2019-07-31T17:49:07.275399Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'pub_date__max': datetime.date(2010, 2, 13), 'pub_date__min': datetime.date(2009, 12, 7)}\n",
      "- 2009-12-07 ( <class 'datetime.date'> ) count: 2\n",
      "- 2009-12-08 ( <class 'datetime.date'> ) count: 5\n",
      "- 2009-12-09 ( <class 'datetime.date'> ) count: 6\n",
      "- 2009-12-10 ( <class 'datetime.date'> ) count: 7\n",
      "- 2009-12-11 ( <class 'datetime.date'> ) count: 4\n",
      "- 2009-12-12 ( <class 'datetime.date'> ) count: 3\n",
      "- 2010-02-07 ( <class 'datetime.date'> ) count: 1\n",
      "- 2010-02-08 ( <class 'datetime.date'> ) count: 4\n",
      "- 2010-02-09 ( <class 'datetime.date'> ) count: 2\n",
      "- 2010-02-11 ( <class 'datetime.date'> ) count: 6\n",
      "- 2010-02-12 ( <class 'datetime.date'> ) count: 1\n",
      "- 2010-02-13 ( <class 'datetime.date'> ) count: 5\n"
     ]
    }
   ],
   "source": [
    "# profile these publications\n",
    "min_pub_date = None\n",
    "max_pub_date = None\n",
    "current_pub_date = None\n",
    "pub_date_count = None\n",
    "date_to_count_map = {}\n",
    "date_to_articles_map = {}\n",
    "pub_date_article_dict = None\n",
    "\n",
    "# try aggregates\n",
    "pub_date_info = article_qs.aggregate( Max( 'pub_date' ), Min( 'pub_date' ) )\n",
    "print( pub_date_info )\n",
    "\n",
    "# counts of pubs by date\n",
    "for current_article in article_qs:\n",
    "    \n",
    "    # get pub_date\n",
    "    current_pub_date = current_article.pub_date\n",
    "    current_article_id = current_article.id\n",
    "    \n",
    "    # get count, increment, and store.\n",
    "    pub_date_count = date_to_count_map.get( current_pub_date, 0 )\n",
    "    pub_date_count += 1\n",
    "    date_to_count_map[ current_pub_date ] = pub_date_count\n",
    "    \n",
    "    # also, store up ids and instances\n",
    "    \n",
    "    # get dict of article ids to article instances for date\n",
    "    pub_date_article_dict = date_to_articles_map.get( current_pub_date, {} )\n",
    "    \n",
    "    # article already there?\n",
    "    if ( current_article_id not in pub_date_article_dict ):\n",
    "        \n",
    "        # no - add it.\n",
    "        pub_date_article_dict[ current_article_id ] = current_article\n",
    "        \n",
    "    #-- END check to see if article already there.\n",
    "    \n",
    "    # put dict back.\n",
    "    date_to_articles_map[ current_pub_date ] = pub_date_article_dict\n",
    "    \n",
    "#-- END loop over articles. --#\n",
    "\n",
    "# output dates and counts.\n",
    "\n",
    "# get list of keys from map\n",
    "keys_list = list( six.viewkeys( date_to_count_map ) )\n",
    "keys_list.sort()\n",
    "for current_pub_date in keys_list:\n",
    "    \n",
    "    # get count\n",
    "    pub_date_count = date_to_count_map.get( current_pub_date, 0 )\n",
    "    print( \"- {} ( {} ) count: {}\".format( current_pub_date, type( current_pub_date ), pub_date_count ) )\n",
    "    \n",
    "#-- END loop over dates --#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "203740fa-bc90-437a-9af6-3fe7298ca213",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-31T17:49:13.567482Z",
     "start_time": "2019-07-31T17:49:13.535813Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==> Article 91132: 91132 - Dec 10, 2009, Business ( 3E ), UID: 12C83AAD9AF87EC8 - Customized classic cars go green ( Detroit News, The )\n",
      "- tags: <QuerySet [<Tag: coded-OpenCalaisV2ArticleCoder>]>\n",
      "----> article_data: 105 - user1 - no coder_type -- Article: 91132 - Dec 10, 2009, Business ( 3E ), UID: 12C83AAD9AF87EC8 - Customized classic cars go green ( Detroit News, The )\n",
      "----> article_data: 142 - user2 - no coder_type -- Article: 91132 - Dec 10, 2009, Business ( 3E ), UID: 12C83AAD9AF87EC8 - Customized classic cars go green ( Detroit News, The )\n",
      "----> article_data: 237 - automated ( ADCT: OpenCalais_REST_API_v2 )  -- Article: 91132 - Dec 10, 2009, Business ( 3E ), UID: 12C83AAD9AF87EC8 - Customized classic cars go green ( Detroit News, The )\n",
      "\n",
      "==> Article 91112: 91112 - Dec 10, 2009, Business ( 4B ), UID: 12C83AAEDE938B90 - Chevy's new boss to retire ( Detroit News, The )\n",
      "- tags: <QuerySet [<Tag: coded-OpenCalaisV2ArticleCoder>]>\n",
      "----> article_data: 106 - user1 - no coder_type -- Article: 91112 - Dec 10, 2009, Business ( 4B ), UID: 12C83AAEDE938B90 - Chevy's new boss to retire ( Detroit News, The )\n",
      "----> article_data: 140 - user2 - no coder_type -- Article: 91112 - Dec 10, 2009, Business ( 4B ), UID: 12C83AAEDE938B90 - Chevy's new boss to retire ( Detroit News, The )\n",
      "----> article_data: 240 - automated ( ADCT: OpenCalais_REST_API_v2 )  -- Article: 91112 - Dec 10, 2009, Business ( 4B ), UID: 12C83AAEDE938B90 - Chevy's new boss to retire ( Detroit News, The )\n",
      "\n",
      "==> Article 91157: 91157 - Dec 10, 2009, Business ( 8B ), UID: 12C83AADA29CFA28 - Extension of bailout could benefit GMAC ( Detroit News, The )\n",
      "- tags: <QuerySet [<Tag: coded-OpenCalaisV2ArticleCoder>]>\n",
      "----> article_data: 102 - user1 - no coder_type -- Article: 91157 - Dec 10, 2009, Business ( 8B ), UID: 12C83AADA29CFA28 - Extension of bailout could benefit GMAC ( Detroit News, The )\n",
      "----> article_data: 143 - user2 - no coder_type -- Article: 91157 - Dec 10, 2009, Business ( 8B ), UID: 12C83AADA29CFA28 - Extension of bailout could benefit GMAC ( Detroit News, The )\n",
      "----> article_data: 243 - automated ( ADCT: OpenCalais_REST_API_v2 )  -- Article: 91157 - Dec 10, 2009, Business ( 8B ), UID: 12C83AADA29CFA28 - Extension of bailout could benefit GMAC ( Detroit News, The )\n",
      "\n",
      "==> Article 21738: 21738 - Dec 10, 2009, Business ( A12 ), UID: 12C893C9821CF460 - $2.1 million is local share in class action - Visa, MasterCard foot the bill in $1.1 billion settlement ( Grand Rapids Press, The )\n",
      "- tags: <QuerySet [<Tag: coded-OpenCalaisV2ArticleCoder>, <Tag: local_hard_news>]>\n",
      "----> article_data: 108 - user1 - no coder_type -- Article: 21738 - Dec 10, 2009, Business ( A12 ), UID: 12C893C9821CF460 - $2.1 million is local share in class action - Visa, MasterCard foot the bill in $1.1 billion settlement ( Grand Rapids Press, The )\n",
      "----> article_data: 136 - user2 - no coder_type -- Article: 21738 - Dec 10, 2009, Business ( A12 ), UID: 12C893C9821CF460 - $2.1 million is local share in class action - Visa, MasterCard foot the bill in $1.1 billion settlement ( Grand Rapids Press, The )\n",
      "----> article_data: 239 - automated ( ADCT: OpenCalais_REST_API_v2 )  -- Article: 21738 - Dec 10, 2009, Business ( A12 ), UID: 12C893C9821CF460 - $2.1 million is local share in class action - Visa, MasterCard foot the bill in $1.1 billion settlement ( Grand Rapids Press, The )\n",
      "\n",
      "==> Article 21719: 21719 - Dec 10, 2009, City and Region ( A6 ), UID: 12C893C993107670 - Both drivers share blame in crash, traffic expert says - Man ordered to stand trial in fatal accident ( Grand Rapids Press, The )\n",
      "- tags: <QuerySet [<Tag: coded-OpenCalaisV2ArticleCoder>, <Tag: local_hard_news>]>\n",
      "----> article_data: 107 - user1 ( ADCT: coder_type_filter_test )  -- Article: 21719 - Dec 10, 2009, City and Region ( A6 ), UID: 12C893C993107670 - Both drivers share blame in crash, traffic expert says - Man ordered to stand trial in fatal accident ( Grand Rapids Press, The )\n",
      "----> article_data: 138 - user2 ( ADCT: coder_type_filter_test )  -- Article: 21719 - Dec 10, 2009, City and Region ( A6 ), UID: 12C893C993107670 - Both drivers share blame in crash, traffic expert says - Man ordered to stand trial in fatal accident ( Grand Rapids Press, The )\n",
      "----> article_data: 238 - automated ( ADCT: OpenCalais_REST_API_v2 )  -- Article: 21719 - Dec 10, 2009, City and Region ( A6 ), UID: 12C893C993107670 - Both drivers share blame in crash, traffic expert says - Man ordered to stand trial in fatal accident ( Grand Rapids Press, The )\n",
      "\n",
      "==> Article 91133: 91133 - Dec 10, 2009, Metro ( 4A ), UID: 12C83AAEB4C5AC90 - Bond granted for woman in child abuse case ( Detroit News, The )\n",
      "- tags: <QuerySet [<Tag: coded-OpenCalaisV2ArticleCoder>]>\n",
      "----> article_data: 99 - user1 - no coder_type -- Article: 91133 - Dec 10, 2009, Metro ( 4A ), UID: 12C83AAEB4C5AC90 - Bond granted for woman in child abuse case ( Detroit News, The )\n",
      "----> article_data: 147 - user2 - no coder_type -- Article: 91133 - Dec 10, 2009, Metro ( 4A ), UID: 12C83AAEB4C5AC90 - Bond granted for woman in child abuse case ( Detroit News, The )\n",
      "----> article_data: 242 - automated ( ADCT: OpenCalais_REST_API_v2 )  -- Article: 91133 - Dec 10, 2009, Metro ( 4A ), UID: 12C83AAEB4C5AC90 - Bond granted for woman in child abuse case ( Detroit News, The )\n",
      "\n",
      "==> Article 91114: 91114 - Dec 10, 2009, Metro ( 7A ), UID: 12C83AAE358F7CA0 - 5 firms in Mich. to get grants ( Detroit News, The )\n",
      "- tags: <QuerySet [<Tag: coded-OpenCalaisV2ArticleCoder>]>\n",
      "----> article_data: 101 - user1 - no coder_type -- Article: 91114 - Dec 10, 2009, Metro ( 7A ), UID: 12C83AAE358F7CA0 - 5 firms in Mich. to get grants ( Detroit News, The )\n",
      "----> article_data: 145 - user2 - no coder_type -- Article: 91114 - Dec 10, 2009, Metro ( 7A ), UID: 12C83AAE358F7CA0 - 5 firms in Mich. to get grants ( Detroit News, The )\n",
      "----> article_data: 241 - automated ( ADCT: OpenCalais_REST_API_v2 )  -- Article: 91114 - Dec 10, 2009, Metro ( 7A ), UID: 12C83AAE358F7CA0 - 5 firms in Mich. to get grants ( Detroit News, The )\n"
     ]
    }
   ],
   "source": [
    "# look at articles for a particular date\n",
    "focus_date = \"2009-12-10\"\n",
    "pub_date = datetime.datetime.strptime( focus_date, \"%Y-%m-%d\" ).date()\n",
    "articles_for_date = date_to_articles_map.get( pub_date, {} )\n",
    "\n",
    "# get each article\n",
    "for article_id, article_instance in articles_for_date.items():\n",
    "    \n",
    "    # look at its tags.\n",
    "    print( \"\\n==> Article {article_id}: {article_summary}\".format( article_id = article_id, article_summary = article_instance ) )\n",
    "    print( \"- tags: {}\".format( article_instance.tags.all() ) )\n",
    "\n",
    "    # loop over associated Article_Data instances.\n",
    "    for article_data in article_instance.article_data_set.all():\n",
    "\n",
    "        print( \"----> article_data: {}\".format( article_data ) )\n",
    "\n",
    "    #-- END loop over associated Article_Data instances --#\n",
    "\n",
    "#-- END loop over articles for date."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2d5722d-ad2b-44ed-8247-9235dd1b514b",
   "metadata": {},
   "source": [
    "## tag all local news\n",
    "\n",
    "- Back to [Table of Contents](#Table-of-Contents)\n",
    "\n",
    "Definition of local hard news by in-house implementor for Grand Rapids Press and Detroit News follow.  For each, tag all articles in database that match as \"local_hard_news\"."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e221ed7c-15c7-45db-9191-df1f6da06334",
   "metadata": {},
   "source": [
    "### TODO\n",
    "\n",
    "- Back to [Table of Contents](#Table-of-Contents)\n",
    "\n",
    "TODO:\n",
    "\n",
    "- make class for GRPB at NewsBank.\n",
    "\n",
    "    - also, pull the things that are newspaper specific out of ArticleCoder.py and into the GRPB.py class.\n",
    "\n",
    "- refine \"local news\" and \"locally created\" regular expressions for Grand Rapids Press based on contents of `author_string` and `author_affiliation`.\n",
    "- do the same for TDN.\n",
    "- then, use the updated classes and definitions below to flag all local hard news in database for each publication."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16b068cd-751c-426c-adb1-01c32770394b",
   "metadata": {},
   "source": [
    "#### DONE\n",
    "\n",
    "- Back to [Table of Contents](#Table-of-Contents)\n",
    "\n",
    "DONE:\n",
    "\n",
    "- abstract out shared stuff from GRPB.py and DTNB.py into abstract parent class context_text/collectors/newsbank/newspapers/newsbank_newspaper.py\n",
    "\n",
    "    - update DTNB.py to use the parent class.\n",
    "    \n",
    "- make class for GRPB at NewsBank.\n",
    "\n",
    "    - context_text/collectors/newsbank/newspapers/GRPB.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2265bf69-d618-4298-bf26-2a4ce2a28392",
   "metadata": {},
   "source": [
    "### Grand Rapids Press local news\n",
    "\n",
    "- Back to [Table of Contents](#Table-of-Contents)\n",
    "\n",
    "Grand Rapids Press local hard news:\n",
    "\n",
    "- `context_text/examples/articles/articles-GRP-local_news.py`\n",
    "- local hard news sections (stored in `Article.GRP_NEWS_SECTION_NAME_LIST`):\n",
    "\n",
    "    - \"Business\"\n",
    "    - \"City and Region\"\n",
    "    - \"Front Page\"\n",
    "    - \"Lakeshore\"\n",
    "    - \"Religion\"\n",
    "    - \"Special\"\n",
    "    - \"State\"\n",
    "\n",
    "- in-house implementor (based on byline patterns, stored in `sourcenet.models.Article.Q_GRP_IN_HOUSE_AUTHOR`):\n",
    "\n",
    "    - Byline ends in \"/ THE GRAND RAPIDS PRESS\", ignore case.\n",
    "\n",
    "        - `Q( author_varchar__iregex = r'.* */ *THE GRAND RAPIDS PRESS$'`\n",
    "\n",
    "    - Byline ends in \"/ PRESS * EDITOR\", ignore case.\n",
    "\n",
    "        - `Q( author_varchar__iregex = r'.* */ *PRESS .* EDITOR$' )`\n",
    "\n",
    "    - Byline ends in \"/ GRAND RAPIDS PRESS * BUREAU\", ignore case.\n",
    "\n",
    "        - `Q( author_varchar__iregex = r'.* */ *GRAND RAPIDS PRESS .* BUREAU$' )`\n",
    "\n",
    "    - Byline ends in \"/ SPECIAL TO THE PRESS\", ignore case.\n",
    "\n",
    "        - `Q( author_varchar__iregex = r'.* */ *SPECIAL TO THE PRESS$' )`\n",
    "        \n",
    "- can also exclude columns (I will not):\n",
    "\n",
    "        grp_article_qs = grp_article_qs.exclude( index_terms__icontains = \"Column\" )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da3e0658-feb9-4c42-b2d4-1aaa7d23ee76",
   "metadata": {},
   "source": [
    "Need to work to further refine this.\n",
    "\n",
    "Looking at affiliation strings:\n",
    "\n",
    "    SELECT author_affiliation, COUNT( author_affiliation ) as affiliation_count\n",
    "    FROM context_text_article\n",
    "    WHERE newspaper_id = 1\n",
    "    GROUP BY author_affiliation\n",
    "    ORDER BY COUNT( author_affiliation ) DESC;\n",
    "    \n",
    "And at author strings for collective bylines:\n",
    "\n",
    "    SELECT author_string, COUNT( author_string ) as author_count\n",
    "    FROM context_text_article\n",
    "    WHERE newspaper_id = 1\n",
    "    GROUP BY author_string\n",
    "    ORDER BY COUNT( author_string ) DESC\n",
    "    LIMIT 10;\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "908fed9a-c6e9-455e-a0d6-9f6aac57354f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-13T02:22:10.778554Z",
     "start_time": "2019-09-13T02:22:10.521947Z"
    }
   },
   "outputs": [],
   "source": [
    "# filter queryset to just locally created Grand Rapids Press (GRP) articles.\n",
    "# imports\n",
    "from context_text.models import Article\n",
    "from context_text.models import Newspaper\n",
    "from context_text.shared.context_text_base import ContextTextBase\n",
    "from context_text.collectors.newsbank.newspapers.GRPB import GRPB\n",
    "\n",
    "# declare variables - Grand Rapids Press\n",
    "do_apply_tag = False\n",
    "tag_to_apply = None\n",
    "grp_local_news_sections = []\n",
    "grp_newspaper = None\n",
    "grp_article_qs = None\n",
    "article_count = -1\n",
    "\n",
    "# declare variables - filtering\n",
    "include_opinion_columns = True\n",
    "tags_in_list = []\n",
    "tags_not_in_list = []\n",
    "filter_out_prelim_tags = False\n",
    "random_count = -1\n",
    "\n",
    "# declare variables - make list of article IDs from QS.\n",
    "article_id_list = []\n",
    "article_counter = -1\n",
    "current_article = None\n",
    "article_tag_name_list = None\n",
    "article_update_counter = -1\n",
    "\n",
    "# ==> configure\n",
    "\n",
    "# configure - size of random sample we want\n",
    "#random_count = 60\n",
    "\n",
    "# configure - also, apply tag?\n",
    "do_apply_tag = False\n",
    "tag_to_apply = ContextTextBase.TAG_LOCAL_HARD_NEWS\n",
    "\n",
    "# set up \"local, regional and state news\" sections\n",
    "grp_local_news_sections = GRPB.LOCAL_NEWS_SECTION_NAME_LIST\n",
    "\n",
    "# Grand Rapids Press\n",
    "# get newspaper instance for GRP.\n",
    "grp_newspaper = Newspaper.objects.get( id = GRPB.NEWSPAPER_ID )\n",
    "\n",
    "# start with all articles\n",
    "#grp_article_qs = Article.objects.all()\n",
    "\n",
    "# ==> filter to newspaper, local news section list, and in-house reporters.\n",
    "\n",
    "# ----> manually\n",
    "\n",
    "# now, need to find local news articles to test on.\n",
    "#grp_article_qs = grp_article_qs.filter( newspaper = grp_newspaper )\n",
    "\n",
    "# only the locally implemented sections\n",
    "#grp_article_qs = grp_article_qs.filter( section__in = grp_local_news_sections )\n",
    "\n",
    "# and, with an in-house author\n",
    "#grp_article_qs = grp_article_qs.filter( Article.Q_GRP_IN_HOUSE_AUTHOR )\n",
    "\n",
    "#print( \"manual filter count: {}\".format( grp_article_qs.count() ) )\n",
    "\n",
    "# ----> using Article.filter_articles()\n",
    "grp_article_qs = Article.filter_articles( qs_IN = grp_article_qs,\n",
    "                                          newspaper = grp_newspaper,\n",
    "                                          section_name_list = grp_local_news_sections,\n",
    "                                          custom_article_q = GRPB.Q_IN_HOUSE_AUTHOR )\n",
    "\n",
    "print( \"Article.filter_articles count: {}\".format( grp_article_qs.count() ) )\n",
    "\n",
    "# and include opinion columns?\n",
    "if ( include_opinion_columns == False ):\n",
    "    \n",
    "    # do not include columns\n",
    "    grp_article_qs = grp_article_qs.exclude( index_terms__icontains = \"Column\" )\n",
    "    \n",
    "#-- END check to see if we include columns. --#\n",
    "\n",
    "'''\n",
    "# filter to newspaper, section list, and in-house reporters.\n",
    "grp_article_qs = Article.filter_articles( qs_IN = grp_article_qs,\n",
    "                                          start_date = \"2009-12-01\",\n",
    "                                          end_date = \"2009-12-31\",\n",
    "                                          newspaper = grp_newspaper,\n",
    "                                          section_name_list = grp_local_news_sections,\n",
    "                                          custom_article_q = Article.Q_GRP_IN_HOUSE_AUTHOR )\n",
    "'''\n",
    "\n",
    "# how many is that?\n",
    "article_count = grp_article_qs.count()\n",
    "\n",
    "print( \"Article count before filtering on tags: \" + str( article_count ) )\n",
    "\n",
    "# ==> tags\n",
    "\n",
    "# tags to exclude\n",
    "tags_not_in_list = []\n",
    "\n",
    "# Example: prelim-related tags\n",
    "#tags_not_in_list.append( \"prelim_reliability\" )\n",
    "#tags_not_in_list.append( \"prelim_network\" ]\n",
    "#tags_not_in_list.append( \"minnesota1-20160328\" )\n",
    "#tags_not_in_list.append( \"minnesota2-20160328\" )\n",
    "\n",
    "# for later - exclude articles already coded.\n",
    "#tags_not_in_list.append( OpenCalaisV2ArticleCoder.TAG_CODED_BY_ME )\n",
    "\n",
    "# exclude any already tagged with tag_to_apply\n",
    "tags_not_in_list.append( tag_to_apply )\n",
    "\n",
    "if ( ( tags_not_in_list is not None ) and ( len( tags_not_in_list ) > 0 ) ):\n",
    "\n",
    "    # exclude those in a list\n",
    "    print( \"filtering out articles with tags: \" + str( tags_not_in_list ) )\n",
    "    grp_article_qs = grp_article_qs.exclude( tags__name__in = tags_not_in_list )\n",
    "\n",
    "#-- END check to see if we have a specific list of tags we want to exclude --#\n",
    "\n",
    "# include only those with certain tags.\n",
    "tags_in_list = []\n",
    "\n",
    "# Examples\n",
    "\n",
    "# Examples: prelim-related tags\n",
    "#tags_in_list.append( \"prelim_unit_test_001\" )\n",
    "#tags_in_list.append( \"prelim_unit_test_002\" )\n",
    "#tags_in_list.append( \"prelim_unit_test_003\" )\n",
    "#tags_in_list.append( \"prelim_unit_test_004\" )\n",
    "#tags_in_list.append( \"prelim_unit_test_005\" )\n",
    "#tags_in_list.append( \"prelim_unit_test_006\" )\n",
    "#tags_in_list.append( \"prelim_unit_test_007\" )\n",
    "\n",
    "# Example: grp_month\n",
    "#tags_in_list.append( \"grp_month\" )\n",
    "\n",
    "if ( ( tags_in_list is not None ) and ( len( tags_in_list ) > 0 ) ):\n",
    "\n",
    "    # filter\n",
    "    print( \"filtering to just articles with tags: \" + str( tags_in_list ) )\n",
    "    grp_article_qs = grp_article_qs.filter( tags__name__in = tags_in_list )\n",
    "    \n",
    "#-- END check to see if we have a specific list of tags we want to include --#\n",
    "\n",
    "# filter out \"*prelim*\" tags?\n",
    "#filter_out_prelim_tags = True\n",
    "if ( filter_out_prelim_tags == True ):\n",
    "\n",
    "    # ifilter out all articles with any tag whose name contains \"prelim\".\n",
    "    print( \"filtering out articles with tags that contain \\\"prelim\\\"\" )\n",
    "    grp_article_qs = grp_article_qs.exclude( tags__name__icontains = \"prelim\" )\n",
    "    \n",
    "#-- END check to see if we filter out \"prelim_*\" tags --#\n",
    "\n",
    "# how many is that?\n",
    "article_count = grp_article_qs.count()\n",
    "\n",
    "print( \"Article count after tag filtering: \" + str( article_count ) )\n",
    "\n",
    "# do we want a random sample?\n",
    "if ( random_count > 0 ):\n",
    "\n",
    "    # to get random, order them by \"?\", then use slicing to retrieve requested\n",
    "    #     number.\n",
    "    grp_article_qs = grp_article_qs.order_by( \"?\" )[ : random_count ]\n",
    "    \n",
    "#-- END check to see if we want random sample --#\n",
    "\n",
    "# this is a nice algorithm, also:\n",
    "# - http://www.titov.net/2005/09/21/do-not-use-order-by-rand-or-how-to-get-random-rows-from-table/\n",
    "\n",
    "# make ID list, tag articles if configured to.\n",
    "article_id_list = []\n",
    "article_counter = 0\n",
    "article_update_counter = 0\n",
    "for current_article in grp_article_qs:\n",
    "\n",
    "    # increment article_counter\n",
    "    article_counter += 1\n",
    "\n",
    "    # add IDs to article_id_list\n",
    "    article_id_list.append( str( current_article.id ) )\n",
    "    \n",
    "    # apply a tag while we are at it?\n",
    "    if ( ( do_apply_tag == True ) and ( tag_to_apply is not None ) and ( tag_to_apply != \"\" ) ):\n",
    "    \n",
    "        # yes, please.  Tag already present?\n",
    "        article_tag_name_list = current_article.tags.names()\n",
    "        if ( tag_to_apply not in article_tag_name_list ):\n",
    "\n",
    "            # Add tag.\n",
    "            current_article.tags.add( tag_to_apply )\n",
    "            \n",
    "            # increment counter\n",
    "            article_update_counter += 1\n",
    "            \n",
    "        #-- END check to see if tag already present. --#\n",
    "        \n",
    "    #-- END check to see if we apply tag. --#\n",
    "\n",
    "    # output the tags.\n",
    "    if ( debug_flag == True ):\n",
    "        print( \"- Tags for article \" + str( current_article.id ) + \" : \" + str( current_article.tags.all() ) )\n",
    "    #-- END DEBUG --#\n",
    "\n",
    "#-- END loop over articles --#\n",
    "\n",
    "# output the list.\n",
    "print( \"grp_article_qs count: {}\".format( grp_article_qs.count() ) )\n",
    "print( \"Found \" + str( article_counter ) + \" articles ( \" + str( article_count ) + \" ).\" )\n",
    "print( \"- Updated {} articles to add tag {}.\".format( article_update_counter, tag_to_apply ) )\n",
    "if ( debug_flag == True ):\n",
    "    print( \"List of \" + str( len( article_id_list ) ) + \" local GRP staff article IDs: \" + \", \".join( article_id_list ) )\n",
    "#-- END DEBUG --#\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14f263d7-f76b-4922-a6ef-5383541353a3",
   "metadata": {},
   "source": [
    "### Detroit News local news\n",
    "\n",
    "- Back to [Table of Contents](#Table-of-Contents)\n",
    "\n",
    "Detroit News local news:\n",
    "\n",
    "- `context_text/examples/articles/articles-TDN-local_news.py`\n",
    "- local hard news sections (stored in `from context_text.collectors.newsbank.newspapers.DTNB import DTNB` - `DTNB.NEWS_SECTION_NAME_LIST`):\n",
    "\n",
    "    - \"Business\"\n",
    "    - \"Metro\"\n",
    "    - \"Nation\" - because of auto industry stories\n",
    "\n",
    "- in-house implementor (based on byline patterns, stored in `DTNB.Q_IN_HOUSE_AUTHOR`):\n",
    "\n",
    "    - Byline ends in \"/ The Detroit News\", ignore case.\n",
    "\n",
    "        - `Q( author_varchar__iregex = r'.*\\s*/\\s*the\\s*detroit\\s*news$' )`\n",
    "\n",
    "    - Byline ends in \"Special to The Detroit News\", ignore case.\n",
    "\n",
    "        - `Q( author_varchar__iregex = r'.*\\s*/\\s*special\\s*to\\s*the\\s*detroit\\s*news$' )`\n",
    "\n",
    "    - Byline ends in \"Detroit News * Bureau\", ignore case.\n",
    "\n",
    "        - `Q( author_varchar__iregex = r'.*\\s*/\\s*detroit\\s*news\\s*.*\\s*bureau$' )`   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c9d047a-a210-405b-a3a1-3df84ce39a89",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-04T02:42:42.445226Z",
     "start_time": "2019-07-04T02:42:39.261361Z"
    }
   },
   "outputs": [],
   "source": [
    "# filter queryset to just locally created Detroit News (TDN) articles.\n",
    "# imports\n",
    "from context_text.models import Article\n",
    "from context_text.models import Newspaper\n",
    "from context_text.shared.context_text_base import ContextTextBase\n",
    "from context_text.collectors.newsbank.newspapers.DTNB import DTNB\n",
    "\n",
    "# declare variables - Detroit News\n",
    "do_apply_tag = False\n",
    "tag_to_apply = None\n",
    "tdn_local_news_sections = []\n",
    "tdn_newspaper = None\n",
    "tdn_article_qs = None\n",
    "article_count = -1\n",
    "\n",
    "# declare variables - filtering\n",
    "include_opinion_columns = True\n",
    "tags_in_list = []\n",
    "tags_not_in_list = []\n",
    "filter_out_prelim_tags = False\n",
    "random_count = -1\n",
    "\n",
    "# declare variables - make list of article IDs from QS.\n",
    "article_id_list = []\n",
    "article_counter = -1\n",
    "current_article = None\n",
    "\n",
    "# ==> configure\n",
    "\n",
    "# configure - size of random sample we want\n",
    "#random_count = 60\n",
    "\n",
    "# configure - also, apply tag?\n",
    "do_apply_tag = False\n",
    "tag_to_apply = ContextTextBase.TAG_LOCAL_HARD_NEWS\n",
    "\n",
    "# set up \"local, regional and state news\" sections\n",
    "tdn_local_news_sections = DTNB.LOCAL_NEWS_SECTION_NAME_LIST\n",
    "\n",
    "# Detroit News\n",
    "# get newspaper instance for TDN.\n",
    "tdn_newspaper = Newspaper.objects.get( id = DTNB.NEWSPAPER_ID )\n",
    "\n",
    "# start with all articles\n",
    "#tdn_article_qs = Article.objects.all()\n",
    "\n",
    "# ==> filter to newspaper, local news section list, and in-house reporters.\n",
    "\n",
    "# ----> manually\n",
    "\n",
    "# now, need to find local news articles to test on.\n",
    "#tdn_article_qs = tdn_article_qs.filter( newspaper = tdn_newspaper )\n",
    "\n",
    "# only the locally implemented sections\n",
    "#tdn_article_qs = tdn_article_qs.filter( section__in = tdn_local_news_sections )\n",
    "\n",
    "# and, with an in-house author\n",
    "#tdn_article_qs = tdn_article_qs.filter( DTNB.Q_IN_HOUSE_AUTHOR )\n",
    "\n",
    "#print( \"manual filter count: {}\".format( tdn_article_qs.count() ) )\n",
    "\n",
    "# ----> using Article.filter_articles()\n",
    "tdn_article_qs = Article.filter_articles( qs_IN = tdn_article_qs,\n",
    "                                          newspaper = tdn_newspaper,\n",
    "                                          section_name_list = tdn_local_news_sections,\n",
    "                                          custom_article_q = DTNB.Q_IN_HOUSE_AUTHOR )\n",
    "\n",
    "print( \"Article.filter_articles count: {}\".format( tdn_article_qs.count() ) )\n",
    "\n",
    "# and include opinion columns?\n",
    "if ( include_opinion_columns == False ):\n",
    "    \n",
    "    # do not include columns\n",
    "    tdn_article_qs = tdn_article_qs.exclude( author_string__in = DTNB.COLUMNIST_NAME_LIST )\n",
    "    \n",
    "#-- END check to see if we include columns. --#\n",
    "\n",
    "'''\n",
    "# filter to newspaper, section list, and in-house reporters.\n",
    "tdn_article_qs = Article.filter_articles( qs_IN = tdn_article_qs,\n",
    "                                          start_date = \"2009-12-01\",\n",
    "                                          end_date = \"2009-12-31\",\n",
    "                                          newspaper = tdn_newspaper,\n",
    "                                          section_name_list = tdn_local_news_sections,\n",
    "                                          custom_article_q = DTNB.Q_IN_HOUSE_AUTHOR )\n",
    "'''\n",
    "\n",
    "# how many is that?\n",
    "article_count = tdn_article_qs.count()\n",
    "\n",
    "print( \"Article count before filtering on tags: \" + str( article_count ) )\n",
    "\n",
    "# ==> tags\n",
    "\n",
    "# tags to exclude\n",
    "#tags_not_in_list = [ \"prelim_reliability\", \"prelim_network\" ]\n",
    "#tags_not_in_list = [ \"minnesota1-20160328\", \"minnesota2-20160328\", ]\n",
    "\n",
    "# for later - exclude articles already coded.\n",
    "#tags_not_in_list = [ OpenCalaisV2ArticleCoder.TAG_CODED_BY_ME ]\n",
    "\n",
    "tags_not_in_list = None\n",
    "if ( ( tags_not_in_list is not None ) and ( len( tags_not_in_list ) > 0 ) ):\n",
    "\n",
    "    # exclude those in a list\n",
    "    print( \"filtering out articles with tags: \" + str( tags_not_in_list ) )\n",
    "    tdn_article_qs = tdn_article_qs.exclude( tags__name__in = tags_not_in_list )\n",
    "\n",
    "#-- END check to see if we have a specific list of tags we want to exclude --#\n",
    "\n",
    "# include only those with certain tags.\n",
    "#tags_in_list = [ \"prelim_unit_test_001\", \"prelim_unit_test_002\", \"prelim_unit_test_003\", \"prelim_unit_test_004\", \"prelim_unit_test_005\", \"prelim_unit_test_006\", \"prelim_unit_test_007\" ]\n",
    "#tags_in_list = [ \"tdn_month\", ]\n",
    "tags_in_list = None\n",
    "if ( ( tags_in_list is not None ) and ( len( tags_in_list ) > 0 ) ):\n",
    "\n",
    "    # filter\n",
    "    print( \"filtering to just articles with tags: \" + str( tags_in_list ) )\n",
    "    tdn_article_qs = tdn_article_qs.filter( tags__name__in = tags_in_list )\n",
    "    \n",
    "#-- END check to see if we have a specific list of tags we want to include --#\n",
    "\n",
    "# filter out \"*prelim*\" tags?\n",
    "#filter_out_prelim_tags = True\n",
    "if ( filter_out_prelim_tags == True ):\n",
    "\n",
    "    # ifilter out all articles with any tag whose name contains \"prelim\".\n",
    "    print( \"filtering out articles with tags that contain \\\"prelim\\\"\" )\n",
    "    tdn_article_qs = tdn_article_qs.exclude( tags__name__icontains = \"prelim\" )\n",
    "    \n",
    "#-- END check to see if we filter out \"prelim_*\" tags --#\n",
    "\n",
    "# how many is that?\n",
    "article_count = tdn_article_qs.count()\n",
    "\n",
    "print( \"Article count after tag filtering: \" + str( article_count ) )\n",
    "\n",
    "# do we want a random sample?\n",
    "if ( random_count > 0 ):\n",
    "\n",
    "    # to get random, order them by \"?\", then use slicing to retrieve requested\n",
    "    #     number.\n",
    "    tdn_article_qs = tdn_article_qs.order_by( \"?\" )[ : random_count ]\n",
    "    \n",
    "#-- END check to see if we want random sample --#\n",
    "\n",
    "# this is a nice algorithm, also:\n",
    "# - http://www.titov.net/2005/09/21/do-not-use-order-by-rand-or-how-to-get-random-rows-from-table/\n",
    "\n",
    "# make ID list, tag articles if configured to.\n",
    "article_id_list = []\n",
    "article_counter = 0\n",
    "for current_article in tdn_article_qs:\n",
    "\n",
    "    # increment article_counter\n",
    "    article_counter += 1\n",
    "\n",
    "    # add IDs to article_id_list\n",
    "    article_id_list.append( str( current_article.id ) )\n",
    "    \n",
    "    # apply a tag while we are at it?\n",
    "    if ( ( do_apply_tag == True ) and ( tag_to_apply is not None ) and ( tag_to_apply != \"\" ) ):\n",
    "    \n",
    "        # yes, please.  Add tag.\n",
    "        current_article.tags.add( tag_to_apply )\n",
    "        \n",
    "    #-- END check to see if we apply tag. --#\n",
    "\n",
    "    # output the tags.\n",
    "    if ( debug_flag == True ):\n",
    "        print( \"- Tags for article \" + str( current_article.id ) + \" : \" + str( current_article.tags.all() ) )\n",
    "    #-- END DEBUG --#\n",
    "\n",
    "#-- END loop over articles --#\n",
    "\n",
    "# output the list.\n",
    "print( \"tdn_article_qs count: {}\".format( tdn_article_qs.count() ) )\n",
    "print( \"Found \" + str( article_counter ) + \" articles ( \" + str( article_count ) + \" ).\" )\n",
    "if ( debug_flag == True ):\n",
    "    print( \"List of \" + str( len( article_id_list ) ) + \" local TDN staff article IDs: \" + \", \".join( article_id_list ) )\n",
    "#-- END DEBUG --#\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5adae2ed-56f9-4728-b46f-3492056c38c0",
   "metadata": {},
   "source": [
    "# Update data and write unit tests"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae6c70d0-2d10-4125-bbda-a65bb49f51ec",
   "metadata": {},
   "source": [
    "## update Article_Subjects to create single names\n",
    "\n",
    "    ==> Article 21738: 21738 - Dec 10, 2009, Business ( A12 ), UID: 12C893C9821CF460 - $2.1 million is local share in class action - Visa, MasterCard foot the bill in $1.1 billion settlement ( Grand Rapids Press, The )\n",
    "    - tags: <QuerySet [<Tag: coded-OpenCalaisV2ArticleCoder>, <Tag: local_hard_news>]>\n",
    "    ----> article_data: 239 - automated ( ADCT: OpenCalais_REST_API_v2 )  -- Article: 21738 - Dec 10, 2009, Business ( A12 ), UID: 12C893C9821CF460 - $2.1 million is local share in class action - Visa, MasterCard foot the bill in $1.1 billion settlement ( Grand Rapids Press, The )\n",
    "\n",
    "    ==> Article 21719: 21719 - Dec 10, 2009, City and Region ( A6 ), UID: 12C893C993107670 - Both drivers share blame in crash, traffic expert says - Man ordered to stand trial in fatal accident ( Grand Rapids Press, The )\n",
    "    - tags: <QuerySet [<Tag: coded-OpenCalaisV2ArticleCoder>, <Tag: local_hard_news>]>\n",
    "    ----> article_data: 238 - automated ( ADCT: OpenCalais_REST_API_v2 )  -- Article: 21719 - Dec 10, 2009, City and Region ( A6 ), UID: 12C893C993107670 - Both drivers share blame in crash, traffic expert says - Man ordered to stand trial in fatal accident ( Grand Rapids Press, The )\n",
    "    \n",
    "Start with Article_Data 238, 239 - look at all Article_Subjects they contain."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eac32491-58a2-4360-91e4-d07cc155e2de",
   "metadata": {},
   "source": [
    "# Code Articles\n",
    "\n",
    "- Back to [Table of Contents](#Table-of-Contente)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c48b7d6d-5308-46b5-b3cf-48fc02aabb21",
   "metadata": {},
   "source": [
    "Retrieve just publications that are tagged as being local hard news and that also are not tagged as having been coded by OpenCalaisV2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ceb8d13-6a94-43e1-89fe-aa4ac2e03b11",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-17T15:44:26.264960Z",
     "start_time": "2019-09-17T15:44:26.253637Z"
    }
   },
   "outputs": [],
   "source": [
    "article_qs = Article.objects.all()\n",
    "article_count = article_qs.count()\n",
    "print( \"Article count: {}\".format( article_count ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "427c293b-88d4-495d-8f17-79d73b7fee0f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-17T15:46:53.738865Z",
     "start_time": "2019-09-17T15:44:35.197039Z"
    }
   },
   "outputs": [],
   "source": [
    "# declare variables\n",
    "\n",
    "# declare variables - article filter parameters\n",
    "start_pub_date = None # should be datetime instance\n",
    "end_pub_date = None # should be datetime instance\n",
    "tags_in_list = []\n",
    "tags_not_in_list = []\n",
    "paper_id_in_list = []\n",
    "section_list = []\n",
    "article_id_in_list = []\n",
    "params = {}\n",
    "\n",
    "# declare variables - processing\n",
    "do_i_print_updates = True\n",
    "my_article_coding = None\n",
    "article_qs = None\n",
    "article_count = -1\n",
    "coding_status = \"\"\n",
    "limit_to = -1\n",
    "do_coding = True\n",
    "\n",
    "# declare variables - results\n",
    "success_count = -1\n",
    "success_list = None\n",
    "got_errors = False\n",
    "error_count = -1\n",
    "error_dictionary = None\n",
    "error_article_id = -1\n",
    "error_status_list = None\n",
    "error_status = \"\"\n",
    "error_status_counter = -1\n",
    "\n",
    "# first, get a list of articles to code.\n",
    "\n",
    "# ! Set param values.\n",
    "\n",
    "# ==> start and end dates\n",
    "#start_pub_date = \"2009-12-06\"\n",
    "#end_pub_date = \"2009-12-12\"\n",
    "\n",
    "# ==> tagged articles\n",
    "\n",
    "# Examples:\n",
    "#tag_in_list = \"prelim_reliability\"\n",
    "#tag_in_list = \"prelim_network\"\n",
    "#tag_in_list = \"prelim_unit_test_007\"\n",
    "#tag_in_list = [ \"prelim_reliability\", \"prelim_network\" ]\n",
    "#tag_in_list = [ \"prelim_reliability_test\" ] # 60 articles - Grand Rapids only.\n",
    "#tag_in_list = [ \"prelim_reliability_combined\" ] # 87 articles, Grand Rapids and Detroit.\n",
    "#tag_in_list = [ \"prelim_training_001\" ]\n",
    "#tag_in_list = [ \"grp_month\" ]\n",
    "\n",
    "# ----> include articles when these tags are present.\n",
    "tags_in_list = None\n",
    "#tags_in_list = []\n",
    "#tags_in_list.append( ContextTextBase.TAG_LOCAL_HARD_NEWS )\n",
    "\n",
    "# ---> exclude articles when these tags are present.\n",
    "tags_not_in_list = None\n",
    "#tags_not_in_list = []\n",
    "#tags_not_in_list.append( OpenCalaisV2ArticleCoder.TAG_CODED_BY_ME )\n",
    "\n",
    "# ==> IDs of newspapers to include.\n",
    "#paper_id_in_list = \"1\"\n",
    "\n",
    "# ==> names of sections to include.\n",
    "#section_list = \"Lakeshore,Front Page,City and Region,Business\"\n",
    "\n",
    "# ==> just limit to specific articles by ID.\n",
    "article_id_in_list = []\n",
    "#article_id_in_list = [ 360962 ]\n",
    "#article_id_in_list = [ 28598 ]\n",
    "#article_id_in_list = [ 21653, 21756 ]\n",
    "#article_id_in_list = [ 90948 ]\n",
    "#article_id_in_list = [ 21627, 21609, 21579 ]\n",
    "#article_id_in_list = [ 48778 ]\n",
    "#article_id_in_list = [ 6065 ]\n",
    "#article_id_in_list = [ 221858 ]\n",
    "#article_id_in_list = [ 23804, 22630 ]\n",
    "#article_id_in_list = [ 23804 ]\n",
    "\n",
    "# debugging exception\n",
    "#article_id_in_list.append( 402670 )\n",
    "#article_id_in_list.append( 408735 )\n",
    "\n",
    "# filter parameters\n",
    "params[ ArticleCoding.PARAM_START_DATE ] = start_pub_date\n",
    "params[ ArticleCoding.PARAM_END_DATE ] = end_pub_date\n",
    "params[ ArticleCoding.PARAM_TAGS_IN_LIST ] = tags_in_list\n",
    "params[ ArticleCoding.PARAM_TAGS_NOT_IN_LIST ] = tags_not_in_list\n",
    "params[ ArticleCoding.PARAM_PUBLICATION_LIST ] = paper_id_in_list\n",
    "params[ ArticleCoding.PARAM_SECTION_LIST ] = section_list\n",
    "params[ ArticleCoding.PARAM_ARTICLE_ID_LIST ] = article_id_in_list\n",
    "\n",
    "# set coder you want to use.\n",
    "\n",
    "# OpenCalais REST API v.2\n",
    "params[ ArticleCoding.PARAM_CODER_TYPE ] = ArticleCoding.ARTICLE_CODING_IMPL_OPEN_CALAIS_API_V2\n",
    "\n",
    "# get instance of ArticleCoding\n",
    "my_article_coding = ArticleCoding()\n",
    "my_article_coding.do_print_updates = do_i_print_updates\n",
    "\n",
    "# to adjust timing, you need to update the ArticleCoder class for your\n",
    "#     coder.  That overrides the value set here (so we respect limits\n",
    "#     if they are coded into a particular coder):\n",
    "my_article_coding.rate_limit_in_seconds = 3\n",
    "\n",
    "# set params\n",
    "my_article_coding.store_parameters( params )\n",
    "\n",
    "print( \"Query Parameters: {}\".format( params ) )\n",
    "\n",
    "# create query set - ArticleCoding does the filtering for you.\n",
    "article_qs = my_article_coding.create_article_query_set()\n",
    "\n",
    "print( \"After my_article_coding.create_article_query_set(), count: {}\".format( article_qs.count() ) )\n",
    "if ( article_qs._result_cache is None ):\n",
    "    \n",
    "    print( \"article_qs evaluated: NO ( {} )\".format( article_qs._result_cache ) )\n",
    "    \n",
    "else:\n",
    "    \n",
    "    print( \"article_qs evaluated: YES\" )\n",
    "\n",
    "#-- END check to see if _result_cache --#\n",
    "\n",
    "# order by pub_date DESC, so we do most recent first.\n",
    "article_qs = article_qs.order_by( \"-pub_date\" )\n",
    "\n",
    "# limit for an initial test?\n",
    "limit_to = 5000\n",
    "# limit_to = 5\n",
    "if ( ( limit_to is not None ) and ( isinstance( limit_to, int ) == True ) and ( limit_to > 0 ) ):\n",
    "\n",
    "    # yes.\n",
    "    article_qs = article_qs[ : limit_to ]\n",
    "\n",
    "#-- END check to see if limit --#\n",
    "\n",
    "# get article count\n",
    "if ( isinstance( article_qs, list ) == True ):\n",
    "\n",
    "    # list - call len()\n",
    "    article_list = article_qs\n",
    "    article_count = len( article_list )\n",
    "    \n",
    "else:\n",
    "\n",
    "    # not a list - call count()\n",
    "    article_count = article_qs.count()\n",
    "    \n",
    "#-- END figure out how to get count --#\n",
    "\n",
    "print( \"Matching article count: \" + str( article_count ) )\n",
    "\n",
    "# Do coding?\n",
    "if ( do_coding == True ):\n",
    "\n",
    "    print( \"do_coding == True - it's on!\" )\n",
    "\n",
    "    # yes - make sure we have at least one article:\n",
    "    if ( article_count > 0 ):\n",
    "\n",
    "        # invoke the code_article_data( self, query_set_IN ) method.\n",
    "        coding_status = my_article_coding.code_article_data( article_qs )\n",
    "    \n",
    "        # output status\n",
    "        print( \"\\n\\n==============================\\n\\nCoding status: \\\"\" + coding_status + \"\\\"\" )\n",
    "        \n",
    "        # get success count\n",
    "        success_count = my_article_coding.get_success_count()\n",
    "        print( \"\\n\\n====> Count of articles successfully processed: \" + str( success_count ) )    \n",
    "        \n",
    "        # if successes, list out IDs.\n",
    "        if ( success_count > 0 ):\n",
    "        \n",
    "            # there were successes.\n",
    "            success_list = my_article_coding.get_success_list()\n",
    "            print( \"- list of successfully processed articles: \" + str( success_list ) )\n",
    "        \n",
    "        #-- END check to see if successes. --#\n",
    "        \n",
    "        # got errors?\n",
    "        got_errors = my_article_coding.has_errors()\n",
    "        if ( got_errors == True ):\n",
    "        \n",
    "            # get error dictionary\n",
    "            error_dictionary = my_article_coding.get_error_dictionary()\n",
    "            \n",
    "            # get error count\n",
    "            error_count = len( error_dictionary )\n",
    "            print( \"\\n\\n====> Count of articles with errors: \" + str( error_count ) )\n",
    "            \n",
    "            # loop...\n",
    "            for error_article_id, error_status_list in six.iteritems( error_dictionary ):\n",
    "            \n",
    "                # output errors for this article.\n",
    "                print( \"- errors for article ID \" + str( error_article_id ) + \":\" )\n",
    "                \n",
    "                # loop over status messages.\n",
    "                error_status_counter = 0\n",
    "                for error_status in error_status_list:\n",
    "                \n",
    "                    # increment status\n",
    "                    error_status_counter += 1\n",
    "\n",
    "                    # print status\n",
    "                    print( \"----> status #\" + str( error_status_counter ) + \": \" + error_status )\n",
    "                    \n",
    "                #-- END loop over status messages. --#\n",
    "            \n",
    "            #-- END loop over articles. --#\n",
    "   \n",
    "        #-- END check to see if errors --#\n",
    "    \n",
    "    #-- END check to see if article count. --#\n",
    "    \n",
    "else:\n",
    "    \n",
    "    # output matching article count.\n",
    "    print( \"do_coding == False, so dry run\" )\n",
    "    \n",
    "#-- END check to see if we do_coding --#\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "859c01da-57be-4a32-a5fe-1b210ebc7c28",
   "metadata": {},
   "source": [
    "- 2019.07.31 - 5000 - started: execution queued 22:42:01 2019-07-31 --> executed in 3h 47m 55s, finished 02:29:55 2019-08-01\n",
    "- 2019.08.03 - 4990 - started: execution queued 00:38:05 2019-08-03 --> \n",
    "- 2019.08.04 - 5000 - started: execution queued 22:28:45 2019-08-04 --> executed in 4h 45m 21s, finished 03:14:07 2019-08-05\n",
    "- 2019.08.05 - 5000 - started: execution queued 23:04:50 2019-08-05 --> \n",
    "- 2019.08.06 - 5000 - started: execution queued 22:27:34 2019-08-06 --> executed in 5h 21m 21s, finished 03:48:55 2019-08-07\n",
    "- 2019.08.07 - 5000 - started: execution queued 00:11:32 2019-08-08 --> executed in 4h 51m 22s, finished 05:02:54 2019-08-08\n",
    "- 2019.08.08 - 5000 - started: execution queued 00:00:00 2019-08-09 --> executed in 4h 54m 50s, finished 03:04:21 2019-08-10\n",
    "- 2091.08.10 - 3819 - started: execution queued 22:09:20 2019-08-10 --> finished 02:52:51.48118 2019-08-01"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f6620fb-9c00-4b15-ac84-e99cdfed2709",
   "metadata": {},
   "source": [
    "## Optional Validation\n",
    "\n",
    "- Back to [Table of Contents](#Table-of-Contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfd09ad1-ddf9-41d4-bf50-741c6030317d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-11T14:17:18.594154Z",
     "start_time": "2019-08-11T14:17:18.584023Z"
    }
   },
   "outputs": [],
   "source": [
    "# get automated coder\n",
    "automated_coder_user = ArticleCoder.get_automated_coding_user()\n",
    "\n",
    "print( \"{} - Loaded automated user: {}, id = {}\".format( datetime.datetime.now(), automated_coder_user, automated_coder_user.id ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edce01ba-9f72-4f2a-83c7-96b1b6aedaf5",
   "metadata": {},
   "source": [
    "### Validate success publications\n",
    "\n",
    "- Back to [Table of Contents](#Table-of-Contents)\n",
    "\n",
    "Loop over all successful records and verify:\n",
    "\n",
    "- that they have the OpenCalais coded-by-me tag (`OpenCalaisV2ArticleCoder.TAG_CODED_BY_ME`).\n",
    "- that they have an ArticleData for automated coding user.\n",
    "- that it isn't all just 0 sources.  Perhaps, collect and average source and subject counts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b9ab29c-8138-401e-9fb1-7305a3dfd30a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-13T02:22:55.867663Z",
     "start_time": "2019-09-13T02:22:55.169220Z"
    }
   },
   "outputs": [],
   "source": [
    "# declare variables\n",
    "success_count = -1\n",
    "success_list = None\n",
    "article_id = None\n",
    "has_coded_tag = None\n",
    "has_coded_tag_counter = None\n",
    "has_article_data_counter = None\n",
    "article_instance = None\n",
    "\n",
    "# declare variables - tag validation\n",
    "tag_name_list = None\n",
    "coded_by_tag_name = None\n",
    "has_coded_by_tag = None\n",
    "\n",
    "# declare variables - ArticleData validation\n",
    "article_id_to_data_map = None\n",
    "article_data_qs = None\n",
    "article_data_count = None\n",
    "article_data_instance = None\n",
    "article_data_id = None\n",
    "automated_coder_type = None\n",
    "article_data_map = None\n",
    "article_author_qs = None\n",
    "author_count = None\n",
    "article_subject_qs = None\n",
    "subject_qs = None\n",
    "subject_count = None\n",
    "source_qs = None\n",
    "source_count = None\n",
    "has_data_count = None\n",
    "has_people_count = None\n",
    "has_subjects_count = None\n",
    "has_sources_count = None\n",
    "article_counter = None\n",
    "start_time = None\n",
    "previous_time = None\n",
    "current_time = None\n",
    "time_since_start = None\n",
    "time_since_previous = None\n",
    "\n",
    "# validation\n",
    "\n",
    "# init\n",
    "coded_by_tag_name = OpenCalaisV2ArticleCoder.TAG_CODED_BY_ME\n",
    "#automated_coder_user = ArticleCoder.get_automated_coding_user()\n",
    "automated_coder_type = OpenCalaisV2ArticleCoder.CONFIG_APPLICATION\n",
    "article_id_to_data_map = {}\n",
    "\n",
    "# get success count\n",
    "success_count = my_article_coding.get_success_count()\n",
    "log_message = \"\\n\\n====> Count of articles successfully processed: {}\".format( success_count )\n",
    "my_logging_helper.output_message( log_message, do_print_IN = True, log_level_code_IN = logging.INFO )\n",
    "\n",
    "# if successes, list out IDs.\n",
    "if ( success_count > 0 ):\n",
    "\n",
    "    # there were successes.\n",
    "    success_list = my_article_coding.get_success_list()\n",
    "    #print( \"- list of successfully processed articles: \" + str( success_list ) )\n",
    "    \n",
    "    # loop over success articles\n",
    "    article_counter = 0\n",
    "    has_coded_tag_counter = 0\n",
    "    has_article_data_counter = 0\n",
    "    has_data_count = 0\n",
    "    has_people_count = 0\n",
    "    has_subjects_count = 0\n",
    "    has_sources_count = 0\n",
    "    start_time = datetime.datetime.now()\n",
    "    current_time = start_time\n",
    "    for article_id in success_list:\n",
    "        \n",
    "        article_counter += 1\n",
    "        \n",
    "        # load article\n",
    "        article_instance = Article.objects.get( pk = article_id )\n",
    "        \n",
    "        # get tag name list\n",
    "        tag_name_list = article_instance.tags.names()\n",
    "        \n",
    "        # is coded-by tag name present?\n",
    "        if ( coded_by_tag_name in tag_name_list ):\n",
    "            \n",
    "            # it is there, as it should be.\n",
    "            has_coded_by_tag =  True\n",
    "            has_coded_tag_counter += 1\n",
    "            \n",
    "        else:\n",
    "            \n",
    "            # not there.  Error.\n",
    "            has_coded_by_tag = False\n",
    "            log_message = \"ERROR in article {}: coded-by tag ( {} ) not in tag list: {}\".format( article_id, coded_by_tag_name, tag_name_list )\n",
    "            my_logging_helper.output_message( log_message, do_print_IN = True, log_level_code_IN = logging.INFO )\n",
    "        \n",
    "        #-- END check for coded-by tag name in tag list. --#\n",
    "        \n",
    "        # is there an ArticleData instance by automated coder for OpenCalais V.2?\n",
    "        article_data_qs = article_instance.article_data_set.filter( coder = automated_coder_user )\n",
    "        article_data_qs = article_data_qs.filter( coder_type = automated_coder_type )\n",
    "        article_data_count = article_data_qs.count()\n",
    "        if ( article_data_count == 1 ):\n",
    "            \n",
    "            # got one.  Increment counter.\n",
    "            has_article_data_counter += 1\n",
    "            \n",
    "            # TODO - check how many sources, subjects.\n",
    "            article_data_instance = article_data_qs.get()\n",
    "            article_data_id = article_data_instance.id\n",
    "            \n",
    "            # create article data map\n",
    "            article_data_map = {}\n",
    "            article_data_map[ \"article_id\" ] = article_id\n",
    "            article_data_map[ \"article_instance\" ] = article_instance\n",
    "            article_data_map[ \"article_data_instance\" ] = article_data_instance\n",
    "            article_data_map[ \"article_data_id\" ] = article_data_id\n",
    "            \n",
    "            # get count of authors\n",
    "            article_author_qs = article_data_instance.article_author_set.all()\n",
    "            author_count = article_author_qs.count()\n",
    "            article_data_map[ \"author_count\" ] = author_count\n",
    "            \n",
    "            # get count of subjects\n",
    "            article_subject_qs = article_data_instance.article_subject_set.all()\n",
    "            article_subject_total_count = article_subject_qs.count()\n",
    "            article_data_map[ \"article_subject_total_count\" ] = article_subject_total_count\n",
    "            if ( article_subject_total_count > 0 ):\n",
    "                \n",
    "                has_people_count += 1\n",
    "                \n",
    "            #-- END check to see if any people found at all --#\n",
    "            \n",
    "            # just subjects\n",
    "            subject_qs = article_subject_qs.filter( subject_type = Article_Subject.SUBJECT_TYPE_MENTIONED )\n",
    "            subject_count = subject_qs.count()\n",
    "            article_data_map[ \"subject_count\" ] = subject_count\n",
    "            if ( subject_count > 0 ):\n",
    "                \n",
    "                has_subjects_count += 1\n",
    "                \n",
    "            #-- END check to see if any subjects found --#\n",
    "            \n",
    "            # get count of sources\n",
    "            source_qs = article_subject_qs.filter( subject_type = Article_Subject.SUBJECT_TYPE_QUOTED )\n",
    "            source_count = source_qs.count()\n",
    "            article_data_map[ \"source_count\" ] = source_count\n",
    "            if ( source_count > 0 ):\n",
    "                \n",
    "                has_sources_count += 1\n",
    "                \n",
    "            #-- END check to see if any sources found --#\n",
    "            \n",
    "            # store information for article.\n",
    "            article_id_to_data_map[ article_id ] = article_data_map\n",
    "            \n",
    "            if ( ( author_count == 0 ) and ( article_subject_total_count == 0 ) ):\n",
    "                \n",
    "                # get current time and time elapsed since start\n",
    "                log_message = \"No authors or sources in article {}\".format( article_id )\n",
    "                my_logging_helper.output_message( log_message, do_print_IN = True, log_level_code_IN = logging.INFO )\n",
    "                \n",
    "            else:\n",
    "                \n",
    "                # increment populated data count\n",
    "                has_data_count += 1\n",
    "                \n",
    "            #-- END sanity check for empty data (won't be zero, shouldn't be many) --#\n",
    "            \n",
    "        elif ( article_data_count > 1 ):\n",
    "            \n",
    "            # more than one?\n",
    "            log_message = \"ERROR in article {}: more than one ArticleData instance ( {} ) for automated coder ( {} ), coder type: {}.\".format( article_id, article_data_count, automated_coder_user, automated_coder_type )\n",
    "            my_logging_helper.output_message( log_message, do_print_IN = True, log_level_code_IN = logging.INFO )\n",
    "            \n",
    "        else:\n",
    "            \n",
    "            # error - no ArticleData.\n",
    "            log_message = \"ERROR in article {}: no ArticleData instances for automated coder ( {} ), coder type: {}.\".format( article_id, automated_coder_user, automated_coder_type )\n",
    "            my_logging_helper.output_message( log_message, do_print_IN = True, log_level_code_IN = logging.INFO )\n",
    "            \n",
    "        #-- END check to see if ArticleData by automated coder, Open Calais v.2 --#\n",
    "        \n",
    "        # progress output\n",
    "        if ( ( article_counter % 100 ) == 0 ):\n",
    "            \n",
    "            log_message = \"----> article counter: {}\".format( article_counter )\n",
    "            my_logging_helper.output_message( log_message, do_print_IN = True, log_level_code_IN = logging.INFO )\n",
    "            \n",
    "            # get current time and time elapsed since start\n",
    "            previous_time = current_time\n",
    "            current_time = datetime.datetime.now()\n",
    "            time_since_start = current_time - start_time\n",
    "            time_since_previous = current_time - previous_time\n",
    "            log_message = \"         @ {} - time since previous: {}; time since start: {}\".format( current_time, time_since_previous, time_since_start )\n",
    "            my_logging_helper.output_message( log_message, do_print_IN = True, log_level_code_IN = logging.INFO )\n",
    "\n",
    "        #-- END progress output. --#\n",
    "        \n",
    "    #-- END loop over IDs of sucessfully processed articles. --#\n",
    "\n",
    "#-- END check to see if successes. --#\n",
    "        \n",
    "log_message = \"- Tagged article count: {}\".format( has_coded_tag_counter )\n",
    "my_logging_helper.output_message( log_message, do_print_IN = True, log_level_code_IN = logging.INFO )\n",
    "log_message = \"- Correct ArticleData count: {}\".format( has_article_data_counter )\n",
    "my_logging_helper.output_message( log_message, do_print_IN = True, log_level_code_IN = logging.INFO )\n",
    "log_message = \"- Has data count: {}\".format( has_data_count )\n",
    "my_logging_helper.output_message( log_message, do_print_IN = True, log_level_code_IN = logging.INFO )\n",
    "log_message = \"- Has people count: {}\".format( has_people_count )\n",
    "my_logging_helper.output_message( log_message, do_print_IN = True, log_level_code_IN = logging.INFO )\n",
    "log_message = \"- Has subjects count: {}\".format( has_subjects_count )\n",
    "my_logging_helper.output_message( log_message, do_print_IN = True, log_level_code_IN = logging.INFO )\n",
    "log_message = \"- Has sources count: {}\".format( has_sources_count )\n",
    "my_logging_helper.output_message( log_message, do_print_IN = True, log_level_code_IN = logging.INFO )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4c6d9a3-4e4a-41ab-a617-dfbc909ba829",
   "metadata": {},
   "source": [
    "### Validate error publications\n",
    "\n",
    "- Back to [Table of Contents](#Table-of-Contents)\n",
    "\n",
    "Loop over all error records and verify:\n",
    "\n",
    "- that they do not have the OpenCalais coded-by-me tag (`OpenCalaisV2ArticleCoder.TAG_CODED_BY_ME`).\n",
    "- check on the status of their ArticleData.  Do they have any?  If so, what to do?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "163dc032-09a9-4fd4-9630-8b75d4b8adb6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-11T14:17:29.032548Z",
     "start_time": "2019-08-11T14:17:28.968994Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# declare variables\n",
    "got_errors = None\n",
    "error_dictionary = None\n",
    "error_count = None\n",
    "error_article_id = None\n",
    "error_status_list = None\n",
    "error_status_counter = None\n",
    "article_instance = None\n",
    "tag_name_list = None\n",
    "coded_by_tag_name = None\n",
    "has_coded_by_tag = None\n",
    "\n",
    "# declare variables - ArticleData validation\n",
    "error_article_id_to_data_map = None\n",
    "article_data_qs = None\n",
    "article_data_count = None\n",
    "article_data_instance = None\n",
    "article_data_id = None\n",
    "automated_coder_type = None\n",
    "article_data_map = None\n",
    "article_author_qs = None\n",
    "author_count = None\n",
    "article_subject_qs = None\n",
    "subject_qs = None\n",
    "subject_count = None\n",
    "source_qs = None\n",
    "source_count = None\n",
    "has_data_count = None\n",
    "has_people_count = None\n",
    "has_subjects_count = None\n",
    "has_sources_count = None\n",
    "\n",
    "# init\n",
    "coded_by_tag_name = OpenCalaisV2ArticleCoder.TAG_CODED_BY_ME\n",
    "#automated_coder_user = ArticleCoder.get_automated_coding_user()\n",
    "automated_coder_type = OpenCalaisV2ArticleCoder.CONFIG_APPLICATION\n",
    "error_article_id_to_data_map = {}\n",
    "\n",
    "# got errors?\n",
    "got_errors = my_article_coding.has_errors()\n",
    "if ( got_errors == True ):\n",
    "\n",
    "    # get error dictionary\n",
    "    error_dictionary = my_article_coding.get_error_dictionary()\n",
    "\n",
    "    # get error count\n",
    "    error_count = len( error_dictionary )\n",
    "    log_message = \"\\n\\n====> Count of articles with errors: {}\".format( error_count )\n",
    "    my_logging_helper.output_message( log_message, do_print_IN = True, log_level_code_IN = logging.INFO )\n",
    "\n",
    "    # loop...\n",
    "    has_coded_tag_counter = 0\n",
    "    has_article_data_counter = 0\n",
    "    has_data_count = 0\n",
    "    has_people_count = 0\n",
    "    has_subjects_count = 0\n",
    "    has_sources_count = 0\n",
    "    for error_article_id, error_status_list in six.iteritems( error_dictionary ):\n",
    "\n",
    "        log_message = \"\\nError article ID: {}\".format( error_article_id )\n",
    "        my_logging_helper.output_message( log_message, do_print_IN = True, log_level_code_IN = logging.INFO )\n",
    "\n",
    "        # output errors for this article.\n",
    "        log_message = \"- errors for article ID {}:\".format( error_article_id )\n",
    "        my_logging_helper.output_message( log_message, do_print_IN = True, log_level_code_IN = logging.INFO )\n",
    "\n",
    "        # loop over status messages.\n",
    "        error_status_counter = 0\n",
    "        for error_status in error_status_list:\n",
    "\n",
    "            # increment status\n",
    "            error_status_counter += 1\n",
    "\n",
    "            # print status\n",
    "            log_message = \"----> status #{}: {}\".format( error_status_counter, error_status )\n",
    "            my_logging_helper.output_message( log_message, do_print_IN = True, log_level_code_IN = logging.INFO )\n",
    "            \n",
    "        #-- END loop over status messages. --#\n",
    "\n",
    "        # load article\n",
    "        article_instance = Article.objects.get( pk = error_article_id )\n",
    "        \n",
    "        # get tag name list\n",
    "        tag_name_list = article_instance.tags.names()\n",
    "        \n",
    "        # is coded-by tag name present?\n",
    "        if ( coded_by_tag_name in tag_name_list ):\n",
    "            \n",
    "            # it is there, as it should be.\n",
    "            has_coded_by_tag =  True\n",
    "            has_coded_tag_counter += 1\n",
    "            \n",
    "        else:\n",
    "            \n",
    "            # not there.  Error.\n",
    "            has_coded_by_tag = False\n",
    "            #print( \"ERROR in article {}: coded-by tag ( {} ) not in tag list: {}\".format( error_article_id, coded_by_tag_name, tag_name_list ) )\n",
    "        \n",
    "        #-- END check for coded-by tag name in tag list. --#\n",
    "        \n",
    "        # is there an ArticleData instance by automated coder for OpenCalais V.2?\n",
    "        article_data_qs = article_instance.article_data_set.filter( coder = automated_coder_user )\n",
    "        article_data_qs = article_data_qs.filter( coder_type = automated_coder_type )\n",
    "        article_data_count = article_data_qs.count()\n",
    "        if ( article_data_count == 1 ):\n",
    "            \n",
    "            # got one.  Increment counter.\n",
    "            has_article_data_counter += 1\n",
    "            \n",
    "            # TODO - check how many sources, subjects.\n",
    "            article_data_instance = article_data_qs.get()\n",
    "            article_data_id = article_data_instance.id\n",
    "            \n",
    "            # create article data map\n",
    "            article_data_map = {}\n",
    "            article_data_map[ \"article_id\" ] = error_article_id\n",
    "            article_data_map[ \"article_instance\" ] = article_instance\n",
    "            article_data_map[ \"article_data_instance\" ] = article_data_instance\n",
    "            article_data_map[ \"article_data_id\" ] = article_data_id\n",
    "            \n",
    "            # get count of authors\n",
    "            article_author_qs = article_data_instance.article_author_set.all()\n",
    "            author_count = article_author_qs.count()\n",
    "            article_data_map[ \"author_count\" ] = author_count\n",
    "            \n",
    "            # get count of subjects\n",
    "            article_subject_qs = article_data_instance.article_subject_set.all()\n",
    "            article_subject_total_count = article_subject_qs.count()\n",
    "            article_data_map[ \"article_subject_total_count\" ] = article_subject_total_count\n",
    "            if ( article_subject_total_count > 0 ):\n",
    "                \n",
    "                has_people_count += 1\n",
    "                \n",
    "            #-- END check to see if any people found at all --#\n",
    "            \n",
    "            # just subjects\n",
    "            subject_qs = article_subject_qs.filter( subject_type = Article_Subject.SUBJECT_TYPE_MENTIONED )\n",
    "            subject_count = subject_qs.count()\n",
    "            article_data_map[ \"subject_count\" ] = subject_count\n",
    "            if ( subject_count > 0 ):\n",
    "                \n",
    "                has_subjects_count += 1\n",
    "                \n",
    "            #-- END check to see if any subjects found --#\n",
    "            \n",
    "            # get count of sources\n",
    "            source_qs = article_subject_qs.filter( subject_type = Article_Subject.SUBJECT_TYPE_QUOTED )\n",
    "            source_count = source_qs.count()\n",
    "            article_data_map[ \"source_count\" ] = source_count\n",
    "            if ( source_count > 0 ):\n",
    "                \n",
    "                has_sources_count += 1\n",
    "                \n",
    "            #-- END check to see if any sources found --#\n",
    "            \n",
    "            # store information for article.\n",
    "            error_article_id_to_data_map[ error_article_id ] = article_data_map\n",
    "            \n",
    "            if ( ( author_count == 0 ) and ( article_subject_total_count == 0 ) ):\n",
    "                \n",
    "                pass\n",
    "                #print( \"- No authors or sources in article {}\".format( error_article_id ) )\n",
    "                \n",
    "            else:\n",
    "                \n",
    "                # increment populated data count\n",
    "                has_data_count += 1\n",
    "                log_message = \"- Found data in article {}: person = {}; subject = {}; source = {}\".format( error_article_id, article_subject_total_count, subject_count, source_count )\n",
    "                my_logging_helper.output_message( log_message, do_print_IN = True, log_level_code_IN = logging.INFO )\n",
    "                \n",
    "            #-- END sanity check for empty data (won't be zero, shouldn't be many) --#\n",
    "            \n",
    "        elif ( article_data_count > 1 ):\n",
    "            \n",
    "            # more than one?\n",
    "            log_message = \"ERROR in article {}: more than one ArticleData instance ( {} ) for automated coder ( {} ), coder type: {}.\".format( error_article_id, article_data_count, automated_coder_user, automated_coder_type )\n",
    "            my_logging_helper.output_message( log_message, do_print_IN = True, log_level_code_IN = logging.INFO )\n",
    "            \n",
    "        else:\n",
    "            \n",
    "            # no ArticleData.\n",
    "            pass\n",
    "            \n",
    "        #-- END check to see if ArticleData by automated coder, Open Calais v.2 --#\n",
    "\n",
    "    #-- END loop over articles. --#\n",
    "\n",
    "    log_message = \"- Tagged article count: {}\".format( has_coded_tag_counter )\n",
    "    my_logging_helper.output_message( log_message, do_print_IN = True, log_level_code_IN = logging.INFO )\n",
    "    log_message = \"- Correct ArticleData count: {}\".format( has_article_data_counter )\n",
    "    my_logging_helper.output_message( log_message, do_print_IN = True, log_level_code_IN = logging.INFO )\n",
    "    log_message = \"- Has data count: {}\".format( has_data_count )\n",
    "    my_logging_helper.output_message( log_message, do_print_IN = True, log_level_code_IN = logging.INFO )\n",
    "    log_message = \"- Has people count: {}\".format( has_people_count )\n",
    "    my_logging_helper.output_message( log_message, do_print_IN = True, log_level_code_IN = logging.INFO )\n",
    "    log_message = \"- Has subjects count: {}\".format( has_subjects_count )\n",
    "    my_logging_helper.output_message( log_message, do_print_IN = True, log_level_code_IN = logging.INFO )\n",
    "    log_message = \"- Has sources count: {}\".format( has_sources_count )\n",
    "    my_logging_helper.output_message( log_message, do_print_IN = True, log_level_code_IN = logging.INFO )\n",
    "    \n",
    "else:\n",
    "\n",
    "    log_message = \"NO ERRORS!  YAY!\"\n",
    "    my_logging_helper.output_message( log_message, do_print_IN = True, log_level_code_IN = logging.INFO )\n",
    "    \n",
    "#-- END check to see if errors --#\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "476eb9d0-fef2-4a16-809e-4c103144a97a",
   "metadata": {},
   "source": [
    "NOTE: Looks like publications where there is an OpenCalais network error are not getting the Coded tag applied, so they will remain in the pool to be re-coded in subsequent runs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9249e825-7b5b-45b7-9001-69bf83744c3c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-11T14:17:34.563937Z",
     "start_time": "2019-08-11T14:17:34.547460Z"
    }
   },
   "outputs": [],
   "source": [
    "# get list of error IDs from map.\n",
    "if ( error_dictionary is not None ):\n",
    "\n",
    "    error_article_id_list = list( six.viewkeys( error_dictionary ) )\n",
    "    log_message = \"IDs of articles with errors: {}\".format( error_article_id_list )\n",
    "    my_logging_helper.output_message( log_message, do_print_IN = True, log_level_code_IN = logging.INFO )\n",
    "    \n",
    "else:\n",
    "\n",
    "    log_message = \"STILL NO ERRORS!  YAY!\"\n",
    "    my_logging_helper.output_message( log_message, do_print_IN = True, log_level_code_IN = logging.INFO )\n",
    "    \n",
    "#-- END check to see if None --#"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44cf3ca1-70f5-44fa-92f9-de51066b3b5c",
   "metadata": {},
   "source": [
    "# Export coded data to new fixtures\n",
    "\n",
    "- Back to [Table of Contents](#Table-of-Contents)\n",
    "\n",
    "Once you have coded articles, you'll want to re-export the coded data to fixtures.\n",
    "\n",
    "Export them to JSON fixture files using manage.py / django-admin dumpdata ( [https://docs.djangoproject.com/en/dev/ref/django-admin/#django-admin-dumpdata](https://docs.djangoproject.com/en/dev/ref/django-admin/#django-admin-dumpdata) ) so they can be imported using python manage.py or django-admin loaddata ( [https://docs.djangoproject.com/en/dev/ref/django-admin/#django-admin-loaddata](https://docs.djangoproject.com/en/dev/ref/django-admin/#django-admin-loaddata) ) rather than having to input them in the admin:\n",
    "\n",
    "    python manage.py dumpdata [app_label[.ModelName] [app_label[.ModelName] ...]] --indent INDENT --output <output_file_path>\n",
    "    \n",
    "    Example: \n",
    "    \n",
    "    python manage.py dumpdata \\\n",
    "        --indent 4 \\\n",
    "        --output context-sourcenet_entities_and_relations.json \\\n",
    "        context.Entity_Identifier_Type \\\n",
    "        context.Entity_Relation_Type \\\n",
    "        context.Entity_Relation_Type_Trait \\\n",
    "        context.Entity_Type \\\n",
    "        context.Entity_Type_Trait \\\n",
    "        context.Trait_Type \\\n",
    "        context.Term \\\n",
    "        context.Term_Relation \\\n",
    "        context.Term_Relation_Type \\\n",
    "        context.Vocabulary \\\n",
    "        \n",
    "    No line breaks:\n",
    "    \n",
    "        python manage.py dumpdata --indent 4 --output context-sourcenet_entities_and_relations.json context.Entity_Identifier_Type context.Entity_Relation_Type context.Entity_Relation_Type_Trait context.Entity_Type context.Entity_Type_Trait context.Trait_Type context.Term context.Term_Relation context.Term_Relation_Type context.Vocabulary\n",
    "\n",
    "The changes we've made here are in three applications: `auth`, `context_text`, and `taggit`.  To make a new fixture for each:\n",
    "\n",
    "- `python manage.py dumpdata --indent 4 --output context_text_unittest_export_auth_data.json auth.user`\n",
    "- `python manage.py dumpdata --indent 4 --output context_text_unittest_export_data.json --exclude context_text.article_data_notes context_text`\n",
    "- `python manage.py dumpdata --indent 4 --output context_text_unittest_export_taggit_data.json taggit`\n",
    "\n",
    "Note, for the `auth` fixture, we're just exporting the `user` table, so we don't include permission information.\n",
    "\n",
    "These are stored in the `context_text` github repo, in `context_text/fixtures`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f34e79c-1f6f-49f5-8eb0-e3731dee0f40",
   "metadata": {},
   "source": [
    "# TODO\n",
    "\n",
    "- Back to [Table of Contents](#Table-of-Contents)\n",
    "\n",
    "TODO:\n",
    "\n",
    "- make sure that I am including author-to-author based on shared byline (different tie type).\n",
    "- figure out the naive date-time error in coding.\n",
    "- test change to rate limiting values being in static variables in OpenCalaisv.2 coder.\n",
    "- start loading data from XML\n",
    "- move data from Article_Data into context.\n",
    "- make the network data creator work against context, then generalize it for tie and node types.\n",
    "- think how we specify which class to use for author strings - needs to be speced to an interface, but not just a newsbank one - so, abstraction here should be higher up - in shared?\n",
    "\n",
    "DONE:\n",
    "\n",
    "- // Save log of coding first 4990 of next round of data.\n",
    "- // for next round of coding, sort on publication date, descending, so we fill in the year before and after the layoffs first.\n",
    "- // adjust django logging to output DEBUG, then test Article.filter_articles() to see where QuerySet is evaluated (DISTINCT check?).\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "research_virtualenv",
   "language": "python",
   "name": "research"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
