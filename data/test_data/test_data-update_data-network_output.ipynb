{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4c07e508-d83b-4d08-92b6-41f43cf4c9e0",
   "metadata": {},
   "source": [
    "**_test_data-update_data-network_output.ipynb_ - Update unit test data to include network_output test info**\n",
    "\n",
    "- derived from: [newsbank-article_coding-unittest.ipynb](../article_coding/newsbank-article_coding-unittest.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8304d859-973d-4f99-b40a-89d0c53de4aa",
   "metadata": {},
   "source": [
    "# work"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6a15e82-789c-43a4-a4c3-9edae9efba62",
   "metadata": {
    "tags": []
   },
   "source": [
    "- update test data:\n",
    "\n",
    "    - // load existing fixtures.\n",
    "    - // includes 43 coded with OpenCalais.\n",
    "    - // make sure we can generate network data from them.\n",
    "    - single-name data\n",
    "    \n",
    "        - there are two single-name sources.\n",
    "\n",
    "    - things to include in actual data:\n",
    "        - build out network data for a few different specs (one with single name, one without).\n",
    "\n",
    "            - specs are in [analysis-network_data_output_example.ipynb](./analysis/analysis-network_data_output_example.ipynb)\n",
    "\n",
    "        - for each data spec, in NetworkDataOutputLog, capture output from no-single-names for original data and data where set records have single-names introduced, both with details on and details off.\n",
    "        - also get the hashes and length of output strings you'd expect and store the values in the test case.\n",
    "        - tag a few Article_Subjects with a random tag (\"name_error\"...?), for testing removing Article_Subjects that have been assigned tag.\n",
    "\n",
    "            - Tag `from_press_release` added to the following `Article_Subject` instances:\n",
    "\n",
    "                - 740 - granholm (person 102)\n",
    "                - 637 - Mark Meadows (person 224)\n",
    "                - 677 - Gary Nelund (person 261)\n",
    "\n",
    "            - Tag `godwin_heights` added to the following `Article_Subject` instances:\n",
    "\n",
    "                - 623 - Felske, Jon (person 188)\n",
    "                - 622 - Johnston, Allen E. (person 187)\n",
    "                - 621 - Hornecker, Kenneth (person 189)\n",
    "\n",
    "    - remove superuser user from auth.\n",
    "    - re-export the \"export\" fixture that includes the network data output log and the tags (should just need those two).\n",
    "\n",
    "- make sure existing unit tests work with new data.\n",
    "- new unit tests:\n",
    "\n",
    "    - unit test code is in [analysis-network_data_output_example.ipynb](./analysis/analysis-network_data_output_example.ipynb)\n",
    "    - simple network data creation test - run with a few specs against test data, make sure I get the right size of output back for each.\n",
    "    - even lower level, make tests for the method to build person dictionaries, and the base lookup method.\n",
    "    - also apply a tag to a few article_subject and test for omitting tags? Same as omitting single names (or stacked?).\n",
    "    - ? - make sure the Article_Data method `filter_article_persons()` works as I intend. To start, create tests in notebook against actual database, using full Article_Subject and Article_Author QuerySets, compare numbers to raw queries. Then, do the same against test database, use numbers to create unit tests. This should be covered by simple network creation tests (they call this method)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff8fc975-af92-414e-879a-966ebc7c6131",
   "metadata": {},
   "source": [
    "# Setup\n",
    "\n",
    "- Back to [Table of Contents](#Table-of-Contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7472a19-8a94-4874-97c5-e439b4d95e93",
   "metadata": {},
   "source": [
    "## Setup - Debug\n",
    "\n",
    "- Back to [Table of Contents](#Table-of-Contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b227033c-54b0-4a7c-8fcb-321f79b49e3c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-17T15:43:45.678275Z",
     "start_time": "2019-09-17T15:43:45.673730Z"
    }
   },
   "outputs": [],
   "source": [
    "debug_flag = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e935a804-bd7c-4f9b-bee9-fe0c19a8f4c3",
   "metadata": {},
   "source": [
    "## Setup - Imports\n",
    "\n",
    "- Back to [Table of Contents](#Table-of-Contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbea8370-9e1e-4814-a3bf-bffbdfc28061",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-17T15:43:46.692379Z",
     "start_time": "2019-09-17T15:43:46.578604Z"
    }
   },
   "outputs": [],
   "source": [
    "import datetime\n",
    "from django.db.models import Avg, Max, Min\n",
    "import hashlib\n",
    "import json\n",
    "import logging\n",
    "import six\n",
    "\n",
    "print( \"packages imported at \" + str( datetime.datetime.now() ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfda3c6c-66f1-4b63-935c-0be496897b7d",
   "metadata": {},
   "source": [
    "## Setup - working folder paths\n",
    "\n",
    "- Back to [Table of Contents](#Table-of-Contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8f7f040-8330-4516-a99e-033ddc4e35fe",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-17T15:43:47.305614Z",
     "start_time": "2019-09-17T15:43:47.286949Z"
    }
   },
   "outputs": [],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7da0eb89-3e23-46e0-ba4b-e50e52e56932",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-17T15:43:48.159415Z",
     "start_time": "2019-09-17T15:43:48.153181Z"
    }
   },
   "outputs": [],
   "source": [
    "# current working folder\n",
    "current_working_folder = \"/home/jonathanmorgan/work/django/research/research/work/phd_work/analysis\"\n",
    "current_datetime = datetime.datetime.now()\n",
    "current_date_string = current_datetime.strftime( \"%Y-%m-%d-%H-%M-%S\" )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47cc9023-52b4-4501-a158-154a390bd0e4",
   "metadata": {},
   "source": [
    "## Setup - logging\n",
    "\n",
    "- Back to [Table of Contents](#Table-of-Contents)\n",
    "\n",
    "configure logging for this notebook's kernel (If you do not run this cell, you'll get the django application's logging configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e69e52fb-2f35-4dda-8726-ae7ff20a05d6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-17T15:43:49.038050Z",
     "start_time": "2019-09-17T15:43:49.031535Z"
    }
   },
   "outputs": [],
   "source": [
    "# build file name\n",
    "logging_file_name = \"{}/article_coding-{}.log.txt\".format( current_working_folder, current_date_string )\n",
    "\n",
    "# set up logging.\n",
    "logging.basicConfig(\n",
    "    level = logging.DEBUG,\n",
    "    format = '%(asctime)s - %(levelname)s - %(name)s - %(message)s',\n",
    "    filename = logging_file_name,\n",
    "    filemode = 'w' # set to 'a' if you want to append, rather than overwrite each time.\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd3718b3-341f-4ed2-8e03-327c5d577884",
   "metadata": {},
   "source": [
    "## Setup - virtualenv jupyter kernel\n",
    "\n",
    "- Back to [Table of Contents](#Table-of-Contents)\n",
    "\n",
    "If you are using a virtualenv, make sure that you:\n",
    "\n",
    "- have installed your virtualenv as a kernel.\n",
    "- choose the kernel for your virtualenv as the kernel for your notebook (Kernel --> Change kernel).\n",
    "\n",
    "Since I use a virtualenv, need to get that activated somehow inside this notebook.  One option is to run `../dev/wsgi.py` in this notebook, to configure the python environment manually as if you had activated the `sourcenet` virtualenv.  To do this, you'd make a code cell that contains:\n",
    "\n",
    "    %run ../dev/wsgi.py\n",
    "    \n",
    "This is sketchy, however, because of the changes it makes to your Python environment within the context of whatever your current kernel is.  I'd worry about collisions with the actual Python 3 kernel.  Better, one can install their virtualenv as a separate kernel.  Steps:\n",
    "\n",
    "- activate your virtualenv:\n",
    "\n",
    "        workon research\n",
    "\n",
    "- in your virtualenv, install the package `ipykernel`.\n",
    "\n",
    "        pip install ipykernel\n",
    "\n",
    "- use the ipykernel python program to install the current environment as a kernel:\n",
    "\n",
    "        python -m ipykernel install --user --name <env_name> --display-name \"<display_name>\"\n",
    "        \n",
    "    `sourcenet` example:\n",
    "    \n",
    "        python -m ipykernel install --user --name sourcenet --display-name \"research (Python 3)\"\n",
    "        \n",
    "More details: [http://ipython.readthedocs.io/en/stable/install/kernel_install.html](http://ipython.readthedocs.io/en/stable/install/kernel_install.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78500ee3-1953-4c10-9e7b-5b26ea94952e",
   "metadata": {},
   "source": [
    "## Setup - Initialize Django\n",
    "\n",
    "- Back to [Table of Contents](#Table-of-Contents)\n",
    "\n",
    "First, initialize my dev django project, so I can run code in this notebook that references my django models and can talk to the database using my project's settings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0955ef1d-8300-4c01-9e1d-ef232e0646d8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-17T15:43:52.077172Z",
     "start_time": "2019-09-17T15:43:52.071107Z"
    }
   },
   "outputs": [],
   "source": [
    "# init django\n",
    "django_init_folder = \"/home/jonathanmorgan/work/django/research/research/work/phd_work\"\n",
    "django_init_path = \"django_init.py\"\n",
    "if( ( django_init_folder is not None ) and ( django_init_folder != \"\" ) ):\n",
    "    \n",
    "    # add folder to front of path.\n",
    "    django_init_path = \"{}/{}\".format( django_init_folder, django_init_path )\n",
    "    \n",
    "#-- END check to see if django_init folder. --#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06d7b339-deb5-48fe-855a-f6f5dabc9ff7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-17T15:43:54.453200Z",
     "start_time": "2019-09-17T15:43:52.833671Z"
    }
   },
   "outputs": [],
   "source": [
    "%run $django_init_path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b43bc258-0608-4215-9698-3fe276f5347c",
   "metadata": {},
   "source": [
    "### Setup - django-related imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6a4e76a-de34-4d86-9e43-b74e4e662332",
   "metadata": {},
   "outputs": [],
   "source": [
    "# python utilities\n",
    "from python_utilities.strings.string_helper import StringHelper\n",
    "\n",
    "# django imports\n",
    "from django.contrib.auth.models import User\n",
    "from django.db.models import Max\n",
    "from django.db.models import Min\n",
    "\n",
    "# sourcenet imports\n",
    "from context_text.shared.context_text_base import ContextTextBase\n",
    "\n",
    "# context_analysis imports\n",
    "from context_analysis.network.network_person_info import NetworkPersonInfo\n",
    "\n",
    "# sourcenet imports\n",
    "from context_text.models import Article\n",
    "from context_text.models import Article_Author\n",
    "from context_text.models import Article_Data\n",
    "from context_text.models import Article_Subject\n",
    "from context_text.models import Newspaper\n",
    "from context_text.models import Person\n",
    "\n",
    "# article coding\n",
    "from context_text.article_coding.article_coder import ArticleCoder\n",
    "from context_text.article_coding.article_coding import ArticleCoding\n",
    "from context_text.article_coding.open_calais_v2.open_calais_v2_article_coder import OpenCalaisV2ArticleCoder\n",
    "from context_text.article_coding.open_calais_v2.open_calais_v2_api_response import OpenCalaisV2ApiResponse\n",
    "\n",
    "# article data collection\n",
    "from context_text.collectors.newsbank.newspapers.GRPB import GRPB\n",
    "from context_text.collectors.newsbank.newspapers.DTNB import DTNB\n",
    "\n",
    "# import class that actually processes requests for outputting networks.\n",
    "from context_text.export.network_output import NetworkOutput\n",
    "\n",
    "# context_text shared\n",
    "from context_text.shared.context_text_base import ContextTextBase\n",
    "\n",
    "print( \"django model packages imported at \" + str( datetime.datetime.now() ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "834b9dd9-c280-4622-a5b7-f5ebb34f3da4",
   "metadata": {},
   "source": [
    "## Setup - Initialize LoggingHelper\n",
    "\n",
    "- Back to [Table of Contents](#Table-of-Contents)\n",
    "\n",
    "Create a LoggingHelper instance to use to log debug and also print at the same time.\n",
    "\n",
    "Preconditions: Must be run after Django is initialized, since `python_utilities` is in the django path."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54ddcce8-4812-4daa-9a22-1419f9861ca8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-17T15:43:55.604797Z",
     "start_time": "2019-09-17T15:43:55.589736Z"
    }
   },
   "outputs": [],
   "source": [
    "# python_utilities\n",
    "from python_utilities.logging.logging_helper import LoggingHelper\n",
    "\n",
    "# init\n",
    "my_logging_helper = LoggingHelper()\n",
    "my_logging_helper.set_logger_name( \"newsbank-article_coding-unittest\" )\n",
    "log_message = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fcc2eb0-805c-44ba-a465-69c5c0a3daae",
   "metadata": {},
   "source": [
    "## Setup - load fixtures and prepare database\n",
    "\n",
    "- Back to [Table of Contents](#Table-of-Contents)\n",
    "\n",
    "Detailed instructions: [https://github.com/jonathanmorgan/context_text#using-unittest-data-for-development](https://github.com/jonathanmorgan/context_text#using-unittest-data-for-development)\n",
    "\n",
    "Create test database, load fixtures, etc.:\n",
    "\n",
    "- create a database where the unit test data can live.  I usually call it the name of the main production database (\"`research`\") followed by \"`_test`\".  Easiest way to do this is to just create the database, then give the same user you use for your production database the same access they have for production for this test database as well.\n",
    "\n",
    "    - postgresql example, where production database name is \"`research`\" and database user is \"`django_user`\":\n",
    "\n",
    "            CREATE DATABASE research_test OWNER django_user;\n",
    "            GRANT ALL PRIVILEGES ON DATABASE research_test TO django_user;\n",
    "\n",
    "- update the DATABASES dictionary in settings.py of the application that contains context_text to point to your test database (in easy example above, could just change the 'NAME' attribute in the 'default' entry to \"`research_test`\" rather than \"`research`\".\n",
    "- cd into your django application's home directory, activate your virtualenv if you created one, then run \"`python manage.py migrate`\" to create all the tables in the database.\n",
    "\n",
    "        cd <django_app_directory>\n",
    "        workon research\n",
    "        python manage.py migrate\n",
    "\n",
    "- use the command \"`python manage.py createsuperuser`\" to make an admin user, for logging into the django admins.\n",
    "\n",
    "        python manage.py createsuperuser\n",
    "\n",
    "- load the unit test fixtures into the database, including \"export\" data with `Article_Data` coded by OpenCalais v.2:\n",
    "\n",
    "        python manage.py loaddata context_text_unittest_export_auth_data.json\n",
    "        python manage.py loaddata context_text_unittest_django_config_data.json\n",
    "        python manage.py loaddata context_text_unittest_export_data.json\n",
    "        python manage.py loaddata context_text_unittest_export_taggit_data.json\n",
    "        python manage.py loaddata context-sourcenet_entity_and_relation_types.json\n",
    "\n",
    "- Then, you can set the OpenCalais v.2 Access Token `django_config` property (application = “OpenCalais_REST_API_v2”; property name = “open_calais_access_token”) to your OpenCalais Token value.  This should let OpenCalais work correctly on this database."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6807bbbf-ecfb-4a1f-b6d2-2e326020192e",
   "metadata": {},
   "source": [
    "## Setup - shared variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1950db4c-a0f2-4e6e-b319-1a261e649732",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get ArticleCoding instance.\n",
    "#article_coding = ArticleCoding()\n",
    "\n",
    "# automated coding user\n",
    "automated_coder = ArticleCoder.get_automated_coding_user()\n",
    "\n",
    "# newspapers for Grand Rapids Press and Detroit News.\n",
    "grand_rapids_press = Newspaper.objects.get( newsbank_code = \"GRPB\" )\n",
    "detroit_news = Newspaper.objects.get( newsbank_code = \"DTNB\" )\n",
    "\n",
    "# OpenCalais v2 coder type\n",
    "ocv2_coder_type = OpenCalaisV2ArticleCoder.CONFIG_APPLICATION"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b136e39-e511-471d-af01-bdf82a41c01a",
   "metadata": {},
   "source": [
    "## Setup - functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41181299-7ef6-429e-b7b6-2a46fbc0efa7",
   "metadata": {},
   "source": [
    "### Setup - function `make_string_hash()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "158da937-36c5-4488-897f-94e6c423a109",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_string_hash( value_IN, hash_function_IN = hashlib.sha256 ):\n",
    "\n",
    "    # return reference\n",
    "    value_OUT = None\n",
    "\n",
    "    # declare variables\n",
    "    me = \"make_string_hash\"\n",
    "\n",
    "    # call StringHelper method.\n",
    "    value_OUT = StringHelper.make_string_hash( value_IN, hash_function_IN = hash_function_IN )\n",
    "\n",
    "    return value_OUT\n",
    "\n",
    "#-- END function make_string_hash() --#\n",
    "\n",
    "print( \"function make_string_hash() defined at \" + str( datetime.datetime.now() ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca3d85f8-5a4e-42fe-b868-ec82afa16c0d",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Examine articles and `Article_Data`\n",
    "\n",
    "- Back to [Table of Contents](#Table-of-Contents)\n",
    "\n",
    "Tag all locally implemented hard news articles in database and all that have already been coded using Open Calais V2, then work through using OpenCalais to code all local hard news that hasn't alredy been coded, starting with those proximal to the coding sample for methods paper."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98193820-335b-4263-bd8d-d5f3dfa1b803",
   "metadata": {},
   "source": [
    "## which articles have already been coded?\n",
    "\n",
    "- Back to [Table of Contents](#Table-of-Contents)\n",
    "\n",
    "More precisely, find all articles that have Article_Data coded by the automated coder with type \"OpenCalais_REST_API_v2\" and tag the articles as \"coded-open_calais_v2\" or something like that.\n",
    "\n",
    "Then, for articles without that tag, use our criteria for local hard news to filter out and tag publications in the year before and after the month used to evaluate the automated coder, in both the Grand Rapids Press and the Detroit News, so I can look at longer time frames, then code all articles currently in database.\n",
    "\n",
    "Eventually, then, we'll code and examine before and after layoffs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb9ed921-13ca-4d38-b683-34b3b8f07434",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-17T16:46:33.532929Z",
     "start_time": "2019-09-17T16:46:33.526815Z"
    }
   },
   "outputs": [],
   "source": [
    "# look for publications that have article data:\n",
    "# - coded by automated coder\n",
    "# - with coder type of \"OpenCalais_REST_API_v2\"\n",
    "\n",
    "# get automated coder\n",
    "automated_coder_user = ArticleCoder.get_automated_coding_user()\n",
    "\n",
    "print( \"{} - Loaded automated user: {}, id = {}\".format( datetime.datetime.now(), automated_coder_user, automated_coder_user.id ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd048431-10c5-4a4d-86f3-2a62aa3311a3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-17T16:46:36.155654Z",
     "start_time": "2019-09-17T16:46:36.145324Z"
    }
   },
   "outputs": [],
   "source": [
    "# try aggregates\n",
    "article_qs = Article.objects.all()\n",
    "pub_date_info = article_qs.aggregate( Max( 'pub_date' ), Min( 'pub_date' ) )\n",
    "print( pub_date_info )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75aa15c5-e0d5-4ff6-a30b-a997e85d0a06",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-17T16:46:38.265503Z",
     "start_time": "2019-09-17T16:46:38.247164Z"
    }
   },
   "outputs": [],
   "source": [
    "# find articles with Article_Data created by the automated user...\n",
    "article_qs = Article.objects.filter( article_data__coder = automated_coder_user )\n",
    "\n",
    "# ...and specifically coded using OpenCalais V2...\n",
    "article_qs = article_qs.filter( article_data__coder_type = OpenCalaisV2ArticleCoder.CONFIG_APPLICATION )\n",
    "\n",
    "# ...and finally, we just want the distinct articles by ID.\n",
    "article_qs = article_qs.order_by( \"id\" ).distinct( \"id\" )\n",
    "\n",
    "# count?\n",
    "article_count = article_qs.count()\n",
    "print( \"Found {} articles\".format( article_count ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ba0be6f-e582-485c-adea-6638d8959e6f",
   "metadata": {},
   "source": [
    "### Tag the coded articles\n",
    "\n",
    "- Back to [Table of Contents](#Table-of-Contents)\n",
    "\n",
    "Removing duplicates present from joining with Article_Data yields 579 articles that were coded by the automated coder.\n",
    "\n",
    "Tag all the coded articles with `OpenCalaisV2ArticleCoder.TAG_CODED_BY_ME`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e833f54-16e3-4c99-bdd4-8a40f001ffe7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-17T16:46:46.423691Z",
     "start_time": "2019-09-17T16:46:46.293284Z"
    }
   },
   "outputs": [],
   "source": [
    "# declare variables\n",
    "current_article = None\n",
    "tag_name_list = None\n",
    "article_count = None\n",
    "untagged_count = None\n",
    "already_tagged_count = None\n",
    "newly_tagged_count = None\n",
    "count_sum = None\n",
    "do_add_tag = False\n",
    "\n",
    "# init\n",
    "do_add_tag = False\n",
    "\n",
    "# get article_count\n",
    "article_count = article_qs.count()\n",
    "\n",
    "# loop over articles.\n",
    "untagged_count = 0\n",
    "already_tagged_count = 0\n",
    "newly_tagged_count = 0\n",
    "for current_article in article_qs:\n",
    "    \n",
    "    # get list of tags for this publication\n",
    "    tag_name_list = current_article.tags.names()\n",
    "    \n",
    "    # is the coded tag in the list?\n",
    "    if ( OpenCalaisV2ArticleCoder.TAG_CODED_BY_ME not in tag_name_list ):\n",
    "        \n",
    "        # are we adding tag?\n",
    "        if ( do_add_tag == True ):\n",
    "\n",
    "            # add tag.\n",
    "            current_article.tags.add( OpenCalaisV2ArticleCoder.TAG_CODED_BY_ME )\n",
    "            newly_tagged_count += 1\n",
    "            \n",
    "        else:\n",
    "\n",
    "            # for now, increment untagged count\n",
    "            untagged_count += 1\n",
    "            \n",
    "        #-- END check to see if we are adding tag. --#\n",
    "        \n",
    "    else:\n",
    "        \n",
    "        # already tagged\n",
    "        already_tagged_count += 1\n",
    "        \n",
    "    #-- END check to see if coded tag is set --#\n",
    "    \n",
    "#-- END loop over articles. --#\n",
    "\n",
    "print( \"Article counts:\" )\n",
    "print( \"- total articles: {}\".format( article_count ) )\n",
    "print( \"- untagged articles: {}\".format( untagged_count ) )\n",
    "print( \"- already tagged: {}\".format( already_tagged_count ) )\n",
    "print( \"- newly tagged: {}\".format( newly_tagged_count ) )\n",
    "count_sum = untagged_count + already_tagged_count + newly_tagged_count\n",
    "print( \"- count sum: {}\".format( count_sum ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8103acb-8361-4954-bfdc-f4af3b2a13fa",
   "metadata": {},
   "source": [
    "### Profile the coded articles\n",
    "\n",
    "- Back to [Table of Contents](#Table-of-Contents)\n",
    "\n",
    "Look at range of pub dates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a009481-e5a3-42e2-8ba9-b234043255b0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-03T03:52:35.494718Z",
     "start_time": "2019-08-03T03:52:35.473029Z"
    }
   },
   "outputs": [],
   "source": [
    "tags_in_list = []\n",
    "tags_in_list.append( OpenCalaisV2ArticleCoder.TAG_CODED_BY_ME )\n",
    "article_qs = Article.objects.filter( tags__name__in = tags_in_list )\n",
    "print( \"Matching article count: {}\".format( article_qs.count() ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12d7a6bb-baab-47ef-ad6b-d44288af48c9",
   "metadata": {},
   "source": [
    "- Original: 579\n",
    "- after coding 10: 589 (tag is being set correctly by Open Calais V2 coder)\n",
    "- 2019.08.02 - after 5000 (minus a few errors because 2 seconds isn't quite enough for rate limit): 5518"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb756371-8206-4ca9-b5a8-dbe04b910af5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-31T17:49:07.328924Z",
     "start_time": "2019-07-31T17:49:07.275399Z"
    }
   },
   "outputs": [],
   "source": [
    "# profile these publications\n",
    "min_pub_date = None\n",
    "max_pub_date = None\n",
    "current_pub_date = None\n",
    "pub_date_count = None\n",
    "date_to_count_map = {}\n",
    "date_to_articles_map = {}\n",
    "pub_date_article_dict = None\n",
    "\n",
    "# try aggregates\n",
    "pub_date_info = article_qs.aggregate( Max( 'pub_date' ), Min( 'pub_date' ) )\n",
    "print( pub_date_info )\n",
    "\n",
    "# counts of pubs by date\n",
    "for current_article in article_qs:\n",
    "    \n",
    "    # get pub_date\n",
    "    current_pub_date = current_article.pub_date\n",
    "    current_article_id = current_article.id\n",
    "    \n",
    "    # get count, increment, and store.\n",
    "    pub_date_count = date_to_count_map.get( current_pub_date, 0 )\n",
    "    pub_date_count += 1\n",
    "    date_to_count_map[ current_pub_date ] = pub_date_count\n",
    "    \n",
    "    # also, store up ids and instances\n",
    "    \n",
    "    # get dict of article ids to article instances for date\n",
    "    pub_date_article_dict = date_to_articles_map.get( current_pub_date, {} )\n",
    "    \n",
    "    # article already there?\n",
    "    if ( current_article_id not in pub_date_article_dict ):\n",
    "        \n",
    "        # no - add it.\n",
    "        pub_date_article_dict[ current_article_id ] = current_article\n",
    "        \n",
    "    #-- END check to see if article already there.\n",
    "    \n",
    "    # put dict back.\n",
    "    date_to_articles_map[ current_pub_date ] = pub_date_article_dict\n",
    "    \n",
    "#-- END loop over articles. --#\n",
    "\n",
    "# output dates and counts.\n",
    "\n",
    "# get list of keys from map\n",
    "keys_list = list( six.viewkeys( date_to_count_map ) )\n",
    "keys_list.sort()\n",
    "for current_pub_date in keys_list:\n",
    "    \n",
    "    # get count\n",
    "    pub_date_count = date_to_count_map.get( current_pub_date, 0 )\n",
    "    print( \"- {} ( {} ) count: {}\".format( current_pub_date, type( current_pub_date ), pub_date_count ) )\n",
    "    \n",
    "#-- END loop over dates --#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "203740fa-bc90-437a-9af6-3fe7298ca213",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-31T17:49:13.567482Z",
     "start_time": "2019-07-31T17:49:13.535813Z"
    }
   },
   "outputs": [],
   "source": [
    "# look at articles for a particular date\n",
    "focus_date = \"2009-12-10\"\n",
    "pub_date = datetime.datetime.strptime( focus_date, \"%Y-%m-%d\" ).date()\n",
    "articles_for_date = date_to_articles_map.get( pub_date, {} )\n",
    "\n",
    "# get each article\n",
    "for article_id, article_instance in articles_for_date.items():\n",
    "    \n",
    "    # look at its tags.\n",
    "    print( \"\\n==> Article {article_id}: {article_summary}\".format( article_id = article_id, article_summary = article_instance ) )\n",
    "    print( \"- tags: {}\".format( article_instance.tags.all() ) )\n",
    "\n",
    "    # loop over associated Article_Data instances.\n",
    "    for article_data in article_instance.article_data_set.all():\n",
    "\n",
    "        print( \"----> article_data: {}\".format( article_data ) )\n",
    "\n",
    "    #-- END loop over associated Article_Data instances --#\n",
    "\n",
    "#-- END loop over articles for date."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2d5722d-ad2b-44ed-8247-9235dd1b514b",
   "metadata": {},
   "source": [
    "## tag all local news\n",
    "\n",
    "- Back to [Table of Contents](#Table-of-Contents)\n",
    "\n",
    "Definition of local hard news by in-house implementor for Grand Rapids Press and Detroit News follow.  For each, tag all articles in database that match as \"local_hard_news\"."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e221ed7c-15c7-45db-9191-df1f6da06334",
   "metadata": {},
   "source": [
    "### TODO\n",
    "\n",
    "- Back to [Table of Contents](#Table-of-Contents)\n",
    "\n",
    "TODO:\n",
    "\n",
    "- make class for GRPB at NewsBank.\n",
    "\n",
    "    - also, pull the things that are newspaper specific out of ArticleCoder.py and into the GRPB.py class.\n",
    "\n",
    "- refine \"local news\" and \"locally created\" regular expressions for Grand Rapids Press based on contents of `author_string` and `author_affiliation`.\n",
    "- do the same for TDN.\n",
    "- then, use the updated classes and definitions below to flag all local hard news in database for each publication."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16b068cd-751c-426c-adb1-01c32770394b",
   "metadata": {},
   "source": [
    "#### DONE\n",
    "\n",
    "- Back to [Table of Contents](#Table-of-Contents)\n",
    "\n",
    "DONE:\n",
    "\n",
    "- abstract out shared stuff from GRPB.py and DTNB.py into abstract parent class context_text/collectors/newsbank/newspapers/newsbank_newspaper.py\n",
    "\n",
    "    - update DTNB.py to use the parent class.\n",
    "    \n",
    "- make class for GRPB at NewsBank.\n",
    "\n",
    "    - context_text/collectors/newsbank/newspapers/GRPB.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2265bf69-d618-4298-bf26-2a4ce2a28392",
   "metadata": {},
   "source": [
    "### Grand Rapids Press local news\n",
    "\n",
    "- Back to [Table of Contents](#Table-of-Contents)\n",
    "\n",
    "Grand Rapids Press local hard news:\n",
    "\n",
    "- `context_text/examples/articles/articles-GRP-local_news.py`\n",
    "- local hard news sections (stored in `Article.GRP_NEWS_SECTION_NAME_LIST`):\n",
    "\n",
    "    - \"Business\"\n",
    "    - \"City and Region\"\n",
    "    - \"Front Page\"\n",
    "    - \"Lakeshore\"\n",
    "    - \"Religion\"\n",
    "    - \"Special\"\n",
    "    - \"State\"\n",
    "\n",
    "- in-house implementor (based on byline patterns, stored in `sourcenet.models.Article.Q_GRP_IN_HOUSE_AUTHOR`):\n",
    "\n",
    "    - Byline ends in \"/ THE GRAND RAPIDS PRESS\", ignore case.\n",
    "\n",
    "        - `Q( author_varchar__iregex = r'.* */ *THE GRAND RAPIDS PRESS$'`\n",
    "\n",
    "    - Byline ends in \"/ PRESS * EDITOR\", ignore case.\n",
    "\n",
    "        - `Q( author_varchar__iregex = r'.* */ *PRESS .* EDITOR$' )`\n",
    "\n",
    "    - Byline ends in \"/ GRAND RAPIDS PRESS * BUREAU\", ignore case.\n",
    "\n",
    "        - `Q( author_varchar__iregex = r'.* */ *GRAND RAPIDS PRESS .* BUREAU$' )`\n",
    "\n",
    "    - Byline ends in \"/ SPECIAL TO THE PRESS\", ignore case.\n",
    "\n",
    "        - `Q( author_varchar__iregex = r'.* */ *SPECIAL TO THE PRESS$' )`\n",
    "        \n",
    "- can also exclude columns (I will not):\n",
    "\n",
    "        grp_article_qs = grp_article_qs.exclude( index_terms__icontains = \"Column\" )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da3e0658-feb9-4c42-b2d4-1aaa7d23ee76",
   "metadata": {},
   "source": [
    "Need to work to further refine this.\n",
    "\n",
    "Looking at affiliation strings:\n",
    "\n",
    "    SELECT author_affiliation, COUNT( author_affiliation ) as affiliation_count\n",
    "    FROM context_text_article\n",
    "    WHERE newspaper_id = 1\n",
    "    GROUP BY author_affiliation\n",
    "    ORDER BY COUNT( author_affiliation ) DESC;\n",
    "    \n",
    "And at author strings for collective bylines:\n",
    "\n",
    "    SELECT author_string, COUNT( author_string ) as author_count\n",
    "    FROM context_text_article\n",
    "    WHERE newspaper_id = 1\n",
    "    GROUP BY author_string\n",
    "    ORDER BY COUNT( author_string ) DESC\n",
    "    LIMIT 10;\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "908fed9a-c6e9-455e-a0d6-9f6aac57354f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-13T02:22:10.778554Z",
     "start_time": "2019-09-13T02:22:10.521947Z"
    }
   },
   "outputs": [],
   "source": [
    "# filter queryset to just locally created Grand Rapids Press (GRP) articles.\n",
    "# imports\n",
    "from context_text.models import Article\n",
    "from context_text.models import Newspaper\n",
    "from context_text.shared.context_text_base import ContextTextBase\n",
    "from context_text.collectors.newsbank.newspapers.GRPB import GRPB\n",
    "\n",
    "# declare variables - Grand Rapids Press\n",
    "do_apply_tag = False\n",
    "tag_to_apply = None\n",
    "grp_local_news_sections = []\n",
    "grp_newspaper = None\n",
    "grp_article_qs = None\n",
    "article_count = -1\n",
    "\n",
    "# declare variables - filtering\n",
    "include_opinion_columns = True\n",
    "tags_in_list = []\n",
    "tags_not_in_list = []\n",
    "filter_out_prelim_tags = False\n",
    "random_count = -1\n",
    "\n",
    "# declare variables - make list of article IDs from QS.\n",
    "article_id_list = []\n",
    "article_counter = -1\n",
    "current_article = None\n",
    "article_tag_name_list = None\n",
    "article_update_counter = -1\n",
    "\n",
    "# ==> configure\n",
    "\n",
    "# configure - size of random sample we want\n",
    "#random_count = 60\n",
    "\n",
    "# configure - also, apply tag?\n",
    "do_apply_tag = False\n",
    "tag_to_apply = ContextTextBase.TAG_LOCAL_HARD_NEWS\n",
    "\n",
    "# set up \"local, regional and state news\" sections\n",
    "grp_local_news_sections = GRPB.LOCAL_NEWS_SECTION_NAME_LIST\n",
    "\n",
    "# Grand Rapids Press\n",
    "# get newspaper instance for GRP.\n",
    "grp_newspaper = Newspaper.objects.get( id = GRPB.NEWSPAPER_ID )\n",
    "\n",
    "# start with all articles\n",
    "#grp_article_qs = Article.objects.all()\n",
    "\n",
    "# ==> filter to newspaper, local news section list, and in-house reporters.\n",
    "\n",
    "# ----> manually\n",
    "\n",
    "# now, need to find local news articles to test on.\n",
    "#grp_article_qs = grp_article_qs.filter( newspaper = grp_newspaper )\n",
    "\n",
    "# only the locally implemented sections\n",
    "#grp_article_qs = grp_article_qs.filter( section__in = grp_local_news_sections )\n",
    "\n",
    "# and, with an in-house author\n",
    "#grp_article_qs = grp_article_qs.filter( Article.Q_GRP_IN_HOUSE_AUTHOR )\n",
    "\n",
    "#print( \"manual filter count: {}\".format( grp_article_qs.count() ) )\n",
    "\n",
    "# ----> using Article.filter_articles()\n",
    "grp_article_qs = Article.filter_articles( qs_IN = grp_article_qs,\n",
    "                                          newspaper = grp_newspaper,\n",
    "                                          section_name_list = grp_local_news_sections,\n",
    "                                          custom_article_q = GRPB.Q_IN_HOUSE_AUTHOR )\n",
    "\n",
    "print( \"Article.filter_articles count: {}\".format( grp_article_qs.count() ) )\n",
    "\n",
    "# and include opinion columns?\n",
    "if ( include_opinion_columns == False ):\n",
    "    \n",
    "    # do not include columns\n",
    "    grp_article_qs = grp_article_qs.exclude( index_terms__icontains = \"Column\" )\n",
    "    \n",
    "#-- END check to see if we include columns. --#\n",
    "\n",
    "'''\n",
    "# filter to newspaper, section list, and in-house reporters.\n",
    "grp_article_qs = Article.filter_articles( qs_IN = grp_article_qs,\n",
    "                                          start_date = \"2009-12-01\",\n",
    "                                          end_date = \"2009-12-31\",\n",
    "                                          newspaper = grp_newspaper,\n",
    "                                          section_name_list = grp_local_news_sections,\n",
    "                                          custom_article_q = Article.Q_GRP_IN_HOUSE_AUTHOR )\n",
    "'''\n",
    "\n",
    "# how many is that?\n",
    "article_count = grp_article_qs.count()\n",
    "\n",
    "print( \"Article count before filtering on tags: \" + str( article_count ) )\n",
    "\n",
    "# ==> tags\n",
    "\n",
    "# tags to exclude\n",
    "tags_not_in_list = []\n",
    "\n",
    "# Example: prelim-related tags\n",
    "#tags_not_in_list.append( \"prelim_reliability\" )\n",
    "#tags_not_in_list.append( \"prelim_network\" ]\n",
    "#tags_not_in_list.append( \"minnesota1-20160328\" )\n",
    "#tags_not_in_list.append( \"minnesota2-20160328\" )\n",
    "\n",
    "# for later - exclude articles already coded.\n",
    "#tags_not_in_list.append( OpenCalaisV2ArticleCoder.TAG_CODED_BY_ME )\n",
    "\n",
    "# exclude any already tagged with tag_to_apply\n",
    "tags_not_in_list.append( tag_to_apply )\n",
    "\n",
    "if ( ( tags_not_in_list is not None ) and ( len( tags_not_in_list ) > 0 ) ):\n",
    "\n",
    "    # exclude those in a list\n",
    "    print( \"filtering out articles with tags: \" + str( tags_not_in_list ) )\n",
    "    grp_article_qs = grp_article_qs.exclude( tags__name__in = tags_not_in_list )\n",
    "\n",
    "#-- END check to see if we have a specific list of tags we want to exclude --#\n",
    "\n",
    "# include only those with certain tags.\n",
    "tags_in_list = []\n",
    "\n",
    "# Examples\n",
    "\n",
    "# Examples: prelim-related tags\n",
    "#tags_in_list.append( \"prelim_unit_test_001\" )\n",
    "#tags_in_list.append( \"prelim_unit_test_002\" )\n",
    "#tags_in_list.append( \"prelim_unit_test_003\" )\n",
    "#tags_in_list.append( \"prelim_unit_test_004\" )\n",
    "#tags_in_list.append( \"prelim_unit_test_005\" )\n",
    "#tags_in_list.append( \"prelim_unit_test_006\" )\n",
    "#tags_in_list.append( \"prelim_unit_test_007\" )\n",
    "\n",
    "# Example: grp_month\n",
    "#tags_in_list.append( \"grp_month\" )\n",
    "\n",
    "if ( ( tags_in_list is not None ) and ( len( tags_in_list ) > 0 ) ):\n",
    "\n",
    "    # filter\n",
    "    print( \"filtering to just articles with tags: \" + str( tags_in_list ) )\n",
    "    grp_article_qs = grp_article_qs.filter( tags__name__in = tags_in_list )\n",
    "    \n",
    "#-- END check to see if we have a specific list of tags we want to include --#\n",
    "\n",
    "# filter out \"*prelim*\" tags?\n",
    "#filter_out_prelim_tags = True\n",
    "if ( filter_out_prelim_tags == True ):\n",
    "\n",
    "    # ifilter out all articles with any tag whose name contains \"prelim\".\n",
    "    print( \"filtering out articles with tags that contain \\\"prelim\\\"\" )\n",
    "    grp_article_qs = grp_article_qs.exclude( tags__name__icontains = \"prelim\" )\n",
    "    \n",
    "#-- END check to see if we filter out \"prelim_*\" tags --#\n",
    "\n",
    "# how many is that?\n",
    "article_count = grp_article_qs.count()\n",
    "\n",
    "print( \"Article count after tag filtering: \" + str( article_count ) )\n",
    "\n",
    "# do we want a random sample?\n",
    "if ( random_count > 0 ):\n",
    "\n",
    "    # to get random, order them by \"?\", then use slicing to retrieve requested\n",
    "    #     number.\n",
    "    grp_article_qs = grp_article_qs.order_by( \"?\" )[ : random_count ]\n",
    "    \n",
    "#-- END check to see if we want random sample --#\n",
    "\n",
    "# this is a nice algorithm, also:\n",
    "# - http://www.titov.net/2005/09/21/do-not-use-order-by-rand-or-how-to-get-random-rows-from-table/\n",
    "\n",
    "# make ID list, tag articles if configured to.\n",
    "article_id_list = []\n",
    "article_counter = 0\n",
    "article_update_counter = 0\n",
    "for current_article in grp_article_qs:\n",
    "\n",
    "    # increment article_counter\n",
    "    article_counter += 1\n",
    "\n",
    "    # add IDs to article_id_list\n",
    "    article_id_list.append( str( current_article.id ) )\n",
    "    \n",
    "    # apply a tag while we are at it?\n",
    "    if ( ( do_apply_tag == True ) and ( tag_to_apply is not None ) and ( tag_to_apply != \"\" ) ):\n",
    "    \n",
    "        # yes, please.  Tag already present?\n",
    "        article_tag_name_list = current_article.tags.names()\n",
    "        if ( tag_to_apply not in article_tag_name_list ):\n",
    "\n",
    "            # Add tag.\n",
    "            current_article.tags.add( tag_to_apply )\n",
    "            \n",
    "            # increment counter\n",
    "            article_update_counter += 1\n",
    "            \n",
    "        #-- END check to see if tag already present. --#\n",
    "        \n",
    "    #-- END check to see if we apply tag. --#\n",
    "\n",
    "    # output the tags.\n",
    "    if ( debug_flag == True ):\n",
    "        print( \"- Tags for article \" + str( current_article.id ) + \" : \" + str( current_article.tags.all() ) )\n",
    "    #-- END DEBUG --#\n",
    "\n",
    "#-- END loop over articles --#\n",
    "\n",
    "# output the list.\n",
    "print( \"grp_article_qs count: {}\".format( grp_article_qs.count() ) )\n",
    "print( \"Found \" + str( article_counter ) + \" articles ( \" + str( article_count ) + \" ).\" )\n",
    "print( \"- Updated {} articles to add tag {}.\".format( article_update_counter, tag_to_apply ) )\n",
    "if ( debug_flag == True ):\n",
    "    print( \"List of \" + str( len( article_id_list ) ) + \" local GRP staff article IDs: \" + \", \".join( article_id_list ) )\n",
    "#-- END DEBUG --#\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14f263d7-f76b-4922-a6ef-5383541353a3",
   "metadata": {},
   "source": [
    "### Detroit News local news\n",
    "\n",
    "- Back to [Table of Contents](#Table-of-Contents)\n",
    "\n",
    "Detroit News local news:\n",
    "\n",
    "- `context_text/examples/articles/articles-TDN-local_news.py`\n",
    "- local hard news sections (stored in `from context_text.collectors.newsbank.newspapers.DTNB import DTNB` - `DTNB.NEWS_SECTION_NAME_LIST`):\n",
    "\n",
    "    - \"Business\"\n",
    "    - \"Metro\"\n",
    "    - \"Nation\" - because of auto industry stories\n",
    "\n",
    "- in-house implementor (based on byline patterns, stored in `DTNB.Q_IN_HOUSE_AUTHOR`):\n",
    "\n",
    "    - Byline ends in \"/ The Detroit News\", ignore case.\n",
    "\n",
    "        - `Q( author_varchar__iregex = r'.*\\s*/\\s*the\\s*detroit\\s*news$' )`\n",
    "\n",
    "    - Byline ends in \"Special to The Detroit News\", ignore case.\n",
    "\n",
    "        - `Q( author_varchar__iregex = r'.*\\s*/\\s*special\\s*to\\s*the\\s*detroit\\s*news$' )`\n",
    "\n",
    "    - Byline ends in \"Detroit News * Bureau\", ignore case.\n",
    "\n",
    "        - `Q( author_varchar__iregex = r'.*\\s*/\\s*detroit\\s*news\\s*.*\\s*bureau$' )`   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c9d047a-a210-405b-a3a1-3df84ce39a89",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-04T02:42:42.445226Z",
     "start_time": "2019-07-04T02:42:39.261361Z"
    }
   },
   "outputs": [],
   "source": [
    "# filter queryset to just locally created Detroit News (TDN) articles.\n",
    "# imports\n",
    "from context_text.models import Article\n",
    "from context_text.models import Newspaper\n",
    "from context_text.shared.context_text_base import ContextTextBase\n",
    "from context_text.collectors.newsbank.newspapers.DTNB import DTNB\n",
    "\n",
    "# declare variables - Detroit News\n",
    "do_apply_tag = False\n",
    "tag_to_apply = None\n",
    "tdn_local_news_sections = []\n",
    "tdn_newspaper = None\n",
    "tdn_article_qs = None\n",
    "article_count = -1\n",
    "\n",
    "# declare variables - filtering\n",
    "include_opinion_columns = True\n",
    "tags_in_list = []\n",
    "tags_not_in_list = []\n",
    "filter_out_prelim_tags = False\n",
    "random_count = -1\n",
    "\n",
    "# declare variables - make list of article IDs from QS.\n",
    "article_id_list = []\n",
    "article_counter = -1\n",
    "current_article = None\n",
    "\n",
    "# ==> configure\n",
    "\n",
    "# configure - size of random sample we want\n",
    "#random_count = 60\n",
    "\n",
    "# configure - also, apply tag?\n",
    "do_apply_tag = False\n",
    "tag_to_apply = ContextTextBase.TAG_LOCAL_HARD_NEWS\n",
    "\n",
    "# set up \"local, regional and state news\" sections\n",
    "tdn_local_news_sections = DTNB.LOCAL_NEWS_SECTION_NAME_LIST\n",
    "\n",
    "# Detroit News\n",
    "# get newspaper instance for TDN.\n",
    "tdn_newspaper = Newspaper.objects.get( id = DTNB.NEWSPAPER_ID )\n",
    "\n",
    "# start with all articles\n",
    "#tdn_article_qs = Article.objects.all()\n",
    "\n",
    "# ==> filter to newspaper, local news section list, and in-house reporters.\n",
    "\n",
    "# ----> manually\n",
    "\n",
    "# now, need to find local news articles to test on.\n",
    "#tdn_article_qs = tdn_article_qs.filter( newspaper = tdn_newspaper )\n",
    "\n",
    "# only the locally implemented sections\n",
    "#tdn_article_qs = tdn_article_qs.filter( section__in = tdn_local_news_sections )\n",
    "\n",
    "# and, with an in-house author\n",
    "#tdn_article_qs = tdn_article_qs.filter( DTNB.Q_IN_HOUSE_AUTHOR )\n",
    "\n",
    "#print( \"manual filter count: {}\".format( tdn_article_qs.count() ) )\n",
    "\n",
    "# ----> using Article.filter_articles()\n",
    "tdn_article_qs = Article.filter_articles( qs_IN = tdn_article_qs,\n",
    "                                          newspaper = tdn_newspaper,\n",
    "                                          section_name_list = tdn_local_news_sections,\n",
    "                                          custom_article_q = DTNB.Q_IN_HOUSE_AUTHOR )\n",
    "\n",
    "print( \"Article.filter_articles count: {}\".format( tdn_article_qs.count() ) )\n",
    "\n",
    "# and include opinion columns?\n",
    "if ( include_opinion_columns == False ):\n",
    "    \n",
    "    # do not include columns\n",
    "    tdn_article_qs = tdn_article_qs.exclude( author_string__in = DTNB.COLUMNIST_NAME_LIST )\n",
    "    \n",
    "#-- END check to see if we include columns. --#\n",
    "\n",
    "'''\n",
    "# filter to newspaper, section list, and in-house reporters.\n",
    "tdn_article_qs = Article.filter_articles( qs_IN = tdn_article_qs,\n",
    "                                          start_date = \"2009-12-01\",\n",
    "                                          end_date = \"2009-12-31\",\n",
    "                                          newspaper = tdn_newspaper,\n",
    "                                          section_name_list = tdn_local_news_sections,\n",
    "                                          custom_article_q = DTNB.Q_IN_HOUSE_AUTHOR )\n",
    "'''\n",
    "\n",
    "# how many is that?\n",
    "article_count = tdn_article_qs.count()\n",
    "\n",
    "print( \"Article count before filtering on tags: \" + str( article_count ) )\n",
    "\n",
    "# ==> tags\n",
    "\n",
    "# tags to exclude\n",
    "#tags_not_in_list = [ \"prelim_reliability\", \"prelim_network\" ]\n",
    "#tags_not_in_list = [ \"minnesota1-20160328\", \"minnesota2-20160328\", ]\n",
    "\n",
    "# for later - exclude articles already coded.\n",
    "#tags_not_in_list = [ OpenCalaisV2ArticleCoder.TAG_CODED_BY_ME ]\n",
    "\n",
    "tags_not_in_list = None\n",
    "if ( ( tags_not_in_list is not None ) and ( len( tags_not_in_list ) > 0 ) ):\n",
    "\n",
    "    # exclude those in a list\n",
    "    print( \"filtering out articles with tags: \" + str( tags_not_in_list ) )\n",
    "    tdn_article_qs = tdn_article_qs.exclude( tags__name__in = tags_not_in_list )\n",
    "\n",
    "#-- END check to see if we have a specific list of tags we want to exclude --#\n",
    "\n",
    "# include only those with certain tags.\n",
    "#tags_in_list = [ \"prelim_unit_test_001\", \"prelim_unit_test_002\", \"prelim_unit_test_003\", \"prelim_unit_test_004\", \"prelim_unit_test_005\", \"prelim_unit_test_006\", \"prelim_unit_test_007\" ]\n",
    "#tags_in_list = [ \"tdn_month\", ]\n",
    "tags_in_list = None\n",
    "if ( ( tags_in_list is not None ) and ( len( tags_in_list ) > 0 ) ):\n",
    "\n",
    "    # filter\n",
    "    print( \"filtering to just articles with tags: \" + str( tags_in_list ) )\n",
    "    tdn_article_qs = tdn_article_qs.filter( tags__name__in = tags_in_list )\n",
    "    \n",
    "#-- END check to see if we have a specific list of tags we want to include --#\n",
    "\n",
    "# filter out \"*prelim*\" tags?\n",
    "#filter_out_prelim_tags = True\n",
    "if ( filter_out_prelim_tags == True ):\n",
    "\n",
    "    # ifilter out all articles with any tag whose name contains \"prelim\".\n",
    "    print( \"filtering out articles with tags that contain \\\"prelim\\\"\" )\n",
    "    tdn_article_qs = tdn_article_qs.exclude( tags__name__icontains = \"prelim\" )\n",
    "    \n",
    "#-- END check to see if we filter out \"prelim_*\" tags --#\n",
    "\n",
    "# how many is that?\n",
    "article_count = tdn_article_qs.count()\n",
    "\n",
    "print( \"Article count after tag filtering: \" + str( article_count ) )\n",
    "\n",
    "# do we want a random sample?\n",
    "if ( random_count > 0 ):\n",
    "\n",
    "    # to get random, order them by \"?\", then use slicing to retrieve requested\n",
    "    #     number.\n",
    "    tdn_article_qs = tdn_article_qs.order_by( \"?\" )[ : random_count ]\n",
    "    \n",
    "#-- END check to see if we want random sample --#\n",
    "\n",
    "# this is a nice algorithm, also:\n",
    "# - http://www.titov.net/2005/09/21/do-not-use-order-by-rand-or-how-to-get-random-rows-from-table/\n",
    "\n",
    "# make ID list, tag articles if configured to.\n",
    "article_id_list = []\n",
    "article_counter = 0\n",
    "for current_article in tdn_article_qs:\n",
    "\n",
    "    # increment article_counter\n",
    "    article_counter += 1\n",
    "\n",
    "    # add IDs to article_id_list\n",
    "    article_id_list.append( str( current_article.id ) )\n",
    "    \n",
    "    # apply a tag while we are at it?\n",
    "    if ( ( do_apply_tag == True ) and ( tag_to_apply is not None ) and ( tag_to_apply != \"\" ) ):\n",
    "    \n",
    "        # yes, please.  Add tag.\n",
    "        current_article.tags.add( tag_to_apply )\n",
    "        \n",
    "    #-- END check to see if we apply tag. --#\n",
    "\n",
    "    # output the tags.\n",
    "    if ( debug_flag == True ):\n",
    "        print( \"- Tags for article \" + str( current_article.id ) + \" : \" + str( current_article.tags.all() ) )\n",
    "    #-- END DEBUG --#\n",
    "\n",
    "#-- END loop over articles --#\n",
    "\n",
    "# output the list.\n",
    "print( \"tdn_article_qs count: {}\".format( tdn_article_qs.count() ) )\n",
    "print( \"Found \" + str( article_counter ) + \" articles ( \" + str( article_count ) + \" ).\" )\n",
    "if ( debug_flag == True ):\n",
    "    print( \"List of \" + str( len( article_id_list ) ) + \" local TDN staff article IDs: \" + \", \".join( article_id_list ) )\n",
    "#-- END DEBUG --#\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5adae2ed-56f9-4728-b46f-3492056c38c0",
   "metadata": {},
   "source": [
    "# Update data and write unit tests"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae6c70d0-2d10-4125-bbda-a65bb49f51ec",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Add tags to `Article_Subject` instances\n",
    "\n",
    "Add the following tags to the following `Article_Subject` instances:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3428f8c-1eb2-4fb3-9790-3ed0aee4e321",
   "metadata": {},
   "source": [
    "### tag `from_press_release`\n",
    "\n",
    "- Tag `from_press_release` added to the following `Article_Subject` instances:\n",
    "\n",
    "    - 637 - Mark Meadows (person 224)\n",
    "    - 677 - Gary Nelund (person 261)\n",
    "    - 740 - Jennifer Granholm (person 102)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc031ec9-4750-4ea2-b1f8-8f4364dc6793",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add tag `from_press_release` to:\n",
    "# - 637 - Mark Meadows (person 224)\n",
    "# - 677 - Gary Nelund (person 261)\n",
    "# - 740 - Jennifer Granholm (person 102)\n",
    "\n",
    "# declare variables\n",
    "tag_to_assign = None\n",
    "article_subject_id_list = None\n",
    "article_subject_id = None\n",
    "article_subject = None\n",
    "\n",
    "# what tag?\n",
    "tag_to_assign = \"from_press_release\"\n",
    "\n",
    "# build list of Article_Subject instances to process\n",
    "article_subject_id_list = list()\n",
    "article_subject_id_list.append( 637 )\n",
    "article_subject_id_list.append( 677 )\n",
    "article_subject_id_list.append( 740 )\n",
    "\n",
    "# loop over Article_Subject instances\n",
    "for article_subject_id in article_subject_id_list:\n",
    "    \n",
    "    # retrieve instance\n",
    "    article_subject = Article_Subject.objects.get( id = article_subject_id )\n",
    "    \n",
    "    # add tag.\n",
    "    article_subject.tags.add( tag_to_assign )\n",
    "    \n",
    "#-- END loop over Article_Subject instances. --#\n",
    "\n",
    "status_message = \"tag {tag_to_assign} added to Article_Subject IDs: {article_subject_id_list} at {timestamp_now}\".format(\n",
    "    tag_to_assign = tag_to_assign,\n",
    "    article_subject_id_list = article_subject_id_list,\n",
    "    timestamp_now = datetime.datetime.now()\n",
    ")\n",
    "print( status_message )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09c7d6ce-dcff-48d5-b8d2-e2dc3b0787d4",
   "metadata": {
    "tags": []
   },
   "source": [
    "### tag `godwin_heights`\n",
    "\n",
    "- Tag `godwin_heights` added to the following `Article_Subject` instances:\n",
    "\n",
    "    - 621 - Hornecker, Kenneth (person 189)\n",
    "    - 622 - Johnston, Allen E. (person 187)\n",
    "    - 623 - Felske, Jon (person 188)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0ef9dc5-d335-415d-8153-9515034204b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add tag `godwin_heights` to:\n",
    "# - 621 - Hornecker, Kenneth (person 189)\n",
    "# - 622 - Johnston, Allen E. (person 187)\n",
    "# - 623 - Felske, Jon (person 188)\n",
    "\n",
    "# declare variables\n",
    "tag_to_assign = None\n",
    "article_subject_id_list = None\n",
    "article_subject_id = None\n",
    "article_subject = None\n",
    "\n",
    "# what tag?\n",
    "tag_to_assign = \"godwin_heights\"\n",
    "\n",
    "# build list of Article_Subject instances to process\n",
    "article_subject_id_list = list()\n",
    "article_subject_id_list.append( 621 )\n",
    "article_subject_id_list.append( 622 )\n",
    "article_subject_id_list.append( 623 )\n",
    "\n",
    "# loop over Article_Subject instances\n",
    "for article_subject_id in article_subject_id_list:\n",
    "    \n",
    "    # retrieve instance\n",
    "    article_subject = Article_Subject.objects.get( id = article_subject_id )\n",
    "    \n",
    "    # add tag.\n",
    "    article_subject.tags.add( tag_to_assign )\n",
    "    \n",
    "#-- END loop over Article_Subject instances. --#\n",
    "\n",
    "status_message = \"tag {tag_to_assign} added to Article_Subject IDs: {article_subject_id_list} at {timestamp_now}\".format(\n",
    "    tag_to_assign = tag_to_assign,\n",
    "    article_subject_id_list = article_subject_id_list,\n",
    "    timestamp_now = datetime.datetime.now()\n",
    ")\n",
    "print( status_message )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "251df8b4-df13-40f6-9924-baa114f03245",
   "metadata": {},
   "source": [
    "# Unit tests\n",
    "\n",
    "To create needed data, run each of the following test network data output specs twice, once with `network_include_render_details` = \"yes\", and once with `network_include_render_details` = \"no\".\n",
    "\n",
    "- _Note: only pass True to `network_outputter.process_network_output_request( debug_flag_IN )` if you really need to debug - it adds garbage data at the end of the output, even if you ask for no render details._"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "346e071c-9c3c-4602-9651-ead636940011",
   "metadata": {},
   "source": [
    "## create network data from \"export\" unit test data - GRP, all names\n",
    "\n",
    "- See [`context_text` github README](https://github.com/jonathanmorgan/context_text#test-data) for more details on loading this data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87d0df77-40e8-4730-abd7-c35d641cbd01",
   "metadata": {},
   "outputs": [],
   "source": [
    "#include_single_word_names = \"yes\"\n",
    "include_single_word_names = \"yes\"\n",
    "\n",
    "request_json_string = \"\"\"{\n",
    "  \"coders\": \"7\",\n",
    "  \"end_date\": \"2010-02-13\",\n",
    "  \"tags_list\": \"\",\n",
    "  \"date_range\": \"\",\n",
    "  \"start_date\": \"2009-12-07\",\n",
    "  \"output_type\": \"tab_delimited_matrix\",\n",
    "  \"publications\": \"1\",\n",
    "  \"network_label\": \"all_names\",\n",
    "  \"person_coders\": \"7\",\n",
    "  \"database_output\": \"yes\",\n",
    "  \"person_end_date\": \"2010-02-13\",\n",
    "  \"person_tag_list\": \"\",\n",
    "  \"coder_types_list\": \"OpenCalais_REST_API_v2\",\n",
    "  \"person_date_range\": \"\",\n",
    "  \"person_query_type\": \"custom\",\n",
    "  \"person_start_date\": \"2009-12-07\",\n",
    "  \"unique_identifiers\": \"\",\n",
    "  \"person_publications\": \"1\",\n",
    "  \"coder_id_priority_list\": \"\",\n",
    "  \"coder_type_filter_type\": \"automated\",\n",
    "  \"network_include_headers\": \"no\",\n",
    "  \"person_coder_types_list\": \"OpenCalais_REST_API_v2\",\n",
    "  \"allow_duplicate_articles\": \"no\",\n",
    "  \"network_data_output_type\": \"net_and_attr_cols\",\n",
    "  \"network_download_as_file\": \"no\",\n",
    "  \"person_unique_identifiers\": \"\",\n",
    "  \"include_source_contact_types\": [\n",
    "    \"direct\",\n",
    "    \"event\",\n",
    "    \"past_quotes\",\n",
    "    \"document\",\n",
    "    \"other\"\n",
    "  ],\n",
    "  \"person_coder_id_priority_list\": \"\",\n",
    "  \"person_coder_type_filter_type\": \"automated\",\n",
    "  \"network_include_render_details\": \"no\",\n",
    "  \"person_allow_duplicate_articles\": \"no\",\n",
    "  \"exclude_persons_with_tags_in_list\": \"\",\n",
    "  \"include_persons_with_single_word_name\": \"yes\"\n",
    "}\"\"\"\n",
    "request_json = json.loads( request_json_string )\n",
    "print( request_json ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1170215-2418-4c03-a566-f3185cda0f79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create network data with render details.\n",
    "request_json[ ContextTextBase.PARAM_NETWORK_INCLUDE_RENDER_DETAILS ] = ContextTextBase.CHOICE_YES\n",
    "network_outputter = NetworkOutput()\n",
    "network_data = network_outputter.process_network_output_request(\n",
    "    params_IN = request_json,\n",
    "    debug_flag_IN = False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f34167d-7da2-4397-bad8-6f82848402ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create network data without render details.\n",
    "request_json[ ContextTextBase.PARAM_NETWORK_INCLUDE_RENDER_DETAILS ] = ContextTextBase.CHOICE_NO\n",
    "network_outputter = NetworkOutput()\n",
    "network_data = network_outputter.process_network_output_request(\n",
    "    params_IN = request_json,\n",
    "    debug_flag_IN = False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d5249be-b211-427e-9ee3-a17ef2ac742f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a hash of the data, for comparison\n",
    "network_data_hash = make_string_hash( network_data )\n",
    "print( \"Network data hash: {}\".format( network_data_hash ) )\n",
    "\n",
    "# match?\n",
    "should_be = \"f879560cab27653185bb4e42baec40b6a5d685b4143388e55041399acb921c5f\"\n",
    "if ( network_data_hash != should_be ):\n",
    "    \n",
    "    # not right hash. Error.\n",
    "    print( \"ERROR! network data hash is {}, should be {}\".format( network_data_hash, should_be ) )\n",
    "    \n",
    "else:\n",
    "    \n",
    "    # a match\n",
    "    print( \"MATCH - network data hash {} matches expected. hooray!\".format( network_data_hash ) )\n",
    "    \n",
    "#-- END debug/test --#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dd2676e-bd88-4f3a-8f56-7657cba1a82f",
   "metadata": {},
   "outputs": [],
   "source": [
    "network_data_length = len( network_data )\n",
    "should_be = 14121\n",
    "print( \"Network data length: {}\".format( network_data_length ) )\n",
    "if ( network_data_length != should_be ):\n",
    "    \n",
    "    # not right length. Error.\n",
    "    print( \"ERROR! network data length is {}, should be {}\".format( network_data_length, should_be ) )\n",
    "    \n",
    "else:\n",
    "    \n",
    "    # a match\n",
    "    print( \"MATCH - string len()gth of {} matches expected. hooray!\".format( network_data_length ) )\n",
    "    \n",
    "#-- END debug/test --#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff432a18-6836-4c64-bb01-39b9de4d3df8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# look at master person dict\n",
    "master_person_dict = network_outputter.create_person_dict( load_person_IN = True )\n",
    "\n",
    "# how many entries?\n",
    "person_count = len( master_person_dict )\n",
    "print( \"- person count: {person_count}\".format( person_count = person_count ) )\n",
    "\n",
    "# right number?\n",
    "should_be = 74\n",
    "if ( person_count != should_be ):\n",
    "    \n",
    "    # not right length. Error.\n",
    "    print( \"ERROR! person count is {}, should be {}\".format( person_count, should_be ) )\n",
    "    \n",
    "else:\n",
    "    \n",
    "    # a match\n",
    "    print( \"MATCH - person count of {} matches expected. hooray!\".format( person_count ) )\n",
    "    \n",
    "#-- END debug/test --#\n",
    "\n",
    "# persons 1049, 752 should be present.\n",
    "find_person_id = 1049\n",
    "if ( find_person_id in master_person_dict ):\n",
    "    \n",
    "    print( \"SUCCESS - single-name person {} is in dictionary\".format( find_person_id ) )\n",
    "    \n",
    "else:\n",
    "    \n",
    "    print( \"ERROR - single-name person {} not in dictionary\".format( find_person_id ) )\n",
    "    \n",
    "#-- END check for person 1049 --#\n",
    "\n",
    "find_person_id = 752\n",
    "if ( find_person_id in master_person_dict ):\n",
    "    \n",
    "    print( \"SUCCESS - single-name person {} is in dictionary\".format( find_person_id ) )\n",
    "    \n",
    "else:\n",
    "    \n",
    "    print( \"ERROR - single-name person {} not in dictionary\".format( find_person_id ) )\n",
    "    \n",
    "#-- END check for person 752 --#\n",
    "\n",
    "# output all persons.\n",
    "for person_id, person_instance in master_person_dict.items():\n",
    "    \n",
    "    print( \"\\n==> Person {person_id}: {person_instance}\".format( person_id = person_id, person_instance = person_instance ) )\n",
    "    \n",
    "#-- END loop over persons --#"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80fce7af-1e1b-4725-9b2c-e4662aa9a708",
   "metadata": {},
   "source": [
    "## network data from \"export\" unit test data - no single names\n",
    "\n",
    "- See [`context_text` github README](https://github.com/jonathanmorgan/context_text#test-data) for more details on loading this data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a41f8e3c-cb61-4da6-9006-a21caf91a5a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#include_single_word_names = \"yes\"\n",
    "include_single_word_names = \"no\"\n",
    "\n",
    "request_json_string = \"\"\"{\n",
    "  \"coders\": \"7\",\n",
    "  \"end_date\": \"2010-02-13\",\n",
    "  \"tags_list\": \"\",\n",
    "  \"date_range\": \"\",\n",
    "  \"start_date\": \"2009-12-07\",\n",
    "  \"output_type\": \"tab_delimited_matrix\",\n",
    "  \"publications\": \"1\",\n",
    "  \"network_label\": \"no_single_names\",\n",
    "  \"person_coders\": \"7\",\n",
    "  \"database_output\": \"yes\",\n",
    "  \"person_end_date\": \"2010-02-13\",\n",
    "  \"person_tag_list\": \"\",\n",
    "  \"coder_types_list\": \"OpenCalais_REST_API_v2\",\n",
    "  \"person_date_range\": \"\",\n",
    "  \"person_query_type\": \"custom\",\n",
    "  \"person_start_date\": \"2009-12-07\",\n",
    "  \"unique_identifiers\": \"\",\n",
    "  \"person_publications\": \"1\",\n",
    "  \"coder_id_priority_list\": \"\",\n",
    "  \"coder_type_filter_type\": \"automated\",\n",
    "  \"network_include_headers\": \"no\",\n",
    "  \"person_coder_types_list\": \"OpenCalais_REST_API_v2\",\n",
    "  \"allow_duplicate_articles\": \"no\",\n",
    "  \"network_data_output_type\": \"net_and_attr_cols\",\n",
    "  \"network_download_as_file\": \"no\",\n",
    "  \"person_unique_identifiers\": \"\",\n",
    "  \"include_source_contact_types\": \"direct,event,past_quotes,document,other\",\n",
    "  \"person_coder_id_priority_list\": \"\",\n",
    "  \"person_coder_type_filter_type\": \"automated\",\n",
    "  \"network_include_render_details\": \"no\",\n",
    "  \"person_allow_duplicate_articles\": \"no\",\n",
    "  \"exclude_persons_with_tags_in_list\": \"\",\n",
    "  \"include_persons_with_single_word_name\": \"no\"\n",
    "}\"\"\"\n",
    "request_json = json.loads( request_json_string )\n",
    "print( request_json ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6523946d-316c-4866-865d-7f840ed0ef8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create network data with render details.\n",
    "request_json[ ContextTextBase.PARAM_NETWORK_INCLUDE_RENDER_DETAILS ] = ContextTextBase.CHOICE_YES\n",
    "network_outputter = NetworkOutput()\n",
    "network_data = network_outputter.process_network_output_request(\n",
    "    params_IN = request_json,\n",
    "    debug_flag_IN = False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ec559df-ab73-4c58-a95d-466d170bb9e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create network data without render details.\n",
    "request_json[ ContextTextBase.PARAM_NETWORK_INCLUDE_RENDER_DETAILS ] = ContextTextBase.CHOICE_NO\n",
    "network_outputter = NetworkOutput()\n",
    "network_data = network_outputter.process_network_output_request(\n",
    "    params_IN = request_json,\n",
    "    debug_flag_IN = False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffea1587-e6da-4b73-8b65-73c700703e16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a hash of the data, for comparison\n",
    "network_data_hash = make_string_hash( network_data )\n",
    "print( \"Network data hash: {}\".format( network_data_hash ) )\n",
    "\n",
    "# match?\n",
    "should_be = \"f85a48630c029f848bbb815d003b188eff38346b8eac0da2d55b7b224b323ac5\"\n",
    "if ( network_data_hash != should_be ):\n",
    "    \n",
    "    # not right hash. Error.\n",
    "    print( \"ERROR! network data hash is {}, should be {}\".format( network_data_hash, should_be ) )\n",
    "    \n",
    "else:\n",
    "    \n",
    "    # a match\n",
    "    print( \"MATCH - network data hash {} matches expected. hooray!\".format( network_data_hash ) )\n",
    "    \n",
    "#-- END debug/test --#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c335fcb7-2682-4d7c-9d4e-2030625a4780",
   "metadata": {},
   "outputs": [],
   "source": [
    "network_data_length = len( network_data )\n",
    "should_be = 13448\n",
    "print( \"Network data length: {}\".format( network_data_length ) )\n",
    "if ( network_data_length != should_be ):\n",
    "    \n",
    "    # not right length. Error.\n",
    "    print( \"ERROR! network data length is {}, should be {}\".format( network_data_length, should_be ) )\n",
    "    \n",
    "else:\n",
    "    \n",
    "    # a match\n",
    "    print( \"MATCH - string len()gth of {} matches expected. hooray!\".format( network_data_length ) )\n",
    "    \n",
    "#-- END debug/test --#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36300648-96fb-4f69-8f86-2e57d8ab1091",
   "metadata": {},
   "outputs": [],
   "source": [
    "# look at master person dict\n",
    "master_person_dict = network_outputter.create_person_dict( load_person_IN = True )\n",
    "\n",
    "# how many entries?\n",
    "person_count = len( master_person_dict )\n",
    "print( \"- person count: {person_count}\".format( person_count = person_count ) )\n",
    "\n",
    "# right number?\n",
    "should_be = 72\n",
    "if ( person_count != should_be ):\n",
    "    \n",
    "    # not right length. Error.\n",
    "    print( \"ERROR! person count is {}, should be {}\".format( person_count, should_be ) )\n",
    "    \n",
    "else:\n",
    "    \n",
    "    # a match\n",
    "    print( \"MATCH - person count of {} matches expected. hooray!\".format( person_count ) )\n",
    "    \n",
    "#-- END debug/test --#\n",
    "\n",
    "# persons 1049, 752 should not be present.\n",
    "find_person_list = list()\n",
    "find_person_list.append( 1049 )\n",
    "find_person_list.append( 752 )\n",
    "for find_person_id in find_person_list:\n",
    "\n",
    "    if ( find_person_id in master_person_dict ):\n",
    "    \n",
    "        print( \"ERROR - single-name person {} is in dictionary\".format( find_person_id ) )\n",
    "    \n",
    "    else:\n",
    "    \n",
    "        print( \"SUCCESS - single-name person {} not in dictionary\".format( find_person_id ) )\n",
    "    \n",
    "    #-- END check for person --#\n",
    "\n",
    "#-- END loop over persons to find. --#\n",
    "\n",
    "# output all persons.\n",
    "for person_id, person_instance in master_person_dict.items():\n",
    "    \n",
    "    print( \"\\n==> Person {person_id}: {person_instance}\".format( person_id = person_id, person_instance = person_instance ) )\n",
    "    \n",
    "#-- END loop over persons --#"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3014b36e-8f0a-428c-bafc-6c9f097ad899",
   "metadata": {},
   "source": [
    "## network data from \"export\" unit test data - exclude tag `from_press_release`\n",
    "\n",
    "- See [`context_text` github README](https://github.com/jonathanmorgan/context_text#test-data) for more details on loading this data.\n",
    "\n",
    "Tag `from_press_release` added to the following `Article_Subject` instances:\n",
    "\n",
    "- 740 - granholm (person 102)\n",
    "- 637 - Mark Meadows (person 224)\n",
    "- 677 - Gary Nelund (person 261)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b63d7bd-cb85-4151-a295-4ca6e599bcc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#include_single_word_names = \"yes\"\n",
    "include_single_word_names = \"no\"\n",
    "\n",
    "request_json_string = \"\"\"{\n",
    "  \"coders\": \"7\",\n",
    "  \"end_date\": \"2010-02-13\",\n",
    "  \"tags_list\": \"\",\n",
    "  \"date_range\": \"\",\n",
    "  \"start_date\": \"2009-12-07\",\n",
    "  \"output_type\": \"tab_delimited_matrix\",\n",
    "  \"publications\": \"1\",\n",
    "  \"network_label\": \"exclude_from_press_release\",\n",
    "  \"person_coders\": \"7\",\n",
    "  \"database_output\": \"yes\",\n",
    "  \"person_end_date\": \"2010-02-13\",\n",
    "  \"person_tag_list\": \"\",\n",
    "  \"coder_types_list\": \"OpenCalais_REST_API_v2\",\n",
    "  \"person_date_range\": \"\",\n",
    "  \"person_query_type\": \"custom\",\n",
    "  \"person_start_date\": \"2009-12-07\",\n",
    "  \"unique_identifiers\": \"\",\n",
    "  \"person_publications\": \"1\",\n",
    "  \"coder_id_priority_list\": \"\",\n",
    "  \"coder_type_filter_type\": \"automated\",\n",
    "  \"network_include_headers\": \"no\",\n",
    "  \"person_coder_types_list\": \"OpenCalais_REST_API_v2\",\n",
    "  \"allow_duplicate_articles\": \"no\",\n",
    "  \"network_data_output_type\": \"net_and_attr_cols\",\n",
    "  \"network_download_as_file\": \"no\",\n",
    "  \"person_unique_identifiers\": \"\",\n",
    "  \"include_source_contact_types\": \"direct,event,past_quotes,document,other\",\n",
    "  \"person_coder_id_priority_list\": \"\",\n",
    "  \"person_coder_type_filter_type\": \"automated\",\n",
    "  \"network_include_render_details\": \"no\",\n",
    "  \"person_allow_duplicate_articles\": \"no\",\n",
    "  \"exclude_persons_with_tags_in_list\": \"from_press_release\",\n",
    "  \"include_persons_with_single_word_name\": \"yes\"\n",
    "}\"\"\"\n",
    "request_json = json.loads( request_json_string )\n",
    "print( request_json ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b64b6e4e-2c30-425b-8936-0be885640cd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create network data with render details.\n",
    "request_json[ ContextTextBase.PARAM_NETWORK_INCLUDE_RENDER_DETAILS ] = ContextTextBase.CHOICE_YES\n",
    "network_outputter = NetworkOutput()\n",
    "network_data = network_outputter.process_network_output_request(\n",
    "    params_IN = request_json,\n",
    "    debug_flag_IN = False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1206ed85-a0ab-4ee2-a2a2-1937e1c42d58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create network data without render details.\n",
    "request_json[ ContextTextBase.PARAM_NETWORK_INCLUDE_RENDER_DETAILS ] = ContextTextBase.CHOICE_NO\n",
    "network_outputter = NetworkOutput()\n",
    "network_data = network_outputter.process_network_output_request(\n",
    "    params_IN = request_json,\n",
    "    debug_flag_IN = False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "657c2621-344b-4b69-a66f-94ad34df2faf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a hash of the data, for comparison\n",
    "network_data_hash = make_string_hash( network_data )\n",
    "print( \"Network data hash: {}\".format( network_data_hash ) )\n",
    "\n",
    "# match?\n",
    "should_be = \"3529e49830a8464cc0d8a497345b56404c73b867b1046fb38df346953a9b3b72\"\n",
    "if ( network_data_hash != should_be ):\n",
    "    \n",
    "    # not right hash. Error.\n",
    "    print( \"ERROR! network data hash is {}, should be {}\".format( network_data_hash, should_be ) )\n",
    "    \n",
    "else:\n",
    "    \n",
    "    # a match\n",
    "    print( \"MATCH - network data hash {} matches expected. hooray!\".format( network_data_hash ) )\n",
    "    \n",
    "#-- END debug/test --#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5d43d82-5784-4dc8-bb04-4b6317c99169",
   "metadata": {},
   "outputs": [],
   "source": [
    "network_data_length = len( network_data )\n",
    "should_be = 13122\n",
    "print( \"Network data length: {}\".format( network_data_length ) )\n",
    "if ( network_data_length != should_be ):\n",
    "    \n",
    "    # not right length. Error.\n",
    "    print( \"ERROR! network data length is {}, should be {}\".format( network_data_length, should_be ) )\n",
    "    \n",
    "else:\n",
    "    \n",
    "    # a match\n",
    "    print( \"MATCH - string len()gth of {} matches expected. hooray!\".format( network_data_length ) )\n",
    "    \n",
    "#-- END debug/test --#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df182418-6eef-4b12-bf62-26a7098ed899",
   "metadata": {},
   "outputs": [],
   "source": [
    "# look at master person dict\n",
    "master_person_dict = network_outputter.create_person_dict( load_person_IN = True )\n",
    "\n",
    "# how many entries?\n",
    "person_count = len( master_person_dict )\n",
    "print( \"- person count: {person_count}\".format( person_count = person_count ) )\n",
    "\n",
    "# right number?\n",
    "should_be = 71\n",
    "if ( person_count != should_be ):\n",
    "    \n",
    "    # not right length. Error.\n",
    "    print( \"ERROR! person count is {}, should be {}\".format( person_count, should_be ) )\n",
    "    \n",
    "else:\n",
    "    \n",
    "    # a match\n",
    "    print( \"MATCH - person count of {} matches expected. hooray!\".format( person_count ) )\n",
    "    \n",
    "#-- END debug/test --#\n",
    "\n",
    "# persons 102, 224, 261 should not be present.\n",
    "find_person_list = list()\n",
    "find_person_list.append( 102 )\n",
    "find_person_list.append( 224 )\n",
    "find_person_list.append( 261 )\n",
    "for find_person_id in find_person_list:\n",
    "\n",
    "    if ( find_person_id in master_person_dict ):\n",
    "    \n",
    "        print( \"ERROR - single-name person {} is in dictionary\".format( find_person_id ) )\n",
    "    \n",
    "    else:\n",
    "    \n",
    "        print( \"SUCCESS - single-name person {} not in dictionary\".format( find_person_id ) )\n",
    "    \n",
    "    #-- END check for person --#\n",
    "\n",
    "#-- END loop over persons to find. --#\n",
    "\n",
    "# output all persons.\n",
    "for person_id, person_instance in master_person_dict.items():\n",
    "    \n",
    "    print( \"\\n==> Person {person_id}: {person_instance}\".format( person_id = person_id, person_instance = person_instance ) )\n",
    "    \n",
    "#-- END loop over persons --#"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c00aaf32-9a56-4405-b451-d6d2ae792980",
   "metadata": {
    "tags": []
   },
   "source": [
    "## network data from \"export\" unit test data - exclude tag `godwin_heights`\n",
    "\n",
    "- See [`context_text` github README](https://github.com/jonathanmorgan/context_text#test-data) for more details on loading this data.\n",
    "\n",
    "Tag `godwin_heights` added to the following `Article_Subject` instances:\n",
    "\n",
    "- 623 - Felske, Jon (person 188)\n",
    "- 622 - Johnston, Allen E. (person 187)\n",
    "- 621 - Hornecker, Kenneth (person 189)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80bf2adb-b6a3-40ed-8352-e386135c98dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#include_single_word_names = \"yes\"\n",
    "include_single_word_names = \"no\"\n",
    "\n",
    "request_json_string = \"\"\"{\n",
    "  \"coders\": \"7\",\n",
    "  \"end_date\": \"2010-02-13\",\n",
    "  \"tags_list\": \"\",\n",
    "  \"date_range\": \"\",\n",
    "  \"start_date\": \"2009-12-07\",\n",
    "  \"output_type\": \"tab_delimited_matrix\",\n",
    "  \"publications\": \"1\",\n",
    "  \"network_label\": \"exclude_godwin_heights\",\n",
    "  \"person_coders\": \"7\",\n",
    "  \"database_output\": \"yes\",\n",
    "  \"person_end_date\": \"2010-02-13\",\n",
    "  \"person_tag_list\": \"\",\n",
    "  \"coder_types_list\": \"OpenCalais_REST_API_v2\",\n",
    "  \"person_date_range\": \"\",\n",
    "  \"person_query_type\": \"custom\",\n",
    "  \"person_start_date\": \"2009-12-07\",\n",
    "  \"unique_identifiers\": \"\",\n",
    "  \"person_publications\": \"1\",\n",
    "  \"coder_id_priority_list\": \"\",\n",
    "  \"coder_type_filter_type\": \"automated\",\n",
    "  \"network_include_headers\": \"no\",\n",
    "  \"person_coder_types_list\": \"OpenCalais_REST_API_v2\",\n",
    "  \"allow_duplicate_articles\": \"no\",\n",
    "  \"network_data_output_type\": \"net_and_attr_cols\",\n",
    "  \"network_download_as_file\": \"no\",\n",
    "  \"person_unique_identifiers\": \"\",\n",
    "  \"include_source_contact_types\": \"direct,event,past_quotes,document,other\",\n",
    "  \"person_coder_id_priority_list\": \"\",\n",
    "  \"person_coder_type_filter_type\": \"automated\",\n",
    "  \"network_include_render_details\": \"no\",\n",
    "  \"person_allow_duplicate_articles\": \"no\",\n",
    "  \"exclude_persons_with_tags_in_list\": \"godwin_heights\",\n",
    "  \"include_persons_with_single_word_name\": \"yes\"\n",
    "}\"\"\"\n",
    "request_json = json.loads( request_json_string )\n",
    "print( request_json ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "652819e8-6a17-41ca-8f0f-587ea1c43173",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create network data with render details.\n",
    "request_json[ ContextTextBase.PARAM_NETWORK_INCLUDE_RENDER_DETAILS ] = ContextTextBase.CHOICE_YES\n",
    "network_outputter = NetworkOutput()\n",
    "network_data = network_outputter.process_network_output_request(\n",
    "    params_IN = request_json,\n",
    "    debug_flag_IN = False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bab7bf2f-0389-4de8-b624-17a8fd158f61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create network data without render details.\n",
    "request_json[ ContextTextBase.PARAM_NETWORK_INCLUDE_RENDER_DETAILS ] = ContextTextBase.CHOICE_NO\n",
    "network_outputter = NetworkOutput()\n",
    "network_data = network_outputter.process_network_output_request(\n",
    "    params_IN = request_json,\n",
    "    debug_flag_IN = False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd7216cb-fd1a-41ba-8377-bc2f396cc038",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a hash of the data, for comparison\n",
    "network_data_hash = make_string_hash( network_data )\n",
    "print( \"Network data hash: {}\".format( network_data_hash ) )\n",
    "\n",
    "# match?\n",
    "should_be = \"59e1b6ba6aab28cf37fcb45877d8cdd86d8593df9fa506352d0abd1b6fd3c29b\"\n",
    "if ( network_data_hash != should_be ):\n",
    "    \n",
    "    # not right hash. Error.\n",
    "    print( \"ERROR! network data hash is {}, should be {}\".format( network_data_hash, should_be ) )\n",
    "    \n",
    "else:\n",
    "    \n",
    "    # a match\n",
    "    print( \"MATCH - network data hash {} matches expected. hooray!\".format( network_data_hash ) )\n",
    "    \n",
    "#-- END debug/test --#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0872045b-c733-4f6f-85f0-313e3ee86d6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "network_data_length = len( network_data )\n",
    "should_be = 13122\n",
    "print( \"Network data length: {}\".format( network_data_length ) )\n",
    "if ( network_data_length != should_be ):\n",
    "    \n",
    "    # not right length. Error.\n",
    "    print( \"ERROR! network data length is {}, should be {}\".format( network_data_length, should_be ) )\n",
    "    \n",
    "else:\n",
    "    \n",
    "    # a match\n",
    "    print( \"MATCH - string len()gth of {} matches expected. hooray!\".format( network_data_length ) )\n",
    "    \n",
    "#-- END debug/test --#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f075f5ec-cbae-4366-b4a7-2d859c63e9a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# look at master person dict\n",
    "master_person_dict = network_outputter.create_person_dict( load_person_IN = True )\n",
    "\n",
    "# how many entries?\n",
    "person_count = len( master_person_dict )\n",
    "print( \"- person count: {person_count}\".format( person_count = person_count ) )\n",
    "\n",
    "# right number?\n",
    "should_be = 71\n",
    "if ( person_count != should_be ):\n",
    "    \n",
    "    # not right length. Error.\n",
    "    print( \"ERROR! person count is {}, should be {}\".format( person_count, should_be ) )\n",
    "    \n",
    "else:\n",
    "    \n",
    "    # a match\n",
    "    print( \"MATCH - person count of {} matches expected. hooray!\".format( person_count ) )\n",
    "    \n",
    "#-- END debug/test --#\n",
    "\n",
    "# persons 187, 188, 189 should not be present.\n",
    "find_person_list = list()\n",
    "find_person_list.append( 187 )\n",
    "find_person_list.append( 188 )\n",
    "find_person_list.append( 189 )\n",
    "for find_person_id in find_person_list:\n",
    "\n",
    "    if ( find_person_id in master_person_dict ):\n",
    "    \n",
    "        print( \"ERROR - single-name person {} is in dictionary\".format( find_person_id ) )\n",
    "    \n",
    "    else:\n",
    "    \n",
    "        print( \"SUCCESS - single-name person {} not in dictionary\".format( find_person_id ) )\n",
    "    \n",
    "    #-- END check for person --#\n",
    "\n",
    "#-- END loop over persons to find. --#\n",
    "\n",
    "# output all persons.\n",
    "for person_id, person_instance in master_person_dict.items():\n",
    "    \n",
    "    print( \"\\n==> Person {person_id}: {person_instance}\".format( person_id = person_id, person_instance = person_instance ) )\n",
    "    \n",
    "#-- END loop over persons --#"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36997b63-24c7-4a02-a082-63bdbc8b726b",
   "metadata": {
    "tags": []
   },
   "source": [
    "## network data from \"export\" unit test data - exclude tags `from_press_release` and `godwin_heights`\n",
    "\n",
    "- See [`context_text` github README](https://github.com/jonathanmorgan/context_text#test-data) for more details on loading this data.\n",
    "\n",
    "Tag `from_press_release` added to the following `Article_Subject` instances:\n",
    "\n",
    "- 740 - granholm (person 102)\n",
    "- 637 - Mark Meadows (person 224)\n",
    "- 677 - Gary Nelund (person 261)\n",
    "\n",
    "Tag `godwin_heights` added to the following `Article_Subject` instances:\n",
    "\n",
    "- 623 - Felske, Jon (person 188)\n",
    "- 622 - Johnston, Allen E. (person 187)\n",
    "- 621 - Hornecker, Kenneth (person 189)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53470108-eebc-4c6a-aff5-98bd117a87de",
   "metadata": {},
   "outputs": [],
   "source": [
    "#include_single_word_names = \"yes\"\n",
    "include_single_word_names = \"no\"\n",
    "\n",
    "request_json_string = \"\"\"{\n",
    "  \"coders\": \"7\",\n",
    "  \"end_date\": \"2010-02-13\",\n",
    "  \"tags_list\": \"\",\n",
    "  \"date_range\": \"\",\n",
    "  \"start_date\": \"2009-12-07\",\n",
    "  \"output_type\": \"tab_delimited_matrix\",\n",
    "  \"publications\": \"1\",\n",
    "  \"network_label\": \"exclude_two_tags\",\n",
    "  \"person_coders\": \"7\",\n",
    "  \"database_output\": \"yes\",\n",
    "  \"person_end_date\": \"2010-02-13\",\n",
    "  \"person_tag_list\": \"\",\n",
    "  \"coder_types_list\": \"OpenCalais_REST_API_v2\",\n",
    "  \"person_date_range\": \"\",\n",
    "  \"person_query_type\": \"custom\",\n",
    "  \"person_start_date\": \"2009-12-07\",\n",
    "  \"unique_identifiers\": \"\",\n",
    "  \"person_publications\": \"1\",\n",
    "  \"coder_id_priority_list\": \"\",\n",
    "  \"coder_type_filter_type\": \"automated\",\n",
    "  \"network_include_headers\": \"no\",\n",
    "  \"person_coder_types_list\": \"OpenCalais_REST_API_v2\",\n",
    "  \"allow_duplicate_articles\": \"no\",\n",
    "  \"network_data_output_type\": \"net_and_attr_cols\",\n",
    "  \"network_download_as_file\": \"no\",\n",
    "  \"person_unique_identifiers\": \"\",\n",
    "  \"include_source_contact_types\": \"direct,event,past_quotes,document,other\",\n",
    "  \"person_coder_id_priority_list\": \"\",\n",
    "  \"person_coder_type_filter_type\": \"automated\",\n",
    "  \"network_include_render_details\": \"no\",\n",
    "  \"person_allow_duplicate_articles\": \"no\",\n",
    "  \"exclude_persons_with_tags_in_list\": \"from_press_release,godwin_heights\",\n",
    "  \"include_persons_with_single_word_name\": \"yes\"\n",
    "}\"\"\"\n",
    "request_json = json.loads( request_json_string )\n",
    "print( request_json ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86564df5-71bc-42e5-a4c0-f88f3c697fe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create network data with render details.\n",
    "request_json[ ContextTextBase.PARAM_NETWORK_INCLUDE_RENDER_DETAILS ] = ContextTextBase.CHOICE_YES\n",
    "network_outputter = NetworkOutput()\n",
    "network_data = network_outputter.process_network_output_request(\n",
    "    params_IN = request_json,\n",
    "    debug_flag_IN = False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b6cb677-8ffd-406f-bcd3-052592e6b6ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create network data without render details.\n",
    "request_json[ ContextTextBase.PARAM_NETWORK_INCLUDE_RENDER_DETAILS ] = ContextTextBase.CHOICE_NO\n",
    "network_outputter = NetworkOutput()\n",
    "network_data = network_outputter.process_network_output_request(\n",
    "    params_IN = request_json,\n",
    "    debug_flag_IN = False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4ab7ae3-7dbb-4a52-8525-187c0151d382",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a hash of the data, for comparison\n",
    "network_data_hash = make_string_hash( network_data )\n",
    "print( \"Network data hash: {}\".format( network_data_hash ) )\n",
    "\n",
    "# match?\n",
    "should_be = \"441127876c15eda7fb6cbf64e8555e011a2f459ba64b7111ac3dd4cbcdafbb2a\"\n",
    "if ( network_data_hash != should_be ):\n",
    "    \n",
    "    # not right hash. Error.\n",
    "    print( \"ERROR! network data hash is {}, should be {}\".format( network_data_hash, should_be ) )\n",
    "    \n",
    "else:\n",
    "    \n",
    "    # a match\n",
    "    print( \"MATCH - network data hash {} matches expected. hooray!\".format( network_data_hash ) )\n",
    "    \n",
    "#-- END debug/test --#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f29d109f-589c-482b-ace8-35f66e77a288",
   "metadata": {},
   "outputs": [],
   "source": [
    "network_data_length = len( network_data )\n",
    "should_be = 12159\n",
    "print( \"Network data length: {}\".format( network_data_length ) )\n",
    "if ( network_data_length != should_be ):\n",
    "    \n",
    "    # not right length. Error.\n",
    "    print( \"ERROR! network data length is {}, should be {}\".format( network_data_length, should_be ) )\n",
    "    \n",
    "else:\n",
    "    \n",
    "    # a match\n",
    "    print( \"MATCH - string len()gth of {} matches expected. hooray!\".format( network_data_length ) )\n",
    "    \n",
    "#-- END debug/test --#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e40ef715-9688-4c70-a450-341d689e1f71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# look at master person dict\n",
    "master_person_dict = network_outputter.create_person_dict( load_person_IN = True )\n",
    "\n",
    "# how many entries?\n",
    "person_count = len( master_person_dict )\n",
    "print( \"- person count: {person_count}\".format( person_count = person_count ) )\n",
    "\n",
    "# right number?\n",
    "should_be = 68\n",
    "if ( person_count != should_be ):\n",
    "    \n",
    "    # not right length. Error.\n",
    "    print( \"ERROR! person count is {}, should be {}\".format( person_count, should_be ) )\n",
    "    \n",
    "else:\n",
    "    \n",
    "    # a match\n",
    "    print( \"MATCH - person count of {} matches expected. hooray!\".format( person_count ) )\n",
    "    \n",
    "#-- END debug/test --#\n",
    "\n",
    "# persons 102, 224, 261, 187, 188, 189 should not be present.\n",
    "find_person_list = list()\n",
    "find_person_list.append( 102 )\n",
    "find_person_list.append( 224 )\n",
    "find_person_list.append( 261 )\n",
    "find_person_list.append( 187 )\n",
    "find_person_list.append( 188 )\n",
    "find_person_list.append( 189 )\n",
    "for find_person_id in find_person_list:\n",
    "\n",
    "    if ( find_person_id in master_person_dict ):\n",
    "    \n",
    "        print( \"ERROR - single-name person {} is in dictionary\".format( find_person_id ) )\n",
    "    \n",
    "    else:\n",
    "    \n",
    "        print( \"SUCCESS - single-name person {} not in dictionary\".format( find_person_id ) )\n",
    "    \n",
    "    #-- END check for person --#\n",
    "\n",
    "#-- END loop over persons to find. --#\n",
    "\n",
    "# output all persons.\n",
    "for person_id, person_instance in master_person_dict.items():\n",
    "    \n",
    "    print( \"\\n==> Person {person_id}: {person_instance}\".format( person_id = person_id, person_instance = person_instance ) )\n",
    "    \n",
    "#-- END loop over persons --#"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a717ae8-b19d-4528-9d54-6c7759d34ac2",
   "metadata": {
    "tags": []
   },
   "source": [
    "## network data from \"export\" unit test data - no single names, exclude tags `from_press_release` and `godwin_heights`\n",
    "\n",
    "- See [`context_text` github README](https://github.com/jonathanmorgan/context_text#test-data) for more details on loading this data.\n",
    "\n",
    "Tag `from_press_release` added to the following `Article_Subject` instances:\n",
    "\n",
    "- 740 - granholm (person 102)\n",
    "- 637 - Mark Meadows (person 224)\n",
    "- 677 - Gary Nelund (person 261)\n",
    "\n",
    "Tag `godwin_heights` added to the following `Article_Subject` instances:\n",
    "\n",
    "- 623 - Felske, Jon (person 188)\n",
    "- 622 - Johnston, Allen E. (person 187)\n",
    "- 621 - Hornecker, Kenneth (person 189)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5525b0df-7ccf-4fc3-9c50-2ab6ef782bdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#include_single_word_names = \"yes\"\n",
    "include_single_word_names = \"no\"\n",
    "\n",
    "request_json_string = \"\"\"{\n",
    "  \"coders\": \"7\",\n",
    "  \"end_date\": \"2010-02-13\",\n",
    "  \"tags_list\": \"\",\n",
    "  \"date_range\": \"\",\n",
    "  \"start_date\": \"2009-12-07\",\n",
    "  \"output_type\": \"tab_delimited_matrix\",\n",
    "  \"publications\": \"1\",\n",
    "  \"network_label\": \"exclude_two_tags_and_single_names\",\n",
    "  \"person_coders\": \"7\",\n",
    "  \"database_output\": \"yes\",\n",
    "  \"person_end_date\": \"2010-02-13\",\n",
    "  \"person_tag_list\": \"\",\n",
    "  \"coder_types_list\": \"OpenCalais_REST_API_v2\",\n",
    "  \"person_date_range\": \"\",\n",
    "  \"person_query_type\": \"custom\",\n",
    "  \"person_start_date\": \"2009-12-07\",\n",
    "  \"unique_identifiers\": \"\",\n",
    "  \"person_publications\": \"1\",\n",
    "  \"coder_id_priority_list\": \"\",\n",
    "  \"coder_type_filter_type\": \"automated\",\n",
    "  \"network_include_headers\": \"no\",\n",
    "  \"person_coder_types_list\": \"OpenCalais_REST_API_v2\",\n",
    "  \"allow_duplicate_articles\": \"no\",\n",
    "  \"network_data_output_type\": \"net_and_attr_cols\",\n",
    "  \"network_download_as_file\": \"no\",\n",
    "  \"person_unique_identifiers\": \"\",\n",
    "  \"include_source_contact_types\": \"direct,event,past_quotes,document,other\",\n",
    "  \"person_coder_id_priority_list\": \"\",\n",
    "  \"person_coder_type_filter_type\": \"automated\",\n",
    "  \"network_include_render_details\": \"no\",\n",
    "  \"person_allow_duplicate_articles\": \"no\",\n",
    "  \"exclude_persons_with_tags_in_list\": \"from_press_release,godwin_heights\",\n",
    "  \"include_persons_with_single_word_name\": \"no\"\n",
    "}\"\"\"\n",
    "request_json = json.loads( request_json_string )\n",
    "print( request_json ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b348e5e-3531-4bc4-b307-17a30835e766",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create network data with render details.\n",
    "request_json[ ContextTextBase.PARAM_NETWORK_INCLUDE_RENDER_DETAILS ] = ContextTextBase.CHOICE_YES\n",
    "network_outputter = NetworkOutput()\n",
    "network_data = network_outputter.process_network_output_request(\n",
    "    params_IN = request_json,\n",
    "    debug_flag_IN = False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ab2c7eb-3473-428c-b561-0e214667689e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create network data without render details.\n",
    "request_json[ ContextTextBase.PARAM_NETWORK_INCLUDE_RENDER_DETAILS ] = ContextTextBase.CHOICE_NO\n",
    "network_outputter = NetworkOutput()\n",
    "network_data = network_outputter.process_network_output_request(\n",
    "    params_IN = request_json,\n",
    "    debug_flag_IN = False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31a66122-b02a-4508-b080-d17c475b0f10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a hash of the data, for comparison\n",
    "network_data_hash = make_string_hash( network_data )\n",
    "print( \"Network data hash: {}\".format( network_data_hash ) )\n",
    "\n",
    "# match?\n",
    "should_be = \"0f8a530f18a724b3d724d7fe9caa3082954c049abdc02b77bc480fc432d0a770\"\n",
    "if ( network_data_hash != should_be ):\n",
    "    \n",
    "    # not right hash. Error.\n",
    "    print( \"ERROR! network data hash is {}, should be {}\".format( network_data_hash, should_be ) )\n",
    "    \n",
    "else:\n",
    "    \n",
    "    # a match\n",
    "    print( \"MATCH - network data hash {} matches expected. hooray!\".format( network_data_hash ) )\n",
    "    \n",
    "#-- END debug/test --#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0b8dfe1-18f0-488f-b7e8-b3f72062ae2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "network_data_length = len( network_data )\n",
    "should_be = 11534\n",
    "print( \"Network data length: {}\".format( network_data_length ) )\n",
    "if ( network_data_length != should_be ):\n",
    "    \n",
    "    # not right length. Error.\n",
    "    print( \"ERROR! network data length is {}, should be {}\".format( network_data_length, should_be ) )\n",
    "    \n",
    "else:\n",
    "    \n",
    "    # a match\n",
    "    print( \"MATCH - string len()gth of {} matches expected. hooray!\".format( network_data_length ) )\n",
    "    \n",
    "#-- END debug/test --#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29210397-7d1a-4a69-b028-9a0ab1260f54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# look at master person dict\n",
    "master_person_dict = network_outputter.create_person_dict( load_person_IN = True )\n",
    "\n",
    "# how many entries?\n",
    "person_count = len( master_person_dict )\n",
    "print( \"- person count: {person_count}\".format( person_count = person_count ) )\n",
    "\n",
    "# right number?\n",
    "should_be = 66\n",
    "if ( person_count != should_be ):\n",
    "    \n",
    "    # not right length. Error.\n",
    "    print( \"ERROR! person count is {}, should be {}\".format( person_count, should_be ) )\n",
    "    \n",
    "else:\n",
    "    \n",
    "    # a match\n",
    "    print( \"MATCH - person count of {} matches expected. hooray!\".format( person_count ) )\n",
    "    \n",
    "#-- END debug/test --#\n",
    "\n",
    "# the following persons should not be present\n",
    "find_person_list = list()\n",
    "\n",
    "# 1049, 752 (single names)\n",
    "find_person_list.append( 1049 )\n",
    "find_person_list.append( 752 )\n",
    "\n",
    "# 102, 224, 261 (tag `from_press_release`)\n",
    "find_person_list.append( 102 )\n",
    "find_person_list.append( 224 )\n",
    "find_person_list.append( 261 )\n",
    "\n",
    "# 187, 188, 189 (tag `godwin_heights`)\n",
    "find_person_list.append( 187 )\n",
    "find_person_list.append( 188 )\n",
    "find_person_list.append( 189 )\n",
    "\n",
    "# check for people who should have been removed.\n",
    "for find_person_id in find_person_list:\n",
    "\n",
    "    if ( find_person_id in master_person_dict ):\n",
    "    \n",
    "        print( \"ERROR - single-name person {} is in dictionary\".format( find_person_id ) )\n",
    "    \n",
    "    else:\n",
    "    \n",
    "        print( \"SUCCESS - single-name person {} not in dictionary\".format( find_person_id ) )\n",
    "    \n",
    "    #-- END check for person --#\n",
    "\n",
    "#-- END loop over persons to find. --#\n",
    "\n",
    "# output all persons.\n",
    "for person_id, person_instance in master_person_dict.items():\n",
    "    \n",
    "    print( \"\\n==> Person {person_id}: {person_instance}\".format( person_id = person_id, person_instance = person_instance ) )\n",
    "    \n",
    "#-- END loop over persons --#"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60969dcb-e318-4767-9dfb-8e7ddaefd366",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Export coded data to new fixtures\n",
    "\n",
    "- Back to [Table of Contents](#Table-of-Contents)\n",
    "\n",
    "Once you have coded articles, you'll want to re-export the coded data to fixtures.\n",
    "\n",
    "Export them to JSON fixture files using manage.py / django-admin dumpdata ( [https://docs.djangoproject.com/en/dev/ref/django-admin/#django-admin-dumpdata](https://docs.djangoproject.com/en/dev/ref/django-admin/#django-admin-dumpdata) ) so they can be imported using python manage.py or django-admin loaddata ( [https://docs.djangoproject.com/en/dev/ref/django-admin/#django-admin-loaddata](https://docs.djangoproject.com/en/dev/ref/django-admin/#django-admin-loaddata) ) rather than having to input them in the admin:\n",
    "\n",
    "    python manage.py dumpdata [app_label[.ModelName] [app_label[.ModelName] ...]] --indent INDENT --output <output_file_path>\n",
    "    \n",
    "    Example: \n",
    "    \n",
    "    python manage.py dumpdata \\\n",
    "        --indent 4 \\\n",
    "        --output context-sourcenet_entities_and_relations.json \\\n",
    "        context.Entity_Identifier_Type \\\n",
    "        context.Entity_Relation_Type \\\n",
    "        context.Entity_Relation_Type_Trait \\\n",
    "        context.Entity_Type \\\n",
    "        context.Entity_Type_Trait \\\n",
    "        context.Trait_Type \\\n",
    "        context.Term \\\n",
    "        context.Term_Relation \\\n",
    "        context.Term_Relation_Type \\\n",
    "        context.Vocabulary \\\n",
    "        \n",
    "    No line breaks:\n",
    "    \n",
    "        python manage.py dumpdata --indent 4 --output context-sourcenet_entities_and_relations.json context.Entity_Identifier_Type context.Entity_Relation_Type context.Entity_Relation_Type_Trait context.Entity_Type context.Entity_Type_Trait context.Trait_Type context.Term context.Term_Relation context.Term_Relation_Type context.Vocabulary\n",
    "\n",
    "The changes we've made here are in three applications: `context_text`, and `taggit`.  To make a new fixture for each:\n",
    "\n",
    "- `python manage.py dumpdata --indent 4 --output context_text_unittest_export_data.json context_text`\n",
    "- `python manage.py dumpdata --indent 4 --output context_text_unittest_export_taggit_data.json taggit`\n",
    "\n",
    "_Note: I included the `article_data_notes` (OpenCalais RAW XML) last time, keeping them in this time, too. If you wanted to leave them out:_\n",
    "\n",
    "- `python manage.py dumpdata --indent 4 --output context_text_unittest_export_data.json --exclude context_text.article_data_notes context_text`\n",
    "\n",
    "These are stored in the `context_text` github repo, in `context_text/fixtures`."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "research_virtualenv",
   "language": "python",
   "name": "research"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
