{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**PhD work log**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# working files\n",
    "\n",
    "- data exploration notebook: [analysis-data_exploration.ipynb](./analysis/analysis-data_exploration.ipynb)\n",
    "- network data creation example notebook: [analysis-network_data_output_example.ipynb](./analysis/analysis-network_data_output_example.ipynb)\n",
    "- article overview notebook: [analysis-article_overview.ipynb](./analysis/analysis-article_overview.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## data prep"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### programmatic network data creation\n",
    "\n",
    "- look at [Network Builder](https://research.local/research/context/text/output/network) - see how hard to make it callable with a JSON dictionary rather than through page.\n",
    "\n",
    "    - Switched all code into NetworkOutput class (`context_text/export/network_output.py --> NetworkOutput`), got it to work either in view function or stand-alone code with JSON parameters.\n",
    "    - Example notebook: [analysis-network_data_output_example.ipynb](./analysis/analysis-network_data_output_example.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### article overview\n",
    "\n",
    "- notebooks:\n",
    "\n",
    "    - article overview notebook: [analysis-article_overview.ipynb](./analysis/analysis-article_overview.ipynb)\n",
    "    - tagging local hard news:\n",
    "\n",
    "        - [article_coding.ipynb](./data/article_coding/article_coding.ipynb)\n",
    "        - [newsbank-article_coding.ipynb](./data/article_coding/newsbank-article_coding.ipynb)\n",
    "\n",
    "- from each paper, date range, and set of tags with counts across each range.\n",
    "- identify what filtering has already been done:\n",
    "\n",
    "    - for each paper, sections included, date range, sets of tags.\n",
    "\n",
    "        - documented in \"tagging local hard news\" notebooks above.\n",
    "\n",
    "- Grand Rapids Press local hard news:\n",
    "\n",
    "    - all articles\n",
    "\n",
    "        - count: 354,315\n",
    "        - tags with counts: \n",
    "\n",
    "            - tag coded-OpenCalaisV2ArticleCoder: 43816\n",
    "            - tag export_to_context-20191126-164206: 43816\n",
    "            - tag grp_month: 441\n",
    "            - tag local_hard_news: 43816\n",
    "            - tag minnesota1-20160328: 147\n",
    "            - tag minnesota2-20160328: 147\n",
    "            - tag minnesota3-20160328: 147\n",
    "            - tag prelim_network: 109\n",
    "            - tag prelim_reliability: 19\n",
    "            - tag prelim_reliability_combined: 60\n",
    "            - tag prelim_reliability_test: 60\n",
    "            - tag prelim_training_001: 40\n",
    "            - tag prelim_training_002: 30\n",
    "            - tag prelim_training_003: 60\n",
    "            - tag prelim_unit_test_001: 10\n",
    "            - tag prelim_unit_test_002: 10\n",
    "            - tag prelim_unit_test_003: 10\n",
    "            - tag prelim_unit_test_004: 10\n",
    "            - tag prelim_unit_test_005: 10\n",
    "            - tag prelim_unit_test_006: 10\n",
    "            - tag prelim_unit_test_007: 10\n",
    "\n",
    "    - local hard news have tag \"`ContextTextBase.TAG_LOCAL_HARD_NEWS`\" --> \"`local_hard_news`\"\n",
    "\n",
    "        - count: 43,816\n",
    "        - earliest date: 2005-01-01\n",
    "        - latest date: 2010-11-30\n",
    "        - tags with counts:\n",
    "\n",
    "            - tag coded-OpenCalaisV2ArticleCoder: 43816\n",
    "            - tag export_to_context-20191126-164206: 43816\n",
    "            - tag grp_month: 441\n",
    "            - tag local_hard_news: 43816\n",
    "            - tag minnesota1-20160328: 147\n",
    "            - tag minnesota2-20160328: 147\n",
    "            - tag minnesota3-20160328: 147\n",
    "            - tag prelim_network: 109\n",
    "            - tag prelim_reliability: 19\n",
    "            - tag prelim_reliability_combined: 60\n",
    "            - tag prelim_reliability_test: 60\n",
    "            - tag prelim_training_001: 40\n",
    "            - tag prelim_training_002: 30\n",
    "            - tag prelim_training_003: 60\n",
    "            - tag prelim_unit_test_001: 10\n",
    "            - tag prelim_unit_test_002: 10\n",
    "            - tag prelim_unit_test_003: 10\n",
    "            - tag prelim_unit_test_004: 10\n",
    "            - tag prelim_unit_test_005: 10\n",
    "            - tag prelim_unit_test_006: 10\n",
    "            - tag prelim_unit_test_007: 10        \n",
    "\n",
    "- Detroit News\n",
    "\n",
    "    - all articles:\n",
    "    \n",
    "        - count: 159,716\n",
    "        - earliest date: 2005-01-01\n",
    "        - latest date: 2010-10-31\n",
    "        - tags with counts:\n",
    "\n",
    "            - tag coded-OpenCalaisV2ArticleCoder: 27\n",
    "            - tag export_to_context-20191126-164206: 27\n",
    "            - tag minnesota1-20160409: 27\n",
    "            - tag minnesota2-20160409: 27\n",
    "            - tag minnesota3-20160409: 27\n",
    "            - tag prelim_reliability: 27\n",
    "            - tag prelim_reliability_combined: 27\n",
    "\n",
    "    - local hard news:\n",
    "    \n",
    "        - author bylines are a mess (partially stored in body of article - I was there, I should have helped them fix this).\n",
    "        - need to fix those before I can do locally produced hard news.\n",
    "\n",
    "    - can get counts of articles within hard news sections...?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Article_Data overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- notebooks:\n",
    "\n",
    "    - Article_Data overview notebook: [analysis-article_data_overview.ipynb](./analysis/analysis-article_data_overview.ipynb)\n",
    "    \n",
    "- summary for range of GRP that needs to be examined:\n",
    "\n",
    "    - All 43816 have Article_Data coded by coder_type \"OpenCalais_REST_API_v2\".\n",
    "    - date range of related articles:\n",
    "    \n",
    "        - min: 2005-01-01\n",
    "        - max: 2010-11-30\n",
    "    \n",
    "    - are any missing name, verbatim_name, and lookup_name in Article_Subject or Article_Author?\n",
    "    \n",
    "        - there are 55. What to do?\n",
    "        - first, verify that `verbatim_name` and `lookup_name` are set by default currently.\n",
    "        \n",
    "            - updated unit tests to check, fixed `ArticleCoder.lookup_person()` so it always results in those fields being set (checks if they are empty at the end, if so, sets them to full name).\n",
    "        \n",
    "        - / then, see if I can use the returned OpenCalais data to populate for those where it is empty.\n",
    "        \n",
    "            - OpenCalais JSON is in Article_Data_Notes table.\n",
    "            - Looks like I had the JSON in a text field, then converted it to a JSON field, and it didn't convert right (or maybe exported it and re-imported it and it didn't reimport right?). Need to fix it.\n",
    "            - then, can retrieve JSON for each broken Article_Source and Article_Author and try to see what is going on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- single names\n",
    "\n",
    "    - code:\n",
    "    \n",
    "        - work notebook: [analysis-omit_single_name.ipynb](./analysis/omit_single_name.ipynb)\n",
    "        - notebook where work was done originally (just notes - it was manual): [prelim_month-create_Reliability_Names_data.ipynb](./methods/data_creation/prelim_month-create_Reliability_Names_data.ipynb)\n",
    "    \n",
    "    - from person perspective\n",
    "    \n",
    "        - look at names of all people, see how many are single-name.\n",
    "        - how many persons have only one name part?\n",
    "        - of those, how many are associated with Article_Subject or Article_Author?\n",
    "        - compare that to total Article_Subjects with single word verbatim_name.\n",
    "        - and, see if any match to Article_Subject that doesn't have a single-word verbatim_name.\n",
    "        - hopefully, single-word verbatim name will be enough to detect single-name, with a few whose two-word names might be a single name...?\n",
    "        - then, add a single-name tag to all that are single-name, for ease of subsequent filtering?\n",
    "        - then, look at how many of those single-name people are associated with Article_Subject, Article_Author.\n",
    "        - code to filter to just single first names is in `context_analysis/views.py --> reliability_names_disagreement_view()`:\n",
    "\n",
    "                if ( reliability_names_only_first_name == True ):\n",
    "\n",
    "                    # to start, first name needs to not be null and\n",
    "                    #     not be empty.\n",
    "                    reliability_names_qs = reliability_names_qs.filter( \n",
    "                        Q( person__first_name__isnull = False ) & ~Q( person__first_name = \"\" ),\n",
    "                        Q( person__middle_name__isnull = True ) | Q( person__middle_name = \"\" ),\n",
    "                        Q( person__last_name__isnull = True ) | Q( person__last_name = \"\" ),\n",
    "                        Q( person__name_prefix__isnull = True ) | Q( person__name_prefix = \"\" ),\n",
    "                        Q( person__name_suffix__isnull = True ) | Q( person__name_suffix = \"\" ),\n",
    "                        Q( person__nickname__isnull = True ) | Q( person__nickname = \"\" ),\n",
    "                    )\n",
    "\n",
    "                #-- END only first name --#\n",
    "\n",
    "        - need to use a notebook to check how many only have first name, only have middle, only have last, etc., to see what combination we need to detect all people with a single name.\n",
    "        - then:\n",
    "\n",
    "            - add to the basic person filter/lookup method.\n",
    "            - add it to the network data creation page and the code that processes that request.\n",
    "            - Use it to tag all with single first name with some tag.\n",
    "\n",
    "    - from Article_Data perspective\n",
    "    \n",
    "        - Article_Subject has name columns that capture the name string from the detection. This is where we should look for single names.\n",
    "        \n",
    "            - some might have at one point matched with a person with one part the same as the single name..., so associated person might not be single-name, but verbatim reference in text was, and the association is incorrect - should check for this, also, and disconnect when this has occurred.\n",
    "        \n",
    "        - Look at all Article_Subject with no name information set:\n",
    "        \n",
    "            - first, check if any are related to OpenCalais_REST_API_V2 coding of GRP hard news articles.\n",
    "            - if so, then we need to try to fix these. (will entail looking at original response from OpenCalais to get verbatim name).\n",
    "            - and, make sure the future coding sets name columns.\n",
    "        \n",
    "- code the Detroit News articles.\n",
    "\n",
    "    - for work needed for author strings to be able to find local news correctly, see:\n",
    "    \n",
    "        - `context_text/collectors/newsbank/newspapers/DTNB.py`\n",
    "        - notebook with code to start to deal with this: [data_creation-filter_locally_implemented_hard_news.ipynb](./methods/data_creation/data_creation-filter_locally_implemented_hard_news.ipynb)\n",
    "    \n",
    "    - for filtering down to local news:\n",
    "    \n",
    "        - notes: [newsbank-article_coding.ipynb](./data/article_coding/newsbank-article_coding.ipynb)\n",
    "        - Business, Metro, and Nation\n",
    "        - local byline\n",
    "\n",
    "- code to filter out those with single names... where?\n",
    "\n",
    "\n",
    "    - exploration\n",
    "    \n",
    "- use tags to programatically filter and bin sets of articles I am interested in, then just use those tags to generate network data in [Network Builder](https://research.local/research/context/text/output/network).\n",
    "\n",
    "    - remove single names (add this to the processing in network builder?).\n",
    "    - only appropriate sections\n",
    "    - year before and after layoffs.\n",
    "    - every 3 months of the year before and after layoffs.\n",
    "    - every month of the year before and after layoffs.\n",
    "    - every week of the year before and after layoffs.\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## coding\n",
    "\n",
    "- notebooks:\n",
    "\n",
    "    - OpenCalais V2 (and tagging local hard news):\n",
    "        \n",
    "        - [article_coding.ipynb](./data/article_coding/article_coding.ipynb)\n",
    "        - [newsbank-article_coding.ipynb](./data/article_coding/newsbank-article_coding.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Put things in this cell as they are completed (so, things move from \"TODO\" to notes)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- correlation patterns\n",
    "\n",
    "    - weeks:\n",
    "\n",
    "        - week to week across all weeks\n",
    "        - week to containing month\n",
    "        - week to containing quarter\n",
    "        - week to containing year\n",
    "\n",
    "    - months:\n",
    "\n",
    "        - month to month across all months\n",
    "        - month to containing quarter\n",
    "        - month to containing year\n",
    "\n",
    "    - quarters\n",
    "\n",
    "        - quarter to quarter across all quarters\n",
    "        - quarter to containing year\n",
    "\n",
    "    - years\n",
    "\n",
    "        - year-to-year\n",
    "\n",
    "- consider this for other time slices, also - 6 months, 3 months, 4 weeks, etc.\n",
    "- do within-year correlations for every set of 365 days.\n",
    "- look at correlation patterns of year-before and year-after for all dates that have a year before and after.\n",
    "- modeling:\n",
    "\n",
    "    - input variables:\n",
    "    \n",
    "        - article count before\n",
    "        - average sources per article before\n",
    "        - source count per reporter before\n",
    "        - co-author by reporter before\n",
    "        - average source count of co-authors by reporter before\n",
    "        \n",
    "    - outcome variables (use all inputs to predict each):\n",
    "    \n",
    "        - article count after\n",
    "        - average sources per article after\n",
    "        - source count per reporter after\n",
    "        - co-author by reporter after\n",
    "        - average source count of co-authors by reporter after"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dissertation proposal\n",
    "\n",
    "- Back to [Table of Contents](Table-of-Contents)\n",
    "\n",
    "DONE:\n",
    "\n",
    "- lay out the Journalism-centric thing i’ve been working on.\n",
    "\n",
    "    - start the outline I already wrote, see how it looks.\n",
    "\n",
    "- list of modules I have been preparing for re: dissertation, including all work needed for each, which parts are completed, and a rough estimate timeline for work needed to be done, broken out by task.\n",
    "- a brief dissertation outline based on the work I have already completed, with three papers broken out into details of each.\n",
    "- For the three papers, integrate module estimates with additional work needed for each, which parts are completed, and a rough estimate timeline for work needed to be done, broken out by task.\n",
    "- a narrative for each paper (will try to get into design and methods, including picking data from sets of data I have access to).  For the methods paper, i’ll have a draft completed.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "research_virtualenv",
   "language": "python",
   "name": "research"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "toc-autonumbering": true
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
